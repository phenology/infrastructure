{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between MODIS and Spring-Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n",
    "\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile, Tile, UByteCellType}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.hadoop.io.SequenceFile.Writer\n",
    "import org.apache.hadoop.io.{SequenceFile, _}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "import scala.sys.process._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of operation\n",
    "\n",
    "Here the user can define the mode of operation.\n",
    "* **rdd_offline_mode**: If false it means the notebook will create all data from scratch and store protected_extent and num_cols_rows into HDFS. Otherwise, these data structures are read from HDFS.\n",
    "\n",
    "It is also possible to define which directory of GeoTiffs is to be used and on which **band** to run Kmeans. The options are\n",
    "* **all** which are a multi-band (**8 bands**) GeoTiffs\n",
    "* Or choose single band ones:\n",
    "    0. Onset_Greenness_Increase\n",
    "    1. Onset_Greenness_Maximum\n",
    "    2. Onset_Greenness_Decrease\n",
    "    3. Onset_Greenness_Minimum\n",
    "    4. NBAR_EVI_Onset_Greenness_Minimum\n",
    "    5. NBAR_EVI_Onset_Greenness_Maximum\n",
    "    6. NBAR_EVI_Area\n",
    "    7. Dynamics_QC\n",
    "\n",
    "<span style=\"color:red\">Note that when using a range **kemans offline mode** is not possible and it will be reset to **online mode**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation setup\n",
    "<a id='mode_of_operation_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_offline_mode = true\n",
       "spring_path = hdfs:///user/hadoop/spring-index/\n",
       "spring_dir = BloomFinal\n",
       "modis_path = hdfs:///user/hadoop/avhrr/\n",
       "modis_dir = SOST\n",
       "out_path = hdfs:///user/emma/correlation/\n",
       "band_num = 3\n",
       "toBeMasked = true\n",
       "mask_path = hdfs:///user/hadoop/usa_mask.tif\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/hadoop/usa_mask.tif"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Operation mode\n",
    "var rdd_offline_mode = true\n",
    "\n",
    "//GeoTiffs to be read from \"hdfs:///user/hadoop/spring-index/\"\n",
    "var spring_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var spring_dir = \"BloomFinal\"\n",
    "var modis_path = \"hdfs:///user/hadoop/avhrr/\"\n",
    "var modis_dir = \"SOST\"\n",
    "var out_path = \"hdfs:///user/emma/correlation/\"\n",
    "var band_num = 3\n",
    "\n",
    "//Years between (inclusive) 1989 - 2014\n",
    "var modis_first_year = 1989\n",
    "var modis_last_year = 2014\n",
    "\n",
    "//Years between (inclusive) 1980 - 2015\n",
    "var spring_first_year = 1989\n",
    "var spring_last_year = 2014\n",
    "\n",
    "//Mask\n",
    "val toBeMasked = true\n",
    "val mask_path = \"hdfs:///user/hadoop/usa_mask.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">DON'T MODIFY ANY PIECE OF CODE FROM HERE ON!!!</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of operation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-427935819_36, ugi=emma (auth:SIMPLE)]]\n",
       "spring_grid_path = hdfs:///user/emma/correlation/BloomFinal_spring_grid\n",
       "modis_grid_path = hdfs:///user/emma/correlation/SOST_modis_grid\n",
       "metadata_path = hdfs:///user/emma/correlation//metadata\n",
       "rdd_offline_exists = false\n",
       "corr_tif = hdfs:///user/emma/correlation/_SOST_BloomFinal.tif\n",
       "corr_tif_tmp = /tmp/correlation_SOST_BloomFinal.tif\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "projected_extent: geotrellis....\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/tmp/correlation_SOST_BloomFinal.tif"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Check offline modes\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "var spring_grid_path = out_path + spring_dir + \"_spring_grid\"\n",
    "var modis_grid_path = out_path + modis_dir + \"_modis_grid\"\n",
    "var metadata_path = out_path + \"/metadata\"\n",
    "\n",
    "val rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(metadata_path))\n",
    "if (rdd_offline_mode != rdd_offline_exists) {\n",
    "    println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + rdd_offline_exists.toString())\n",
    "    rdd_offline_mode = rdd_offline_exists\n",
    "} \n",
    "\n",
    "var corr_tif = out_path + \"_\" + modis_dir + \"_\" + spring_dir + \".tif\"\n",
    "var corr_tif_tmp = \"/tmp/correlation_\" + modis_dir + \"_\" + spring_dir + \".tif\"\n",
    "\n",
    "//Years\n",
    "val modis_years = 1980 to 2015\n",
    "val spring_years = 1989 to 2014\n",
    "\n",
    "if (!modis_years.contains(modis_first_year) || !(modis_years.contains(modis_last_year))) {\n",
    "    println(\"Invalid range of years for \" + modis_dir + \". I should be between \" + modis_first_year + \" and \" + modis_last_year)\n",
    "    System.exit(0)\n",
    "}\n",
    "\n",
    "if (!spring_years.contains(spring_first_year) || !(spring_years.contains(spring_last_year))) {\n",
    "    println(\"Invalid range of years for \" + spring_dir + \". I should be between \" + spring_first_year + \" and \" + spring_last_year)\n",
    "    System.exit(0)\n",
    "}\n",
    "\n",
    "if ( ((modis_last_year - spring_first_year) > (spring_last_year - spring_first_year)) || ((modis_last_year - spring_first_year) > (spring_last_year - spring_first_year))) {\n",
    "    println(\"The range of years for each data set should be of the same length.\");\n",
    "    System.exit(0)\n",
    "}\n",
    "    \n",
    "var spring_years_range = (spring_years.indexOf(spring_first_year), spring_years.indexOf(spring_last_year))\n",
    "var modis_years_range = (modis_years.indexOf(modis_first_year), modis_years.indexOf(modis_last_year))\n",
    "\n",
    "//Global variables\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var spring_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var modis_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var cellT :CellType = UByteCellType\n",
    "var mask_tile0 :Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to (de)serialize any structure into Array[Byte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serialize: (value: Any)Array[Byte]\n",
       "deserialize: (bytes: Array[Byte])Any\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def serialize(value: Any): Array[Byte] = {\n",
    "    val out_stream: ByteArrayOutputStream = new ByteArrayOutputStream()\n",
    "    val obj_out_stream = new ObjectOutputStream(out_stream)\n",
    "    obj_out_stream.writeObject(value)\n",
    "    obj_out_stream.close\n",
    "    out_stream.toByteArray\n",
    "}\n",
    "\n",
    "def deserialize(bytes: Array[Byte]): Any = {\n",
    "    val obj_in_stream = new ObjectInputStream(new ByteArrayInputStream(bytes))\n",
    "    val value = obj_in_stream.readObject\n",
    "    obj_in_stream.close\n",
    "    value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GeoTiffs\n",
    "\n",
    "Using GeoTrellis all GeoTiffs of a directory will be loaded into a RDD. Using the RDD, we extract a grid from the first file to lated store the Kmeans cluster_IDS, we build an Index for populate such grid and we filter out here all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Load Mask\n",
    "if (toBeMasked) {\n",
    "    val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mask_path).values\n",
    "    val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    mask_tile0 = (mask_tiles_withIndex.filter(m => m._1==0).filter(m => !m._1.isNaN).values.collect())(0)\n",
    "}\n",
    "\n",
    "//Local variables\n",
    "val pattern: String = \"tif\"\n",
    "val spring_filepath: String = spring_path + \"/\" + spring_dir\n",
    "val modis_filepath: String = modis_path + \"/\" + modis_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 27:====================================================>   (34 + 2) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2101)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:369)\n",
       "  ... 74 elided\n",
       "Caused by: java.io.NotSerializableException: org.apache.hadoop.conf.Configuration\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: org.apache.hadoop.conf.Configuration, value: Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml)\n",
       "\t- field (class: $iw, name: conf, type: class org.apache.hadoop.conf.Configuration)\n",
       "\t- object (class $iw, $iw@41985cc3)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@2e947861)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@44c0c39)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@75c99a41)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7fcc9c7f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1ee3d015)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@46ecc914)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@562cab65)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5ae2d6cf)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7576958f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@586f861)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@27beb1c4)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7512dae8)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5be057f9)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1fd6ba5c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4bc11ec8)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6b26b2ad)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1a1ea953)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6ae1741c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@11e711d5)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@3b300459)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@665c1d62)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1806ad21)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@331fe371)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@46ed13d3)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@10d4d02)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@30e53e03)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@2b4481f0)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6a6ef7ea)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@45e0e687)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@36be352a)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7c453d42)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@148b82e8)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7f08409c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@18f6f0df)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@b73b1ef)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@783eb389)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4382a0f7)\n",
       "\t- field (class: $line317.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line317.$read, $line317.$read@6503346c)\n",
       "\t- field (class: $iw, name: $line317$read, type: class $line317.$read)\n",
       "\t- object (class $iw, $iw@202321c0)\n",
       "\t- field (class: $iw, name: $outer, type: class $iw)\n",
       "\t- object (class $iw, $iw@164113b3)\n",
       "\t- field (class: $anonfun$11, name: $outer, type: class $iw)\n",
       "\t- object (class $anonfun$11, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (rdd_offline_mode) {\n",
    "    spring_grids_RDD = sc.objectFile(spring_grid_path)\n",
    "    val metadata = sc.sequenceFile(metadata_path, classOf[IntWritable], classOf[BytesWritable]).map(_._2.copyBytes()).collect()\n",
    "    projected_extent = deserialize(metadata(0)).asInstanceOf[ProjectedExtent]\n",
    "    num_cols_rows = (deserialize(metadata(1)).asInstanceOf[Int], deserialize(metadata(2)).asInstanceOf[Int])\n",
    "} else {\n",
    "    //Lets load Spring-Index Multiband GeoTiffs and return RDD just with the tiles.\n",
    "    val spring_geos_RDD = sc.hadoopMultibandGeoTiffRDD(spring_filepath, pattern)\n",
    "    val spring_tiles_RDD = spring_geos_RDD.values\n",
    "\n",
    "    //Retrieve the number of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = spring_tiles_RDD.zipWithIndex().map{case (v,i) => (i,v)}\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "    val projected_extents_withIndex = spring_geos_RDD.keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect())(0)\n",
    "\n",
    "    //Lets read the average of the Spring-Index which is stored in the 4th band\n",
    "    if (toBeMasked) {\n",
    "      val mask_tile_broad :Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "      spring_grids_RDD = spring_tiles_RDD.map(m => m.band(band_num).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble().filter(_ != -1000))\n",
    "    } else {\n",
    "      spring_grids_RDD = spring_tiles_RDD.map(m => m.band(band_num).toArrayDouble())\n",
    "    }\n",
    "        \n",
    "    //Store data in HDFS\n",
    "    spring_grids_RDD.saveAsObjectFile(spring_grid_path)\n",
    "\n",
    "    val writer: SequenceFile.Writer = SequenceFile.createWriter(conf,\n",
    "        Writer.file(metadata_path),\n",
    "        Writer.keyClass(classOf[IntWritable]),\n",
    "        Writer.valueClass(classOf[BytesWritable])\n",
    "    )\n",
    "\n",
    "    writer.append(new IntWritable(1), new BytesWritable(serialize(projected_extent)))\n",
    "    writer.append(new IntWritable(2), new BytesWritable(serialize(num_cols_rows._1)))\n",
    "    writer.append(new IntWritable(3), new BytesWritable(serialize(num_cols_rows._2)))\n",
    "    writer.hflush()\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 27:====================================================>   (34 + 2) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2101)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:369)\n",
       "  ... 74 elided\n",
       "Caused by: java.io.NotSerializableException: org.apache.hadoop.conf.Configuration\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: org.apache.hadoop.conf.Configuration, value: Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml)\n",
       "\t- field (class: $iw, name: conf, type: class org.apache.hadoop.conf.Configuration)\n",
       "\t- object (class $iw, $iw@41985cc3)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@2e947861)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@44c0c39)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@75c99a41)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7fcc9c7f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1ee3d015)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@46ecc914)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@562cab65)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5ae2d6cf)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7576958f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@586f861)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@27beb1c4)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7512dae8)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5be057f9)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1fd6ba5c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4bc11ec8)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6b26b2ad)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1a1ea953)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6ae1741c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@11e711d5)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@3b300459)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@665c1d62)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1806ad21)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@331fe371)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@46ed13d3)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@10d4d02)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@30e53e03)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@2b4481f0)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6a6ef7ea)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@45e0e687)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@36be352a)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7c453d42)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@148b82e8)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7f08409c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@18f6f0df)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@b73b1ef)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@783eb389)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4382a0f7)\n",
       "\t- field (class: $line317.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line317.$read, $line317.$read@6503346c)\n",
       "\t- field (class: $iw, name: $line317$read, type: class $line317.$read)\n",
       "\t- object (class $iw, $iw@202321c0)\n",
       "\t- field (class: $iw, name: $outer, type: class $iw)\n",
       "\t- object (class $iw, $iw@164113b3)\n",
       "\t- field (class: $anonfun$11, name: $outer, type: class $iw)\n",
       "\t- object (class $anonfun$11, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (rdd_offline_mode) {\n",
    "    modis_grids_RDD = sc.objectFile(modis_grid_path)\n",
    "} else {\n",
    "    //Lets load MODIS Singleband GeoTiffs and return RDD just with the tiles.\n",
    "    val modis_geos_RDD = sc.hadoopGeoTiffRDD(modis_filepath, pattern)\n",
    "    val modis_tiles_RDD = modis_geos_RDD.values\n",
    "\n",
    "    if (toBeMasked) {\n",
    "        val mask_tile_broad :Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "        modis_grids_RDD = modis_tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble().filter(_ != -1000))\n",
    "    } else {\n",
    "        modis_grids_RDD = modis_tiles_RDD.map(m => m.toArrayDouble())\n",
    "    }\n",
    "        \n",
    "    //Store in HDFS\n",
    "    modis_grids_RDD.saveAsObjectFile(modis_grid_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "\n",
    "We need to do a Matrix transpose to have clusters per cell and not per year. With a GeoTiff representing a single year, the loaded data looks liks this:\n",
    "```\n",
    "bands_RDD.map(s => Vectors.dense(s)).cache()\n",
    "\n",
    "//The vectors are rows and therefore the matrix will look like this:\n",
    "[\n",
    "Vectors.dense(0.0, 1.0, 2.0),\n",
    "Vectors.dense(3.0, 4.0, 5.0),\n",
    "Vectors.dense(6.0, 7.0, 8.0),\n",
    "Vectors.dense(9.0, 0.0, 1.0)\n",
    "]\n",
    "```\n",
    "\n",
    "To achieve that we convert the **RDD[Vector]** into a distributed Matrix, a [**CoordinateMatrix**](https://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix), which as a **transpose** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 105526406ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 46708214638977\n",
       "grids_matrix = MapPartitionsRDD[22] at objectFile at <console>:75\n",
       "grid_cells_sizeB = Broadcast(10)\n",
       "t1 = 46708320165383\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "46708320165383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t0 = System.nanoTime()\n",
    "\n",
    "//Global variables\n",
    "var spring_matrix: RDD[Vector] = sc.emptyRDD\n",
    "\n",
    "val grid_cells_sizeB = sc.broadcast(spring_cells_size)\n",
    "if (matrix_offline_mode) {\n",
    "    spring_matrix = sc.objectFile(spring_matrix_path)\n",
    "} else {\n",
    "    val mat :RowMatrix = new RowMatrix(spring_noNaN_RDD.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(grid_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "    // Split the matrix into one number per line.\n",
    "    val byColumnAndRow = mat.rows.zipWithIndex.map {\n",
    "        case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "            case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "        }   \n",
    "    }.flatMap(x => x)\n",
    "    \n",
    "    val matt: CoordinateMatrix = new CoordinateMatrix(byColumnAndRow)\n",
    "    val matt_T = matt.transpose()\n",
    "    \n",
    "    spring_matrix = matt_T.toIndexedRowMatrix().rows.sortBy(_.index).map(_.vector)\n",
    "    spring_matrix.saveAsObjectFile(spring_matrix_path)\n",
    "}\n",
    "\n",
    "val t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 105526406ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 46708214638977\n",
       "grids_matrix = MapPartitionsRDD[22] at objectFile at <console>:75\n",
       "grid_cells_sizeB = Broadcast(10)\n",
       "t1 = 46708320165383\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "46708320165383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t0 = System.nanoTime()\n",
    "//Global variables\n",
    "var modis_matrix: RDD[Vector] = sc.emptyRDD\n",
    "\n",
    "val grid_cells_sizeB = sc.broadcast(grid_cells_size)\n",
    "if (matrix_offline_mode) {\n",
    "    modis_matrix = sc.objectFile(grids_matrix_path)\n",
    "} else {\n",
    "    val mat :RowMatrix = new RowMatrix(grids_noNaN_RDD.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(grid_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "    // Split the matrix into one number per line.\n",
    "    val byColumnAndRow = mat.rows.zipWithIndex.map {\n",
    "        case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "            case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "        }   \n",
    "    }.flatMap(x => x)\n",
    "    \n",
    "    val matt: CoordinateMatrix = new CoordinateMatrix(byColumnAndRow)\n",
    "    val matt_T = matt.transpose()\n",
    "    \n",
    "    modis_matrix = matt_T.toIndexedRowMatrix().rows.sortBy(_.index).map(_.vector)\n",
    "    modis_matrix.saveAsObjectFile(modis_matrix_path)\n",
    "}\n",
    "\n",
    "val t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var spring_matrix_rows = spring_matrix.count()\n",
    "var modis_matrix_rows = modis_matrix.count()\n",
    "\n",
    "if (spring_matrix_row != modis_matrix_rows) {\n",
    "    println(\"For correlation it is necessary to have a matrix with same number of rows and columns!!!\")\n",
    "    println(\"Spring matrix has \" spring_matrix_rows + \" rows while modis matrix has \" + modis_matrix_rows + \" rows!!!\")\n",
    "    System.exit(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 29:=====================================================>  (25 + 1) / 26]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Can't zip RDDs with unequal numbers of partitions: List(26, 0)\n",
       "StackTrace:   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1333)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.first(RDD.scala:1367)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:331)\n",
       "  at org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n",
       "  at org.apache.spark.mllib.stat.correlation.Correlation$class.computeCorrelationWithMatrixImpl(Correlation.scala:46)\n",
       "  at org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationWithMatrixImpl(PearsonCorrelation.scala:34)\n",
       "  at org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelation(PearsonCorrelation.scala:40)\n",
       "  at org.apache.spark.mllib.stat.correlation.Correlations$.corr(Correlation.scala:60)\n",
       "  at org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:112)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modis = modis_matrix.zipWithIndex().map{ case (v, i) => (i,v)}\n",
    "val spring = spring_matrix.zipWithIndex().map{case (v,i) => (i, v)}    \n",
    "\n",
    "val numCells = modis.count()\n",
    "var corr_res :Array[Double] = Array[Double](numCells)\n",
    "var cell = 0\n",
    "while (cell < numCells) {\n",
    "    val modis_array = modis.filter(_._1 == cell).flatMap(_._2)\n",
    "    val spring_array = spring.filter(_._1 == cell).flatMap(_._2)\n",
    "    corr_res(cell) = Statistics.corr(modis_array, spring_array, \"pearson\")\n",
    "    cell += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GeoTiff with Kmeans cluster_IDs\n",
    "\n",
    "The Grid with the cluster IDs is stored in a SingleBand GeoTiff and uploaded to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign cluster ID to each grid cell and save the grid as SingleBand GeoTiff\n",
    "\n",
    "To assign the clusterID to each grid cell it is necessary to get the indices of gird cells they belong to. The process is not straight forward because the ArrayDouble used for the creation of each dense Vector does not contain the NaN values, therefore there is not a direct between the indices in the Tile's grid and the ones in **kmeans_res** (kmeans result).\n",
    "\n",
    "To join the two RDDS the knowledge was obtaing from a stackoverflow post on [how to perform basic joins of two rdd tables in spark using python](https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:132: error: not enough arguments for method collect: (pf: PartialFunction[Double,B])(implicit bf: scala.collection.generic.CanBuildFrom[Array[Double],B,That])That.\n",
       "Unspecified value parameter pf.\n",
       "       val corr_cells :Array[Double] = corr_res.collect()\n",
       "                                                       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Define a Tile\n",
    "val corr_cells :Array[Double] = corr_res.collect()\n",
    "val corr_cellsD = DoubleArrayTile(corr_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "val geoTif = new SinglebandGeoTiff(corr_cellsD, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "\n",
    "//Save to /tmp/\n",
    "GeoTiffWriter.write(geoTif, corr_tif_tmp)\n",
    "\n",
    "//Upload to HDFS\n",
    "var cmd = \"hadoop dfs -copyFromLocal -f \" + corr_tif_tmp + \" \" + corr_tif\n",
    "Process(cmd)!\n",
    "\n",
    "//Remove from /tmp/\n",
    "cmd = \"rm -fr \" + corr_tif_tmp\n",
    "Process(cmd)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Visualize results](plot_kmeans_clusters.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
