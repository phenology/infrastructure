{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/**\n",
    " The following notebook does:\n",
    "     - Reads multi single- or a multi- band GeoTiff from HDFS\n",
    "     - Filters out or converts NaN values\n",
    "     - Runs Kmeans\n",
    "     - Saves the kmeans model to HDFS\n",
    "**/\n",
    "\n",
    "\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.{DoubleArrayTile, Tile}\n",
    "import geotrellis.raster.io.geotiff._\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{GeoTiff, SinglebandGeoTiff}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================================> (35 + 1) / 36]"
     ]
    }
   ],
   "source": [
    "    val band_count = 1;\n",
    "    val in_memory = 2;\n",
    "    val sample = 1000;\n",
    "    var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "    var num_cols_rows :(Int, Int) = (0, 0)\n",
    "    var band_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "    var band_vec: RDD[Vector] = sc.emptyRDD\n",
    "    var band0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "    var band0_index: Array[Int] = Array.emptyIntArray\n",
    "    val pattern: String = \"tif\"\n",
    "    var filepath: String = \"\"\n",
    "    if (band_count == 1) {\n",
    "      //Single band GeoTiff\n",
    "      filepath = \"hdfs:///user/hadoop/spring-index/LastFreeze/\"\n",
    "    } else {\n",
    "      //Multi band GeoTiff\n",
    "      filepath = \"hdfs:///user/hadoop/spring-index/BloomFinal/\"\n",
    "    }\n",
    "\n",
    "    if (band_count == 1) {\n",
    "      //Lets load a Singleband GeoTiff and return RDD just with the tiles.\n",
    "      //Since it is a single GeoTiff, it will be a RDD with a tile.\n",
    "      val tiles_RDD = sc.hadoopGeoTiffRDD(filepath, pattern).values\n",
    "      val bands_RDD = tiles_RDD.map(m => m.toArrayDouble())\n",
    "\n",
    "      val extents_withIndex = sc.hadoopGeoTiffRDD(filepath, pattern).keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "      projected_extent = extents_withIndex.lookup(0).apply(0)\n",
    "\n",
    "      val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "      num_cols_rows = (tiles_withIndex.lookup(0).apply(0).cols, tiles_withIndex.lookup(0).apply(0).rows)\n",
    "\n",
    "      //Get Index for Cells\n",
    "      val bands_withIndex = bands_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "      //band0_index = bands_withIndex.lookup(0).apply(0).zipWithIndex.filter{ case (v, i) => !v.isNaN }.map { case (v, i) => (i) }\n",
    "      band0_index = bands_withIndex.lookup(0).apply(0).zipWithIndex.filter(m => !m._1.isNaN).take(sample).map { case (v, i) => (i) }\n",
    "\n",
    "      //Get Array[Double] of a Title to later store the cluster ids.\n",
    "      band0 = sc.parallelize(bands_withIndex.lookup(0).take(1)).flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "      //Lets filter out NaN\n",
    "      band_RDD = bands_RDD.map(m => m.filter(!_.isNaN).take(sample))\n",
    "    } else {\n",
    "      //Lets load a Multiband GeoTiff and return RDD just with the tiles.\n",
    "      //Since it is a multi-band GeoTiff, we will take band 4\n",
    "      val tiles_RDD = sc.hadoopMultibandGeoTiffRDD(filepath, pattern).values\n",
    "      val bands_RDD = tiles_RDD.map(m => m.band(3).toArrayDouble())\n",
    "\n",
    "      val extents_withIndex = sc.hadoopGeoTiffRDD(filepath, pattern).keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "      projected_extent = extents_withIndex.lookup(0).apply(0)\n",
    "\n",
    "      val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "      num_cols_rows = (tiles_withIndex.lookup(0).apply(0).cols, tiles_withIndex.lookup(0).apply(0).rows)\n",
    "\n",
    "      //Get Index for Cells\n",
    "      val bands_withIndex = bands_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "      band0_index = bands_withIndex.lookup(0).apply(0).zipWithIndex.filter { case (v, i) => !v.isNaN }.take(sample).map { case (v, i) => (i) }\n",
    "\n",
    "      //Get Array[Double] of a Title to later store the cluster ids.\n",
    "      band0 = sc.parallelize(bands_withIndex.lookup(0).take(1)).flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "      //Let's filter out NaN\n",
    "      band_RDD = bands_RDD.map(m => m.filter(v => !v.isNaN).take(sample))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============================================>         (30 + 4) / 36]"
     ]
    }
   ],
   "source": [
    "    /*\n",
    "    We need to do a Matrix transpose to have clusters per cell\n",
    "    and not per year. If we do:\n",
    "\n",
    "    val band_vec = band_RDD.map(s => Vectors.dense(s)).cache()\n",
    "\n",
    "    The vectors are rows and therefore the matrix will look like this:\n",
    "    Vectors.dense(0.0, 1.0, 2.0),\n",
    "    Vectors.dense(3.0, 4.0, 5.0),\n",
    "    Vectors.dense(6.0, 7.0, 8.0),\n",
    "    Vectors.dense(9.0, 0.0, 1.0)\n",
    "\n",
    "    Inspired in:\n",
    "    http://jacob119.blogspot.nl/2015/11/how-to-convert-matrix-to-rddvector-in.html\n",
    "    and\n",
    "    https://stackoverflow.com/questions/29390717/how-to-transpose-an-rdd-in-spark\n",
    "    */\n",
    "\n",
    "    if (in_memory == 1) {\n",
    "      //A) For small memory footprint RDDs we can simply bring it to memory and transpose it\n",
    "      //First transpose and then parallelize otherwise you get:\n",
    "      //error: polymorphic expression cannot be instantiated to expected type;\n",
    "      val band_vec_T = band_RDD.collect().transpose\n",
    "      band_vec = sc.parallelize(band_vec_T).map(m => Vectors.dense(m)).cache()\n",
    "    } else {\n",
    "      //B) For large memory footpring RDDs we need to run in distributed mode\n",
    "\n",
    "      // Split the matrix into one number per line.\n",
    "      val byColumnAndRow = band_RDD.zipWithIndex.flatMap {\n",
    "        case (row, rowIndex) => row.zipWithIndex.map {\n",
    "          case (number, columnIndex) => columnIndex -> (rowIndex, number)\n",
    "        }\n",
    "      }\n",
    "\n",
    "      // Build up the transposed matrix. Group and sort by column index first.\n",
    "      val byColumn = byColumnAndRow.groupByKey.sortByKey().values\n",
    "\n",
    "      // Then sort by row index.\n",
    "      val transposed = byColumn.map {\n",
    "        indexedRow => indexedRow.toSeq.sortBy(_._1).map(_._2)\n",
    "      }\n",
    "\n",
    "      band_vec = transposed.map(m => Vectors.dense(m.toArray)).cache()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 14:==========================================>             (27 + 4) / 36]1000\n",
      "36\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,136.0]\n"
     ]
    }
   ],
   "source": [
    "    /*\n",
    "     Here we will collect some info to see if the transpose worked correctly\n",
    "    */\n",
    "\n",
    "    val band_vec_col = band_vec.collect()\n",
    "\n",
    "    //Number of Columns, i.e., years\n",
    "    println(band_vec_col.size)\n",
    "\n",
    "    //Number of cells after filtering our NaN and a take()\n",
    "    println(band_vec_col(0).size)\n",
    "\n",
    "    //Values for a cell over the years.\n",
    "    println(band_vec_col(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                        (0 + 0) / 36]Within Set Sum of Squared Errors = 2.4225394591567043E7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[39] at map at <console>:91"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    /*\n",
    "     Here we will train kmeans\n",
    "    */\n",
    "\n",
    "    val numClusters = 3\n",
    "    val numIterations = 5\n",
    "    val clusters = {\n",
    "      KMeans.train(band_vec, numClusters, numIterations)\n",
    "    }\n",
    "\n",
    "    // Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "    val WSSSE = clusters.computeCost(band_vec)\n",
    "    println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n",
    "\n",
    "    //Un-persist the model\n",
    "    band_vec.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[141.4864376130199,142.8245931283906,146.18444846292948,131.62206148282098,154.124773960217,157.07956600361663,151.12115732368898,147.86618444846292,142.5388788426763,140.06690777576853,139.39783001808317,150.58770343580468,136.16817359855335,136.09764918625677,127.51717902350813,139.88969258589512,152.17902350813742,150.62748643761302,141.0379746835443,164.16817359855335,162.39602169981916,161.50813743218805,164.875226039783,156.2368896925859,125.9873417721519,128.877034358047,130.72151898734177,127.4755877034358,139.251356238698,132.59493670886076,136.21699819168174,129.77396021699818,131.9240506329114,128.54611211573237,116.60397830018083,136.0]\n",
      "[17.50462962962963,16.98611111111111,16.62037037037037,9.833333333333332,8.268518518518517,17.828703703703702,20.944444444444443,7.592592592592593,13.347222222222221,25.314814814814813,20.37037037037037,21.38425925925926,2.2268518518518516,22.98148148148148,19.953703703703702,22.23611111111111,25.35648148148148,13.560185185185185,5.236111111111111,2.111111111111111,10.574074074074073,15.324074074074073,30.60648148148148,20.60648148148148,3.9027777777777777,11.800925925925926,22.10648148148148,22.712962962962962,13.458333333333332,27.61111111111111,0.0,23.657407407407405,21.212962962962962,5.101851851851851,17.375,136.0]\n",
      "[73.18614718614718,60.17316017316017,86.34632034632034,48.45454545454545,58.67965367965368,85.31601731601732,73.5930735930736,59.38528138528139,73.63636363636364,72.77922077922078,71.91341991341992,84.85281385281385,45.04329004329004,70.9004329004329,72.14285714285714,74.64935064935065,87.11255411255411,80.10822510822511,56.121212121212125,76.78354978354979,71.23809523809524,73.47186147186147,91.47619047619048,75.0995670995671,45.63203463203463,62.84848484848485,77.33333333333333,77.17748917748918,98.15584415584415,84.87878787878788,42.489177489177486,77.78787878787878,74.2987012987013,48.073593073593074,55.532467532467535,136.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    /*\n",
    "     Cluster model's result management\n",
    "    */\n",
    "\n",
    "    // Lets show the result.\n",
    "    println(\"Cluster Centers: \")\n",
    "    clusters.clusterCenters.foreach(println)\n",
    "\n",
    "    //Lets save the model into HDFS. If the file already exists it will abort and report error.\n",
    "    /*\n",
    "    if (band_count == 1) {\n",
    "        clusters.save(sc, \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model\")\n",
    "    } else {\n",
    "        clusters.save(sc, \"hdfs:///user/emma/spring_index/BloomFinal/all_kmeans_model\")\n",
    "    }\n",
    "    */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    /*\n",
    "     Run Kmeans and obtain the clusters per each cell and collect first 50 results.\n",
    "    */\n",
    "\n",
    "    //Cache the model\n",
    "    band_vec.cache()\n",
    "\n",
    "    val res = clusters.predict(band_vec)\n",
    "    res.repartition(1)getNumPartitions\n",
    "\n",
    "    //Un-persist the model\n",
    "    band_vec.unpersist()\n",
    "\n",
    "    //Collect first 50\n",
    "    val res_out = res.collect()//.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "    /*\n",
    "     Show the cluster ID for the first 50 cells\n",
    "    */\n",
    "\n",
    "    //res_out.foreach(println)\n",
    "    println(res_out.size)\n",
    "    println(band0_index.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,0), (1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (1,10), (1,11), (1,12), (1,13), (1,14), (1,15), (1,16), (1,17), (1,18), (1,19), (1,20), (1,21), (1,22), (1,23), (1,24), (1,25), (1,26), (1,27), (2,28), (2,29), (2,30), (2,31), (1,32), (1,33), (1,34), (1,35), (2,36), (0,37), (0,38), (0,39), (0,40), (0,41), (2,42), (2,43), (2,44), (0,45), (0,46), (0,47), (0,48), (0,49))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    /*\n",
    "     Save the result as GeoTiff. However, it is not straightforward.\n",
    "     We need to get the clusterCenter which is a RDD[Vectors]\n",
    "     It contains a vector per year. However, the vector indices\n",
    "     are the ones from the ArrayOfDoubles with the NaN values.\n",
    "    */\n",
    "\n",
    "    //Merge two RDDs\n",
    "    val cluster_cell_pos = res.repartition(1).zip(sc.parallelize(band0_index, 1))\n",
    "    //val cluster_cell_pos = res.zip(sc.parallelize(band0_index, 36))\n",
    "    cluster_cell_pos.collect().take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    /*\n",
    "     Join the RDD with clusters with the Grid of cells from GeoTiff.\n",
    "     Inspired in:\n",
    "      https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python\n",
    "    */\n",
    "    //val grid_clusters = band0.join(cluster_cell_pos)\n",
    "    //val grid_clusters = band0.take(1000).leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i.toLong, c)})\n",
    "    val band0_0_1010 = band0.filterByRange(0,1010)\n",
    "    val grid_clusters = band0_0_1010.repartition(1000).leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i.toLong, c)}.repartition(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 366:===================================================>(982 + 4) / 1000]"
     ]
    }
   ],
   "source": [
    "    //val grid_clusters_res = grid_clusters.take(50).foreach(println)\n",
    "    //val grid_clusters_res = grid_clusters.sortByKey(true).take(50).foreach(println)\n",
    "    //val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get)}//.collect().foreach(println)\n",
    "    val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, -1.0) else (k, c.get.toDouble)}//.take(50).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 394:==============>                                          (1 + 3) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   /*\n",
    "     Create a GeoTiff and save to HDFS.\n",
    "    */\n",
    "\n",
    "    val cluster_cells :Array[Double] = grid_clusters_res.values.collect()\n",
    "    //val cluster_tile = DoubleArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "    val cluster_tile = DoubleArrayTile(cluster_cells, 101, 10)\n",
    "    val geoTiff = SinglebandGeoTiff(cluster_tile, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT)\n",
    "\n",
    "    sc.parallelize(geoTiff.toByteArray).saveAsObjectFile(\"hdfs:///user/emma/spring-index/BloomFinal/cluster.tif\")\n",
    "\n",
    "    //Write to the local file system\n",
    "    //val path = \"~/clusters.tif\"\n",
    "    //GeoTiffWriter.write(geoTiff, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
