{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD between a Model and Satellite data\n",
    "\n",
    "This notebook shows how to multiply two matrices and calculate SVD. Each matrix is created out a set of GeoTiffs for a series of years. Both matrices should have the same dimension.\n",
    "\n",
    "For demonstration we will use from a model (spring-index) and from a satellite (AVHRR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n",
    "\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile, Tile, UByteCellType}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.hadoop.io.SequenceFile.Writer\n",
    "import org.apache.hadoop.io.{SequenceFile, _}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry, RowMatrix}\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "import scala.sys.process._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of operation\n",
    "\n",
    "Here the user can define the mode of operation.\n",
    "* **rdd_offline_mode**: If false it means the notebook will create all data from scratch and store protected_extent and num_cols_rows into HDFS. Otherwise, these data structures are read from HDFS.\n",
    "\n",
    "It is also possible to define which directory of GeoTiffs is to be used and on which **band** to run Kmeans. The options are\n",
    "* **all** which are a multi-band (**8 bands**) GeoTiffs\n",
    "* Or choose single band ones:\n",
    "    0. Onset_Greenness_Increase\n",
    "    1. Onset_Greenness_Maximum\n",
    "    2. Onset_Greenness_Decrease\n",
    "    3. Onset_Greenness_Minimum\n",
    "    4. NBAR_EVI_Onset_Greenness_Minimum\n",
    "    5. NBAR_EVI_Onset_Greenness_Maximum\n",
    "    6. NBAR_EVI_Area\n",
    "    7. Dynamics_QC\n",
    "\n",
    "<span style=\"color:red\">Note that when using a range **kemans offline mode** is not possible and it will be reset to **online mode**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation setup\n",
    "<a id='mode_of_operation_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_rdd_offline_mode = true\n",
       "model_matrix_offline_mode = true\n",
       "satellite_rdd_offline_mode = true\n",
       "satellite_matrix_offline_mode = true\n",
       "model_path = hdfs:///user/hadoop/spring-index/\n",
       "model_dir = BloomFinal\n",
       "satellite_path = hdfs:///user/hadoop/avhrr/\n",
       "satellite_dir = SOST\n",
       "out_path = hdfs:///user/pheno/svd/\n",
       "band_num = 3\n",
       "satellite_first_year = 1989\n",
       "satellite_last_year = 2014\n",
       "model_first_year = 1989\n",
       "model_last_year = 2014\n",
       "toBeMasked = true\n",
       "mask_path = hdfs:///user/hadoop/usa_mask.tif\n",
       "save_rdds = true\n",
       "save_matrix = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var model_rdd_offline_mode = true\n",
    "var model_matrix_offline_mode = true\n",
    "var satellite_rdd_offline_mode = true\n",
    "var satellite_matrix_offline_mode = true\n",
    "\n",
    "//Using spring-index model\n",
    "var model_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var model_dir = \"BloomFinal\"\n",
    "\n",
    "//Using AVHRR Satellite data\n",
    "var satellite_path = \"hdfs:///user/hadoop/avhrr/\"\n",
    "var satellite_dir = \"SOST\"\n",
    "\n",
    "var out_path = \"hdfs:///user/pheno/svd/\"\n",
    "var band_num = 3\n",
    "\n",
    "//Years between (inclusive) 1989 - 2014\n",
    "var satellite_first_year = 1989\n",
    "var satellite_last_year = 2014\n",
    "\n",
    "//Years between (inclusive) 1980 - 2015\n",
    "var model_first_year = 1989\n",
    "var model_last_year = 2014\n",
    "\n",
    "//Mask\n",
    "val toBeMasked = true\n",
    "val mask_path = \"hdfs:///user/hadoop/usa_mask.tif\"\n",
    "\n",
    "val save_rdds = true\n",
    "val save_matrix = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">DON'T MODIFY ANY PIECE OF CODE FROM HERE ON!!!</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of operation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-504629498_36, ugi=pheno (auth:SIMPLE)]]\n",
       "mask_str = _mask\n",
       "model_grid0_path = hdfs:///user/pheno/svd/BloomFinal_grid0\n",
       "model_grid0_index_path = hdfs:///user/pheno/svd/BloomFinal_grid0_index\n",
       "model_grid_path = hdfs:///user/pheno/svd/BloomFinal_grid\n",
       "satellite_grid_path = hdfs:///user/pheno/svd/SOST_grid\n",
       "model_matrix_path = hdfs:///user/pheno/svd/BloomFinal_matrix\n",
       "satellite_matrix_path = hdfs:///us...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/svd/SOST_matrix"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Check offline modes\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "//Paths to store data structures for Offline runs\n",
    "var mask_str = \"\"\n",
    "if (toBeMasked)\n",
    "  mask_str = \"_mask\"\n",
    "var model_grid0_path = out_path + model_dir + \"_grid0\"\n",
    "var model_grid0_index_path = out_path + model_dir + \"_grid0_index\"\n",
    "\n",
    "var model_grid_path = out_path + model_dir + \"_grid\"\n",
    "var satellite_grid_path = out_path + satellite_dir + \"_grid\"\n",
    "var model_matrix_path = out_path + model_dir + \"_matrix\"\n",
    "var satellite_matrix_path = out_path + satellite_dir + \"_matrix\"\n",
    "var metadata_path = out_path + model_dir + \"_metadata\"\n",
    "\n",
    "val model_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(model_grid_path))\n",
    "val model_matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(model_matrix_path))\n",
    "val satellite_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(satellite_grid_path))\n",
    "val satellite_matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(satellite_matrix_path))\n",
    "\n",
    "if (model_rdd_offline_mode != model_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + model_rdd_offline_exists.toString())\n",
    "  model_rdd_offline_mode = model_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (model_matrix_offline_mode != model_matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + model_matrix_offline_exists.toString())\n",
    "  model_matrix_offline_mode = model_matrix_offline_exists\n",
    "}\n",
    "\n",
    "var model_skip_rdd = false\n",
    "if (model_matrix_offline_exists) {\n",
    "    println(\"Since we have a matrix, the load of the grids RDD will be skipped!!!\")\n",
    "    model_skip_rdd = true\n",
    "}\n",
    "\n",
    "if (satellite_rdd_offline_mode != satellite_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + satellite_rdd_offline_exists.toString())\n",
    "  satellite_rdd_offline_mode = satellite_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (satellite_matrix_offline_mode != satellite_matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + satellite_matrix_offline_exists.toString())\n",
    "  satellite_matrix_offline_mode = satellite_matrix_offline_exists\n",
    "}\n",
    "\n",
    "var satellite_skip_rdd = false\n",
    "if (satellite_matrix_offline_exists) {\n",
    "    println(\"Since we have a matrix, the load of the grids RDD will be skipped!!!\")\n",
    "    satellite_skip_rdd = true\n",
    "}\n",
    "\n",
    "var corr_tif = out_path + \"_\" + satellite_dir + \"_\" + model_dir + \".tif\"\n",
    "var corr_tif_tmp = \"/tmp/svd_\" + satellite_dir + \"_\" + model_dir + \".tif\"\n",
    "\n",
    "//Years\n",
    "val model_years = 1980 to 2015\n",
    "val satellite_years = 1989 to 2014\n",
    "\n",
    "if (!satellite_years.contains(satellite_first_year) || !(satellite_years.contains(satellite_last_year))) {\n",
    "  println(\"Invalid range of years for \" + satellite_dir + \". I should be between \" + satellite_first_year + \" and \" + satellite_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "if (!model_years.contains(model_first_year) || !(model_years.contains(model_last_year))) {\n",
    "  println(\"Invalid range of years for \" + model_dir + \". I should be between \" + model_first_year + \" and \" + model_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "if ( ((satellite_last_year - model_first_year) > (model_last_year - model_first_year)) || ((satellite_last_year - model_first_year) > (model_last_year - model_first_year))) {\n",
    "  println(\"The range of years for each data set should be of the same length.\");\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "var model_years_range = (model_years.indexOf(model_first_year), model_years.indexOf(model_last_year))\n",
    "var satellite_years_range = (satellite_years.indexOf(satellite_first_year), satellite_years.indexOf(satellite_last_year))\n",
    "\n",
    "//Global variables\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var model_grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var model_grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var model_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var model_grids: RDD[Array[Double]] = sc.emptyRDD\n",
    "val rows = sc.parallelize(Array[Double]()).map(m => Vectors.dense(m))\n",
    "var model_summary: MultivariateStatisticalSummary = new RowMatrix(rows).computeColumnSummaryStatistics()\n",
    "var model_std :Array[Double] = new Array[Double](0)\n",
    "\n",
    "var satellite_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_grids: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_summary: MultivariateStatisticalSummary = new RowMatrix(rows).computeColumnSummaryStatistics()\n",
    "var satellite_std :Array[Double] = new Array[Double](0)\n",
    "\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var cellT :CellType = UByteCellType\n",
    "var mask_tile0 :Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile\n",
    "var satellite_cells_size :Long = 0\n",
    "var model_cells_size :Long = 0\n",
    "var t0 : Long = 0\n",
    "var t1 : Long = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to (de)serialize any structure into Array[Byte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serialize: (value: Any)Array[Byte]\n",
       "deserialize: (bytes: Array[Byte])Any\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def serialize(value: Any): Array[Byte] = {\n",
    "    val out_stream: ByteArrayOutputStream = new ByteArrayOutputStream()\n",
    "    val obj_out_stream = new ObjectOutputStream(out_stream)\n",
    "    obj_out_stream.writeObject(value)\n",
    "    obj_out_stream.close\n",
    "    out_stream.toByteArray\n",
    "}\n",
    "\n",
    "def deserialize(bytes: Array[Byte]): Any = {\n",
    "    val obj_in_stream = new ObjectInputStream(new ByteArrayInputStream(bytes))\n",
    "    val value = obj_in_stream.readObject\n",
    "    obj_in_stream.close\n",
    "    value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GeoTiffs\n",
    "\n",
    "Using GeoTrellis all GeoTiffs of a directory will be loaded into a RDD. Using the RDD, we extract a grid from the first file to lated store the Kmeans cluster_IDS, we build an Index for populate such grid and we filter out here all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 4206005010ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 2860612813767\n",
       "pattern = tif\n",
       "satellite_filepath = hdfs:///user/hadoop/avhrr/SOST\n",
       "model_filepath = hdfs:///user/hadoop/spring-index//BloomFinal\n",
       "t1 = 2864818818777\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2864818818777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "//Load Mask\n",
    "if (toBeMasked) {\n",
    "  val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mask_path).values\n",
    "  val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "  mask_tile0 = (mask_tiles_withIndex.filter(m => m._1==0).filter(m => !m._1.isNaN).values.collect())(0)\n",
    "}\n",
    "\n",
    "//Local variables\n",
    "val pattern: String = \"tif\"\n",
    "val satellite_filepath: String = satellite_path + satellite_dir\n",
    "val model_filepath: String = model_path + \"/\" + model_dir\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells is: 10115631                                                    \n",
      "Elapsed time: 60299547554ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 2866874788958\n",
       "satellite_grids_withIndex = MapPartitionsRDD[28] at map at <console>:103\n",
       "satellite_grids = MapPartitionsRDD[30] at values at <console>:106\n",
       "satellite_summary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@7ba42fb2\n",
       "satellite_std = [D@77b2ca2e\n",
       "satellite_grid0_index = MapPartitionsRDD[38] at flatMap at <console>:113\n",
       "satellite_cells_size = 10115631\n",
       "t1 = 2927174336512\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2927174336512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "if (satellite_rdd_offline_mode) {\n",
    "  satellite_grids_RDD = sc.objectFile(satellite_grid_path)\n",
    "} else {\n",
    "  //Lets load MODIS Singleband GeoTiffs and return RDD just with the tiles.\n",
    "  val satellite_geos_RDD = sc.hadoopGeoTiffRDD(satellite_filepath, pattern)\n",
    "  val satellite_tiles_RDD = satellite_geos_RDD.values\n",
    "\n",
    "  if (toBeMasked) {\n",
    "    val mask_tile_broad :Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "    satellite_grids_RDD = satellite_tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble().filter(_ != -1000))\n",
    "  } else {\n",
    "    satellite_grids_RDD = satellite_tiles_RDD.map(m => m.toArrayDouble())\n",
    "  }\n",
    "\n",
    "  //Store in HDFS\n",
    "  if (save_rdds) {\n",
    "      satellite_grids_RDD.saveAsObjectFile(satellite_grid_path)\n",
    "  }\n",
    "}\n",
    "val satellite_grids_withIndex = satellite_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "satellite_grids = satellite_grids_withIndex.filterByRange(satellite_years_range._1, satellite_years_range._2).values\n",
    "satellite_grids.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//Collect Stats:\n",
    "satellite_summary = Statistics.colStats(satellite_grids.map(m => Vectors.dense(m)))\n",
    "satellite_std = satellite_summary.variance.toArray.map(m => scala.math.sqrt(m))\n",
    "\n",
    "var satellite_grid0_index: RDD[Double] = satellite_grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m)\n",
    "satellite_cells_size = satellite_grid0_index.count().toInt\n",
    "println(\"Number of cells is: \" + satellite_cells_size)\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 115249064459ns                                                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 2931208269766\n",
       "model_grids_withIndex = MapPartitionsRDD[48] at map at <console>:160\n",
       "model_grids = MapPartitionsRDD[50] at values at <console>:163\n",
       "model_summary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@2123727a\n",
       "model_tile0_index = MapPartitionsRDD[58] at flatMap at <console>:170\n",
       "model_cells_size = 10115631\n",
       "t1 = 3046457334225\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3046457334225"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "if (model_rdd_offline_mode) {\n",
    "  model_grids_RDD = sc.objectFile(model_grid_path)\n",
    "  model_grid0 = sc.objectFile(model_grid0_path)\n",
    "  model_grid0_index = sc.objectFile(model_grid0_index_path)\n",
    "  val metadata = sc.sequenceFile(metadata_path, classOf[IntWritable], classOf[BytesWritable]).map(_._2.copyBytes()).collect()\n",
    "  projected_extent = deserialize(metadata(0)).asInstanceOf[ProjectedExtent]\n",
    "  num_cols_rows = (deserialize(metadata(1)).asInstanceOf[Int], deserialize(metadata(2)).asInstanceOf[Int])\n",
    "  cellT = deserialize(metadata(3)).asInstanceOf[CellType]\n",
    "} else {\n",
    "  val model_geos_RDD = sc.hadoopMultibandGeoTiffRDD(model_filepath, pattern)\n",
    "  val model_tiles_RDD = model_geos_RDD.values\n",
    "\n",
    "  //Retrieve the number of cols and rows of the Tile's grid\n",
    "  val tiles_withIndex = model_tiles_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "  val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "  num_cols_rows = (tile0.cols, tile0.rows)\n",
    "  cellT = tile0.cellType\n",
    "\n",
    "  //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "  val projected_extents_withIndex = model_geos_RDD.keys.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "  val band_numB: Broadcast[Int] = sc.broadcast(band_num)\n",
    "  if (toBeMasked) {\n",
    "    val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "    grids_RDD = model_tiles_RDD.map(m => m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "  } else {\n",
    "    grids_RDD = model_tiles_RDD.map(m => m.band(band_numB.value).toArrayDouble())\n",
    "  }\n",
    "\n",
    "  //Get Index for each Cell\n",
    "  val grids_withIndex = grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  if (toBeMasked) {\n",
    "    model_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => m._1 != -1000.0).map { case (v, i) => (i) }\n",
    "  } else {\n",
    "    model_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i) }\n",
    "  }\n",
    "\n",
    "  //Get the Tile's grid\n",
    "  model_grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "  //Lets filter out NaN\n",
    "  if (toBeMasked) {\n",
    "    model_grids_RDD = grids_RDD.map(m => m.filter(m => m != -1000.0))\n",
    "  } else {\n",
    "    model_grids_RDD = grids_RDD\n",
    "  }\n",
    "\n",
    "  //Store data in HDFS\n",
    "  model_grids_RDD.saveAsObjectFile(model_grid_path)\n",
    "  model_grid0.saveAsObjectFile(model_grid0_path)\n",
    "  model_grid0_index.saveAsObjectFile(model_grid0_index_path)\n",
    "\n",
    "  val writer: SequenceFile.Writer = SequenceFile.createWriter(conf,\n",
    "    Writer.file(metadata_path),\n",
    "    Writer.keyClass(classOf[IntWritable]),\n",
    "    Writer.valueClass(classOf[BytesWritable])\n",
    "  )\n",
    "\n",
    "  writer.append(new IntWritable(1), new BytesWritable(serialize(projected_extent)))\n",
    "  writer.append(new IntWritable(2), new BytesWritable(serialize(num_cols_rows._1)))\n",
    "  writer.append(new IntWritable(3), new BytesWritable(serialize(num_cols_rows._2)))\n",
    "  writer.append(new IntWritable(4), new BytesWritable(serialize(cellT)))\n",
    "  writer.hflush()\n",
    "  writer.close()\n",
    "}\n",
    "\n",
    "val model_grids_withIndex = model_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "model_grids = model_grids_withIndex.filterByRange(model_years_range._1, model_years_range._2).values\n",
    "model_grids.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//Collect Stats:\n",
    "model_summary = Statistics.colStats(model_grids.map(m => Vectors.dense(m)))\n",
    "//model_std = model_summary.variance.toArray.map(m => scala.math.sqrt(m))\n",
    "\n",
    "var model_tile0_index: RDD[Double] = model_grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m)\n",
    "model_cells_size = model_tile0_index.count().toInt\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 45:===================================================>    (24 + 2) / 26]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "satellite_cells_sizeB = Broadcast(50)\n",
       "satellite_mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@1f8ff47e\n",
       "sat_byColumnAndRow = MapPartitionsRDD[151] at flatMap at <console>:70\n",
       "satellite_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@1bf52d99\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@1bf52d99"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Satellite\n",
    "val satellite_cells_sizeB = sc.broadcast(satellite_cells_size)\n",
    "val satellite_mat: RowMatrix = new RowMatrix(satellite_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(satellite_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "\n",
    "// Split the matrix into one number per line.\n",
    "val sat_byColumnAndRow = satellite_mat.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val satellite_blockMatrix: BlockMatrix = new CoordinateMatrix(sat_byColumnAndRow).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 54:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "satellite_M_1_Gc = MapPartitionsRDD[157] at map at <console>:76\n",
       "satellite_M_1_Gc_RowM = org.apache.spark.mllib.linalg.distributed.RowMatrix@b7ebda\n",
       "sat_M_1_Gc_byColumnAndRow = MapPartitionsRDD[160] at flatMap at <console>:82\n",
       "satellite_M_1_Gc_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@17b9d603\n",
       "sat_matrix_Nt_1 = Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "satellite_M_Nt_1: org.apache.spark.rdd.RDD[org.apache.spa...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val satellite_M_1_Gc = sc.parallelize(Array[Vector](satellite_summary.mean)).map(m => Vectors.dense(m.toArray))\n",
    "val satellite_M_1_Gc_RowM: RowMatrix = new RowMatrix(satellite_M_1_Gc)\n",
    "val sat_M_1_Gc_byColumnAndRow = satellite_M_1_Gc_RowM.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val satellite_M_1_Gc_blockMatrix = new CoordinateMatrix(sat_M_1_Gc_byColumnAndRow).toBlockMatrix()\n",
    "\n",
    "val sat_matrix_Nt_1 = new Array[Double](satellite_grids.count().toInt)\n",
    "satellite_grids.unpersist(false)\n",
    "\n",
    "for (i <- 0 until sat_matrix_Nt_1.length)\n",
    "  sat_matrix_Nt_1(i) = 1\n",
    "val satellite_M_Nt_1 = sc.parallelize(sat_matrix_Nt_1).map(m => Vectors.dense(m))\n",
    "val satellite_M_Nt_1_RowM: RowMatrix = new RowMatrix(satellite_M_Nt_1)\n",
    "val sat_M_Nt_1_byColumnAndRow = satellite_M_Nt_1_RowM.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val satellite_M_Nt_1_blockMatrix = new CoordinateMatrix(sat_M_Nt_1_byColumnAndRow).toBlockMatrix()\n",
    "val satellite_M_Nt_Gc_blockMatrix = satellite_M_Nt_1_blockMatrix.multiply(satellite_M_1_Gc_blockMatrix)\n",
    "\n",
    "val Sc = satellite_blockMatrix.subtract(satellite_M_Nt_Gc_blockMatrix)\n",
    "Sc.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 56:====================================================>   (34 + 2) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_cells_sizeB = Broadcast(62)\n",
       "model_mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@69832c0e\n",
       "mod_byColumnAndRow = MapPartitionsRDD[192] at flatMap at <console>:70\n",
       "model_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@704ec3f2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@704ec3f2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Model\n",
    "val model_cells_sizeB = sc.broadcast(model_cells_size)\n",
    "val model_mat: RowMatrix = new RowMatrix(model_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(model_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "\n",
    "// Split the matrix into one number per line.\n",
    "val mod_byColumnAndRow = model_mat.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val model_blockMatrix: BlockMatrix = new CoordinateMatrix(mod_byColumnAndRow).transpose().toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 65:======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_M_1_Gc = MapPartitionsRDD[199] at map at <console>:70\n",
       "model_M_1_Gc_RowM = org.apache.spark.mllib.linalg.distributed.RowMatrix@6c3996a\n",
       "mod_M_1_Gc_byColumnAndRow = MapPartitionsRDD[202] at flatMap at <console>:76\n",
       "model_M_1_Gc_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@7e406292\n",
       "model_matrix_Nt_1 = Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model_M_Nt_1: org.apache.spark.rdd.RDD[org.apache.spark.mllib.lina...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//MC\n",
    "val model_M_1_Gc = sc.parallelize(Array[Vector](model_summary.mean)).map(m => Vectors.dense(m.toArray))\n",
    "val model_M_1_Gc_RowM: RowMatrix = new RowMatrix(model_M_1_Gc)\n",
    "val mod_M_1_Gc_byColumnAndRow = model_M_1_Gc_RowM.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val model_M_1_Gc_blockMatrix = new CoordinateMatrix(mod_M_1_Gc_byColumnAndRow).toBlockMatrix()\n",
    "\n",
    "val model_matrix_Nt_1 = new Array[Double](model_grids.count().toInt)\n",
    "model_grids.unpersist(false)\n",
    "\n",
    "for (i <- 0 until model_matrix_Nt_1.length)\n",
    "  model_matrix_Nt_1(i) = 1\n",
    "val model_M_Nt_1 = sc.parallelize(model_matrix_Nt_1).map(m => Vectors.dense(m))\n",
    "val model_M_Nt_1_RowM: RowMatrix = new RowMatrix(model_M_Nt_1)\n",
    "val mod_M_Nt_1_byColumnAndRow = model_M_Nt_1_RowM.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val model_M_Nt_1_blockMatrix = new CoordinateMatrix(mod_M_Nt_1_byColumnAndRow).toBlockMatrix()\n",
    "val model_M_Nt_Gc_blockMatrix = model_M_Nt_1_blockMatrix.multiply(model_M_1_Gc_blockMatrix)\n",
    "val model_M_Gc_Nt_blockMatrix = model_M_Nt_Gc_blockMatrix.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mc = org.apache.spark.mllib.linalg.distributed.BlockMatrix@30924d51\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@30924d51"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Mc = model_blockMatrix.subtract(model_M_Gc_Nt_blockMatrix)\n",
    "Mc.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 3) / 3]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 2 in stage 73.0 failed 4 times, most recent failure: Lost task 2.3 in stage 73.0 (TID 1497, 145.100.59.157, executor 6): java.lang.IllegalArgumentException: requirement failed: The last value of colPtrs must equal the number of elements. values.length: 42531, colPtrs.last: 26624\n",
       "\tat scala.Predef$.require(Predef.scala:224)\n",
       "\tat org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:590)\n",
       "\tat org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:618)\n",
       "\tat org.apache.spark.mllib.linalg.Matrices$.fromBreeze(Matrices.scala:995)\n",
       "\tat org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:378)\n",
       "\tat org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:365)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat scala.Predef$.require(Predef.scala:224)\n",
       "\tat org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:590)\n",
       "\tat org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:618)\n",
       "\tat org.apache.spark.mllib.linalg.Matrices$.fromBreeze(Matrices.scala:995)\n",
       "\tat org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:378)\n",
       "\tat org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:365)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
       "  at org.apache.spark.mllib.linalg.distributed.BlockMatrix.toLocalMatrix(BlockMatrix.scala:313)\n",
       "  ... 52 elided\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: The last value of colPtrs must equal the number of elements. values.length: 42531, colPtrs.last: 26624\n",
       "  at scala.Predef$.require(Predef.scala:224)\n",
       "  at org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:590)\n",
       "  at org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:618)\n",
       "  at org.apache.spark.mllib.linalg.Matrices$.fromBreeze(Matrices.scala:995)\n",
       "  at org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:378)\n",
       "  at org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:365)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Matrix Multiplication\n",
    "//val matrix_mul = model_blockMatrix.multiply(satellite_blockMatrix)\n",
    "val matrix_mul = Mc.multiply(Sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1024\n",
      "9879 1024\n",
      "9879 1024\n",
      "1 1024\n",
      "(7808,3892)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    println(Mc.numColBlocks + \" \" + Mc.colsPerBlock)\n",
    "    println(Mc.numRowBlocks + \" \" + Mc.rowsPerBlock)\n",
    "\n",
    "    println(Sc.numColBlocks + \" \" + Sc.colsPerBlock)\n",
    "    println(Sc.numRowBlocks + \" \" + Sc.rowsPerBlock)\n",
    "    print(num_cols_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 43:==============================================>         (15 + 2) / 18]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resRowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@23f7f254\n",
       "svd = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@7e2ba9a4,[1.7605833863003616E12,5.7253741451916664E10,4.151185732299026E10,3.668228226505551E10,3.4341034557407806E10,2.7715911601627808E10,2.7007353130962887E10,2.508487453875547E10,2.2488917091979183E10,2.102419109157361E10],-0.23613735396347657  0.20480673557119244    ... (10 total)\n",
       "-0.24143303278185568  -0.18397058442540593   ...\n",
       "-0.24161245427980513  -0.033384305061724545  ...\n",
       "-0.21216410577714592  -0.02129411618682879   ...\n",
       "-0.2665941903718298   0.14562276645638914    ...\n",
       "-0.2414032554256322   0.17619442814562367    ...\n",
       "-0.22890794179301527  0.08868200678850471    ...\n",
       "-0.23675724054458858  0.21530656760113956    ...\n",
       "-0.25469330957640596  0.18073722214789012    ...\n",
       "-0.2344741040161406   -0.05979106658508153   ...\n",
       "-0.2484656543027556   0.1589357811365968     ...\n",
       "-0.2551123790885418   0.2400536946014573     ...\n",
       "-0.2301680313794673   0.15426225035416138    ...\n",
       "-0.2549973419759259   -0.48412263098227865   ...\n",
       "-0.24175509242547435  -0.6567169251303179    ...\n",
       "-0.2449231919413422   -0.050894735393729004  ...\n",
       "-0.24831445303789787  -0.07073160138608185   ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//SVD\n",
    "val resRowMatrix: RowMatrix = new RowMatrix(matrix_mul.toIndexedRowMatrix().rows.sortBy(_.index).map(_.vector))\n",
    "\n",
    "val svd: SingularValueDecomposition[RowMatrix, Matrix] = resRowMatrix.computeSVD(10, true)\n",
    "\n",
    "val U: RowMatrix = svd.U // The U factor is a RowMatrix.\n",
    "val s: Vector = svd.s // The singular values are stored in a local dense vector.\n",
    "val V: Matrix = svd.V // The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GeoTiff with Kmeans cluster_IDs\n",
    "\n",
    "The Grid with the cluster IDs is stored in a SingleBand GeoTiff and uploaded to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign cluster ID to each grid cell and save the grid as SingleBand GeoTiff\n",
    "\n",
    "To assign the clusterID to each grid cell it is necessary to get the indices of gird cells they belong to. The process is not straight forward because the ArrayDouble used for the creation of each dense Vector does not contain the NaN values, therefore there is not a direct between the indices in the Tile's grid and the ones in **kmeans_res** (kmeans result).\n",
    "\n",
    "To join the two RDDS the knowledge was obtaing from a stackoverflow post on [how to perform basic joins of two rdd tables in spark using python](https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.ArrayIndexOutOfBoundsException\n",
       "Message: 10\n",
       "StackTrace: java.lang.ArrayIndexOutOfBoundsException: 10\n",
       "  at geotrellis.raster.DoubleConstantNoDataArrayTile.applyDouble(DoubleArrayTile.scala:125)\n",
       "  at geotrellis.raster.ArrayTile$class.getDouble(ArrayTile.scala:337)\n",
       "  at geotrellis.raster.DoubleArrayTile.getDouble(DoubleArrayTile.scala:24)\n",
       "  at geotrellis.raster.CroppedTile.getDouble(CroppedTile.scala:116)\n",
       "  at geotrellis.raster.CroppedTile.mutable(CroppedTile.scala:155)\n",
       "  at geotrellis.raster.CroppedTile.mutable(CroppedTile.scala:133)\n",
       "  at geotrellis.raster.CroppedTile.toArrayTile(CroppedTile.scala:125)\n",
       "  at geotrellis.raster.CroppedTile.toBytes(CroppedTile.scala:206)\n",
       "  at geotrellis.raster.io.geotiff.GeoTiffTile$.apply(GeoTiffTile.scala:103)\n",
       "  at geotrellis.raster.io.geotiff.package$GeoTiffTileMethods.toGeoTiffTile(package.scala:31)\n",
       "  at geotrellis.raster.io.geotiff.SinglebandGeoTiff.imageData(SinglebandGeoTiff.scala:40)\n",
       "  at geotrellis.raster.io.geotiff.writer.GeoTiffWriter.<init>(GeoTiffWriter.scala:62)\n",
       "  at geotrellis.raster.io.geotiff.writer.GeoTiffWriter$.write(GeoTiffWriter.scala:35)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells\n",
    "val multi_res = sc.parallelize(s.toArray).zipWithIndex().map{ case (v,i) => (i,v)}\n",
    "val multi_cell_pos = multi_res.join(model_grid0_index.zipWithIndex().map{ case (v,i) => (i,v)}).map{ case (k,(v,i)) => (v,i)}\n",
    "\n",
    "//Associate a Cluster_IDs to respective Grid_cell\n",
    "val grid_multi = model_grid0.map{ case (i, v) => if (v == -1000) (i,Double.NaN) else (i,v)}.leftOuterJoin(multi_cell_pos.map{ case (c,i) => (i.toLong, c)})\n",
    "\n",
    "//Convert all None to NaN\n",
    "val grid_multi_res = grid_multi.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get.toDouble)}\n",
    "\n",
    "//Define a Tile\n",
    "val corr_cells :Array[Double] = grid_multi_res.values.collect()\n",
    "\n",
    "val corr_cellsD = DoubleArrayTile(corr_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "val geoTif = new SinglebandGeoTiff(corr_cellsD, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "\n",
    "//Save to /tmp/\n",
    "GeoTiffWriter.write(geoTif, corr_tif_tmp)\n",
    "\n",
    "//Upload to HDFS\n",
    "var cmd = \"hadoop dfs -copyFromLocal -f \" + corr_tif_tmp + \" \" + corr_tif\n",
    "Process(cmd)!\n",
    "\n",
    "//Remove from /tmp/\n",
    "cmd = \"rm -fr \" + corr_tif_tmp\n",
    "Process(cmd)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Visualize results](plot_kmeans_clusters.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
