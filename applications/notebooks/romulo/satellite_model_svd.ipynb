{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD between a Model and Satellite data\n",
    "\n",
    "This notebook shows how to multiply two matrices and calculate SVD. Each matrix is created out a set of GeoTiffs for a series of years. Both matrices should have the same dimension.\n",
    "\n",
    "For demonstration we will use from a model (spring-index) and from a satellite (AVHRR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n",
    "\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile, Tile, UByteCellType}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.hadoop.io.SequenceFile.Writer\n",
    "import org.apache.hadoop.io.{SequenceFile, _}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import spire.syntax.cfor.cfor\n",
    "\n",
    "import scala.sys.process.Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of operation\n",
    "\n",
    "Here the user can define the mode of operation.\n",
    "* **rdd_offline_mode**: If false it means the notebook will create all data from scratch and store protected_extent and num_cols_rows into HDFS. Otherwise, these data structures are read from HDFS.\n",
    "\n",
    "It is also possible to define which directory of GeoTiffs is to be used and on which **band** to run Kmeans. The options are\n",
    "* **all** which are a multi-band (**8 bands**) GeoTiffs\n",
    "* Or choose single band ones:\n",
    "    0. Onset_Greenness_Increase\n",
    "    1. Onset_Greenness_Maximum\n",
    "    2. Onset_Greenness_Decrease\n",
    "    3. Onset_Greenness_Minimum\n",
    "    4. NBAR_EVI_Onset_Greenness_Minimum\n",
    "    5. NBAR_EVI_Onset_Greenness_Maximum\n",
    "    6. NBAR_EVI_Area\n",
    "    7. Dynamics_QC\n",
    "\n",
    "<span style=\"color:red\">Note that when using a range **kemans offline mode** is not possible and it will be reset to **online mode**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation setup\n",
    "<a id='mode_of_operation_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_rdd_offline_mode = true\n",
       "model_matrix_offline_mode = true\n",
       "satellite_rdd_offline_mode = true\n",
       "satellite_matrix_offline_mode = true\n",
       "model_path = hdfs:///user/hadoop/spring-index/\n",
       "model_dir = BloomGridmet\n",
       "model_band_num = 3\n",
       "satellite_path = hdfs:///user/hadoop/avhrr/\n",
       "satellite_dir = SOST4Km\n",
       "sat_band_num = 0\n",
       "out_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/\n",
       "satellite_first_year = 1989\n",
       "satellite_last_year = 2014\n",
       "model_first_year = 1989\n",
       "model_last_year = 2014\n",
       "modToBeMasked = true\n",
       "satToBeMasked = true\n",
       "mod_mask_path = hdfs:///user/hadoop/usa_mask_gridmet.tif\n",
       "sat_mask_path = hdfs:///user/hadoop/usa_mask_gridmet.tif\n",
       "matrix_mode = 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "save_r...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var model_rdd_offline_mode = true\n",
    "var model_matrix_offline_mode = true\n",
    "var satellite_rdd_offline_mode = true\n",
    "var satellite_matrix_offline_mode = true\n",
    "\n",
    "//Using spring-index model\n",
    "var model_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var model_dir = \"BloomGridmet\"\n",
    "var model_band_num = 3  //First band is 0\n",
    "\n",
    "//Using AVHRR Satellite data\n",
    "var satellite_path = \"hdfs:///user/hadoop/avhrr/\"\n",
    "var satellite_dir = \"SOST4Km\"\n",
    "//var satellite_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "//var satellite_dir = \"LeafGridmet\"\n",
    "var sat_band_num = 0  //First band is 0\n",
    "\n",
    "var out_path = \"hdfs:///user/pheno/svd/spark/\" + model_dir + satellite_dir + \"3/\"\n",
    "\n",
    "//Years between (inclusive) 1989 - 2014\n",
    "var satellite_first_year = 1989\n",
    "var satellite_last_year = 2014\n",
    "\n",
    "//Years between (inclusive) 1980 - 2015\n",
    "var model_first_year = 1989\n",
    "var model_last_year = 2014\n",
    "\n",
    "//Mask\n",
    "val modToBeMasked = true\n",
    "val satToBeMasked = true\n",
    "//val mask_path = \"hdfs:///user/hadoop/usa_mask_low.tif\"\n",
    "val mod_mask_path = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "val sat_mask_path = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "\n",
    "//Matrix Mode: 0 Normal, 1 SC, 2 SR\n",
    "val matrix_mode = 0\n",
    "\n",
    "val save_rdds = true\n",
    "val save_matrix = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">DON'T MODIFY ANY PIECE OF CODE FROM HERE ON!!!</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of operation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "[Stage 2:===============================================>   (1856 + 144) / 2000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1743994909_51, ugi=pheno (auth:SIMPLE)]]\n",
       "mod_mask_str = _mask\n",
       "sat_mask_str = _mask\n",
       "model_grid0_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/BloomGridmet_grid0\n",
       "model_grid0_index_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/BloomGridmet_grid0_index\n",
       "satellite_grid0_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/SOST4Km_grid0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "matrix_mode_str: String = \"\"\n",
       "satellite_grid0_index_pa...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/SOST4Km_grid0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Check offline modes\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "//Paths to store data structures for Offline runs\n",
    "var mod_mask_str = \"\"\n",
    "var sat_mask_str = \"\"\n",
    "if (modToBeMasked)\n",
    "  mod_mask_str = \"_mask\"\n",
    "if (satToBeMasked)\n",
    "  sat_mask_str = \"_mask\"\n",
    "\n",
    "val matrix_mode_str = matrix_mode match {\n",
    "  case 1 => \"_Sc_Mc\"\n",
    "  case 2 => \"_Sr_Mr\"\n",
    "  case _ => \"\"\n",
    "}\n",
    "\n",
    "var model_grid0_path = out_path + model_dir + \"_grid0\"\n",
    "var model_grid0_index_path = out_path + model_dir + \"_grid0_index\"\n",
    "var satellite_grid0_path = out_path + satellite_dir + \"_grid0\"\n",
    "var satellite_grid0_index_path = out_path + satellite_dir + \"_grid0_index\"\n",
    "\n",
    "var model_grid_path = out_path + model_dir + \"_grid\"\n",
    "var satellite_grid_path = out_path + satellite_dir + \"_grid\"\n",
    "var model_matrix_path = out_path + model_dir + \"_matrix\"\n",
    "var satellite_matrix_path = out_path + satellite_dir + \"_matrix\"\n",
    "var metadata_path = out_path + model_dir + \"_metadata\"\n",
    "\n",
    "var sc_path = out_path + model_dir + \"_sc\"\n",
    "var mc_path = out_path + model_dir + \"_mc\"\n",
    "\n",
    "val model_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(model_grid_path))\n",
    "val model_matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(model_matrix_path))\n",
    "val satellite_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(satellite_grid_path))\n",
    "val satellite_matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(satellite_matrix_path))\n",
    "\n",
    "if (model_rdd_offline_mode != model_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + model_rdd_offline_exists.toString())\n",
    "  model_rdd_offline_mode = model_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (model_matrix_offline_mode != model_matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + model_matrix_offline_exists.toString())\n",
    "  model_matrix_offline_mode = model_matrix_offline_exists\n",
    "}\n",
    "\n",
    "var model_skip_rdd = false\n",
    "if (model_matrix_offline_exists) {\n",
    "    println(\"Since we have a matrix, the load of the grids RDD will be skipped!!!\")\n",
    "    model_skip_rdd = true\n",
    "}\n",
    "\n",
    "if (satellite_rdd_offline_mode != satellite_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + satellite_rdd_offline_exists.toString())\n",
    "  satellite_rdd_offline_mode = satellite_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (satellite_matrix_offline_mode != satellite_matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + satellite_matrix_offline_exists.toString())\n",
    "  satellite_matrix_offline_mode = satellite_matrix_offline_exists\n",
    "}\n",
    "\n",
    "var satellite_skip_rdd = false\n",
    "if (satellite_matrix_offline_exists) {\n",
    "    println(\"Since we have a matrix, the load of the grids RDD will be skipped!!!\")\n",
    "    satellite_skip_rdd = true\n",
    "}\n",
    "\n",
    "//Years\n",
    "val model_years = 1989 to 2014\n",
    "val satellite_years = 1989 to 2014\n",
    "\n",
    "if (!satellite_years.contains(satellite_first_year) || !(satellite_years.contains(satellite_last_year))) {\n",
    "  println(\"Invalid range of years for \" + satellite_dir + \". I should be between \" + satellite_first_year + \" and \" + satellite_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "if (!model_years.contains(model_first_year) || !(model_years.contains(model_last_year))) {\n",
    "  println(\"Invalid range of years for \" + model_dir + \". I should be between \" + model_first_year + \" and \" + model_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "if ( ((satellite_last_year - model_first_year) > (model_last_year - model_first_year)) || ((satellite_last_year - model_first_year) > (model_last_year - model_first_year))) {\n",
    "  println(\"The range of years for each data set should be of the same length.\");\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "var model_years_range = (model_years.indexOf(model_first_year), model_years.indexOf(model_last_year))\n",
    "var satellite_years_range = (satellite_years.indexOf(satellite_first_year), satellite_years.indexOf(satellite_last_year))\n",
    "\n",
    "//Global variables\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var model_grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var model_grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var mod_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var model_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var model_grids: RDD[Array[Double]] = sc.emptyRDD\n",
    "val rows = sc.parallelize(Array[Double]()).map(m => Vectors.dense(m))\n",
    "var model_summary: MultivariateStatisticalSummary = new RowMatrix(rows).computeColumnSummaryStatistics()\n",
    "var model_std :Array[Double] = new Array[Double](0)\n",
    "\n",
    "var satellite_grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var satellite_grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var sat_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_grids: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_summary: MultivariateStatisticalSummary = new RowMatrix(rows).computeColumnSummaryStatistics()\n",
    "var satellite_std :Array[Double] = new Array[Double](0)\n",
    "\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var cellT :CellType = UByteCellType\n",
    "var mod_mask_tile0 :Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile\n",
    "var sat_mask_tile0 :Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile\n",
    "var satellite_cells_size :Long = 0\n",
    "var model_cells_size :Long = 0\n",
    "var t0 : Long = 0\n",
    "var t1 : Long = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to (de)serialize any structure into Array[Byte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serialize: (value: Any)Array[Byte]\n",
       "deserialize: (bytes: Array[Byte])Any\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def serialize(value: Any): Array[Byte] = {\n",
    "    val out_stream: ByteArrayOutputStream = new ByteArrayOutputStream()\n",
    "    val obj_out_stream = new ObjectOutputStream(out_stream)\n",
    "    obj_out_stream.writeObject(value)\n",
    "    obj_out_stream.close\n",
    "    out_stream.toByteArray\n",
    "}\n",
    "\n",
    "def deserialize(bytes: Array[Byte]): Any = {\n",
    "    val obj_in_stream = new ObjectInputStream(new ByteArrayInputStream(bytes))\n",
    "    val value = obj_in_stream.readObject\n",
    "    obj_in_stream.close\n",
    "    value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GeoTiffs\n",
    "\n",
    "Using GeoTrellis all GeoTiffs of a directory will be loaded into a RDD. Using the RDD, we extract a grid from the first file to lated store the Kmeans cluster_IDS, we build an Index for populate such grid and we filter out here all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1910481205ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 18063168846811\n",
       "pattern = tif\n",
       "satellite_filepath = hdfs:///user/hadoop/avhrr/SOST4Km\n",
       "model_filepath = hdfs:///user/hadoop/spring-index//BloomGridmet\n",
       "t1 = 18065079328016\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18065079328016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "//Load Mask\n",
    "if (modToBeMasked) {\n",
    "  val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mod_mask_path).values\n",
    "  val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  mod_mask_tile0 = (mask_tiles_withIndex.filter(m => m._1 == 0).filter(m => !m._1.isNaN).values.collect()) (0)\n",
    "}\n",
    "\n",
    "if (satToBeMasked) {\n",
    "  val mask_tiles_RDD = sc.hadoopGeoTiffRDD(sat_mask_path).values\n",
    "  val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  sat_mask_tile0 = (mask_tiles_withIndex.filter(m => m._1 == 0).filter(m => !m._1.isNaN).values.collect()) (0)\n",
    "}\n",
    "\n",
    "//Local variables\n",
    "val pattern: String = \"tif\"\n",
    "val satellite_filepath: String = satellite_path + satellite_dir\n",
    "val model_filepath: String = model_path + \"/\" + model_dir\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (2 + 2) / 4]Number of cells is: 483850\n",
      "Elapsed time: 8527291413ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 18068181904290\n",
       "satellite_grids_withIndex = MapPartitionsRDD[61] at map at <console>:145\n",
       "satellite_grids = MapPartitionsRDD[63] at values at <console>:148\n",
       "satellite_summary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@8c914f1\n",
       "satellite_std = [D@7051efbd\n",
       "satellite_cells_size = 483850\n",
       "t1 = 18076709195703\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18076709195703"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "if (satellite_rdd_offline_mode) {\n",
    "  satellite_grids_RDD = sc.objectFile(satellite_grid_path)\n",
    "  satellite_grid0 = sc.objectFile(satellite_grid0_path)\n",
    "  satellite_grid0_index = sc.objectFile(satellite_grid0_index_path)\n",
    "} else {\n",
    "  //Lets load MODIS Singleband GeoTiffs and return RDD just with the tiles.\n",
    "  if (sat_band_num == 0) {\n",
    "    val satellite_geos_RDD = sc.hadoopGeoTiffRDD(satellite_filepath, pattern)\n",
    "    val satellite_tiles_RDD = satellite_geos_RDD.values\n",
    "\n",
    "    if (satToBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(sat_mask_tile0)\n",
    "      sat_grids_RDD = satellite_tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -2000).toArrayDouble().map(m => if (m == -1000) Double.NaN else m))\n",
    "    } else {\n",
    "      sat_grids_RDD = satellite_tiles_RDD.map(m => m.toArrayDouble())\n",
    "    }\n",
    "  } else {\n",
    "    val satellite_geos_RDD = sc.hadoopMultibandGeoTiffRDD(satellite_filepath, pattern)\n",
    "    val satellite_tiles_RDD = satellite_geos_RDD.values\n",
    "\n",
    "    val band_numB: Broadcast[Int] = sc.broadcast(sat_band_num)\n",
    "    if (satToBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(sat_mask_tile0)\n",
    "      sat_grids_RDD = satellite_tiles_RDD.map(m => m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -2000).toArrayDouble().map(m => if (m == -1000) Double.NaN else m))\n",
    "    } else {\n",
    "      sat_grids_RDD = satellite_tiles_RDD.map(m => m.band(band_numB.value).toArrayDouble())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  //Get Index for each Cell\n",
    "  val grids_withIndex = sat_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  if (satToBeMasked) {\n",
    "    satellite_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => m._1 != -2000.0).map { case (v, i) => (i) }\n",
    "  } else {\n",
    "    satellite_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i) }\n",
    "  }\n",
    "\n",
    "  //Get the Tile's grid\n",
    "  satellite_grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i, v) }\n",
    "\n",
    "  //Lets filter out NaN\n",
    "  if (satToBeMasked) {\n",
    "    satellite_grids_RDD = sat_grids_RDD.map(m => m.filter(m => m != -2000.0))\n",
    "  } else {\n",
    "    satellite_grids_RDD = sat_grids_RDD\n",
    "  }\n",
    "\n",
    "  //Store in HDFS\n",
    "  if (save_rdds) {\n",
    "    satellite_grids_RDD.saveAsObjectFile(satellite_grid_path)\n",
    "    satellite_grid0.saveAsObjectFile(satellite_grid0_path)\n",
    "    satellite_grid0_index.saveAsObjectFile(satellite_grid0_index_path)\n",
    "  }\n",
    "}\n",
    "val satellite_grids_withIndex = satellite_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "satellite_grids = satellite_grids_withIndex.filterByRange(satellite_years_range._1, satellite_years_range._2).values\n",
    "satellite_grids.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//Collect Stats:\n",
    "satellite_summary = Statistics.colStats(satellite_grids.map(m => Vectors.dense(m)))\n",
    "satellite_std = satellite_summary.variance.toArray.map(m => scala.math.sqrt(m))\n",
    "\n",
    "satellite_cells_size = satellite_grid0_index.count().toInt\n",
    "println(\"Number of cells is: \" + satellite_cells_size)\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 30387128751ns                                                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 18080004553999\n",
       "model_grids_withIndex = MapPartitionsRDD[103] at map at <console>:186\n",
       "model_grids = MapPartitionsRDD[105] at values at <console>:189\n",
       "model_summary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@64fb0458\n",
       "model_tile0_index = MapPartitionsRDD[113] at flatMap at <console>:196\n",
       "model_cells_size = 483850\n",
       "t1 = 18110391682750\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18110391682750"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "if (model_rdd_offline_mode) {\n",
    "  model_grids_RDD = sc.objectFile(model_grid_path)\n",
    "  model_grid0 = sc.objectFile(model_grid0_path)\n",
    "  model_grid0_index = sc.objectFile(model_grid0_index_path)\n",
    "  val metadata = sc.sequenceFile(metadata_path, classOf[IntWritable], classOf[BytesWritable]).map(_._2.copyBytes()).collect()\n",
    "  projected_extent = deserialize(metadata(0)).asInstanceOf[ProjectedExtent]\n",
    "  num_cols_rows = (deserialize(metadata(1)).asInstanceOf[Int], deserialize(metadata(2)).asInstanceOf[Int])\n",
    "  cellT = deserialize(metadata(3)).asInstanceOf[CellType]\n",
    "} else {\n",
    "  if (model_band_num == 0) {\n",
    "    val model_geos_RDD = sc.hadoopGeoTiffRDD(model_filepath, pattern)\n",
    "    val model_tiles_RDD = model_geos_RDD.values\n",
    "\n",
    "    //Retrieve the number of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = model_tiles_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "    val projected_extents_withIndex = model_geos_RDD.keys.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "    projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    if (modToBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mod_mask_tile0)\n",
    "      mod_grids_RDD = model_tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "    } else {\n",
    "      mod_grids_RDD = model_tiles_RDD.map(m => m.toArrayDouble())\n",
    "    }\n",
    "  } else {\n",
    "    val model_geos_RDD = sc.hadoopMultibandGeoTiffRDD(model_filepath, pattern)\n",
    "    val model_tiles_RDD = model_geos_RDD.values\n",
    "\n",
    "    //Retrieve the number of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = model_tiles_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "    val projected_extents_withIndex = model_geos_RDD.keys.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "    projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    val band_numB: Broadcast[Int] = sc.broadcast(model_band_num)\n",
    "    if (modToBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mod_mask_tile0)\n",
    "      mod_grids_RDD = model_tiles_RDD.map(m => m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "    } else {\n",
    "      mod_grids_RDD = model_tiles_RDD.map(m => m.band(band_numB.value).toArrayDouble())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  //Get Index for each Cell\n",
    "  val grids_withIndex = mod_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  if (modToBeMasked) {\n",
    "    model_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => m._1 != -1000.0).map { case (v, i) => (i) }\n",
    "  } else {\n",
    "    model_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i) }\n",
    "  }\n",
    "\n",
    "  //Get the Tile's grid\n",
    "  model_grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "  //Lets filter out NaN\n",
    "  if (modToBeMasked) {\n",
    "    model_grids_RDD = mod_grids_RDD.map(m => m.filter(m => m != -1000.0))\n",
    "  } else {\n",
    "    model_grids_RDD = mod_grids_RDD\n",
    "  }\n",
    "\n",
    "  //Store data in HDFS\n",
    "  model_grids_RDD.saveAsObjectFile(model_grid_path)\n",
    "  model_grid0.saveAsObjectFile(model_grid0_path)\n",
    "  model_grid0_index.saveAsObjectFile(model_grid0_index_path)\n",
    "\n",
    "  val writer: SequenceFile.Writer = SequenceFile.createWriter(conf,\n",
    "    Writer.file(metadata_path),\n",
    "    Writer.keyClass(classOf[IntWritable]),\n",
    "    Writer.valueClass(classOf[BytesWritable])\n",
    "  )\n",
    "\n",
    "  writer.append(new IntWritable(1), new BytesWritable(serialize(projected_extent)))\n",
    "  writer.append(new IntWritable(2), new BytesWritable(serialize(num_cols_rows._1)))\n",
    "  writer.append(new IntWritable(3), new BytesWritable(serialize(num_cols_rows._2)))\n",
    "  writer.append(new IntWritable(4), new BytesWritable(serialize(cellT)))\n",
    "  writer.hflush()\n",
    "  writer.close()\n",
    "}\n",
    "\n",
    "val model_grids_withIndex = model_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "model_grids = model_grids_withIndex.filterByRange(model_years_range._1, model_years_range._2).values\n",
    "//model_grids.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//Collect Stats:\n",
    "model_summary = Statistics.colStats(model_grids.map(m => Vectors.dense(m)))\n",
    "//model_std = model_summary.variance.toArray.map(m => scala.math.sqrt(m))\n",
    "\n",
    "var model_tile0_index: RDD[Double] = model_grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m)\n",
    "model_cells_size = model_tile0_index.count().toInt\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "satellite_cells_sizeB = Broadcast(37)\n",
       "satellite_mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@6c82fb88\n",
       "sat_byColumnAndRow = MapPartitionsRDD[119] at flatMap at <console>:67\n",
       "satellite_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@12716cbc\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@12716cbc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Satellite\n",
    "val satellite_cells_sizeB = sc.broadcast(satellite_cells_size)\n",
    "val satellite_mat: RowMatrix = new RowMatrix(satellite_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(satellite_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "\n",
    "// Split the matrix into one number per line.\n",
    "val sat_byColumnAndRow = satellite_mat.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val satellite_blockMatrix: BlockMatrix = new CoordinateMatrix(sat_byColumnAndRow).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 64:===================================================>(1998 + 2) / 2000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sc_exists = false\n",
       "Sc = org.apache.spark.mllib.linalg.distributed.BlockMatrix@4f78186e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@4f78186e"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//SC\n",
    "val sc_exists = fs.exists(new org.apache.hadoop.fs.Path(sc_path))\n",
    "var Sc :BlockMatrix = null\n",
    "if (sc_exists) {\n",
    "  val rdd_indexed_rows :RDD[IndexedRow]= sc.objectFile(sc_path)\n",
    "  Sc = new IndexedRowMatrix(rdd_indexed_rows).toBlockMatrix()\n",
    "} else {\n",
    "  val satellite_M_1_Gc = sc.parallelize(Array[Vector](satellite_summary.mean)).map(m => Vectors.dense(m.toArray))\n",
    "  val satellite_M_1_Gc_RowM: RowMatrix = new RowMatrix(satellite_M_1_Gc)\n",
    "  val sat_M_1_Gc_byColumnAndRow = satellite_M_1_Gc_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val satellite_M_1_Gc_blockMatrix = new CoordinateMatrix(sat_M_1_Gc_byColumnAndRow).toBlockMatrix()\n",
    "\n",
    "  val sat_matrix_Nt_1 = new Array[Double](satellite_grids.count().toInt)\n",
    "  //satellite_grids.unpersist(false)\n",
    "  for (i <- 0 until sat_matrix_Nt_1.length)\n",
    "    sat_matrix_Nt_1(i) = 1\n",
    "  val satellite_M_Nt_1 = sc.parallelize(sat_matrix_Nt_1).map(m => Vectors.dense(m))\n",
    "  val satellite_M_Nt_1_RowM: RowMatrix = new RowMatrix(satellite_M_Nt_1)\n",
    "  val sat_M_Nt_1_byColumnAndRow = satellite_M_Nt_1_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val satellite_M_Nt_1_blockMatrix = new CoordinateMatrix(sat_M_Nt_1_byColumnAndRow).toBlockMatrix()\n",
    "  val satellite_M_Nt_Gc_blockMatrix = satellite_M_Nt_1_blockMatrix.multiply(satellite_M_1_Gc_blockMatrix)\n",
    "\n",
    "  //Sc = satellite_blockMatrix.subtract(satellite_M_Nt_Gc_blockMatrix)\n",
    "  val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = satellite_blockMatrix.toIndexedRowMatrix().rows.map(m => (m.index, m.vector.toArray)).join(satellite_M_Nt_Gc_blockMatrix.toCoordinateMatrix().toIndexedRowMatrix().rows.map(m => (m.index, m.vector.toArray)))\n",
    "  Sc = (new CoordinateMatrix(joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1-m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))).toBlockMatrix() \n",
    "\n",
    "  //save to disk\n",
    "  Sc.toIndexedRowMatrix().rows.saveAsObjectFile(sc_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 66:======================================================> (36 + 1) / 37]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_cells_sizeB = Broadcast(61)\n",
       "model_mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@5ea3d6a5\n",
       "mod_byColumnAndRow = MapPartitionsRDD[180] at flatMap at <console>:67\n",
       "model_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@4ae133b7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@4ae133b7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Model\n",
    "val model_cells_sizeB = sc.broadcast(model_cells_size)\n",
    "val model_mat: RowMatrix = new RowMatrix(model_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(model_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "\n",
    "// Split the matrix into one number per line.\n",
    "val mod_byColumnAndRow = model_mat.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val model_blockMatrix: BlockMatrix = new CoordinateMatrix(mod_byColumnAndRow).transpose().toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 99:==================================================>(1990 + 10) / 2000]877 + 146) / 2000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mc_exists = false\n",
       "Mc = org.apache.spark.mllib.linalg.distributed.BlockMatrix@42d65355\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@42d65355"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//MC\n",
    "val mc_exists = fs.exists(new org.apache.hadoop.fs.Path(mc_path))\n",
    "var Mc :BlockMatrix = null\n",
    "if (mc_exists) {\n",
    "  val rdd_indexed_rows :RDD[IndexedRow]= sc.objectFile(mc_path)\n",
    "  Mc = new IndexedRowMatrix(rdd_indexed_rows).toBlockMatrix()\n",
    "} else {\n",
    "  val model_M_1_Gc = sc.parallelize(Array[Vector](model_summary.mean)).map(m => Vectors.dense(m.toArray))\n",
    "  val model_M_1_Gc_RowM: RowMatrix = new RowMatrix(model_M_1_Gc)\n",
    "  val mod_M_1_Gc_byColumnAndRow = model_M_1_Gc_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val model_M_1_Gc_blockMatrix = new CoordinateMatrix(mod_M_1_Gc_byColumnAndRow).toBlockMatrix()\n",
    "\n",
    "  val model_matrix_Nt_1 = new Array[Double](model_grids.count().toInt)\n",
    "  //model_grids.unpersist(false)\n",
    "\n",
    "  for (i <- 0 until model_matrix_Nt_1.length)\n",
    "    model_matrix_Nt_1(i) = 1\n",
    "  val model_M_Nt_1 = sc.parallelize(model_matrix_Nt_1).map(m => Vectors.dense(m))\n",
    "  val model_M_Nt_1_RowM: RowMatrix = new RowMatrix(model_M_Nt_1)\n",
    "  val mod_M_Nt_1_byColumnAndRow = model_M_Nt_1_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val model_M_Nt_1_blockMatrix = new CoordinateMatrix(mod_M_Nt_1_byColumnAndRow).toBlockMatrix()\n",
    "  val model_M_Nt_Gc_blockMatrix = model_M_Nt_1_blockMatrix.multiply(model_M_1_Gc_blockMatrix)\n",
    "  val model_M_Gc_Nt_blockMatrix = model_M_Nt_Gc_blockMatrix.transpose\n",
    "  \n",
    "  //Mc = model_blockMatrix.subtract(model_M_Gc_Nt_blockMatrix)\n",
    "  val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = model_blockMatrix.toIndexedRowMatrix().rows.map( m => (m.index, m.vector.toArray)).join(model_M_Gc_Nt_blockMatrix.toIndexedRowMatrix().rows.map(m => (m.index, m.vector.toArray)))\n",
    "  Mc = (new CoordinateMatrix(joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1-m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))).toBlockMatrix()\n",
    "\n",
    "  //save to disk\n",
    "  Mc.toIndexedRowMatrix().rows.saveAsObjectFile(mc_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sr: org.apache.spark.mllib.linalg.distributed.BlockMatrix = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//SR\n",
    "var Sr :BlockMatrix = null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr: org.apache.spark.mllib.linalg.distributed.BlockMatrix = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//MR\n",
    "var Mr :BlockMatrix = null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 103:================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix_mul = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@9bae68b\n",
       "n_components = 3\n",
       "n_components = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Matrix Multiplication\n",
    "var matrix_mul :IndexedRowMatrix = null\n",
    "var n_components :Long = 0\n",
    "\n",
    "//Normal Matrix\n",
    "if (matrix_mode == 0) {\n",
    "  model_blockMatrix.persist(StorageLevel.DISK_ONLY)\n",
    "  satellite_blockMatrix.persist(StorageLevel.DISK_ONLY)\n",
    "  matrix_mul = model_blockMatrix.toIndexedRowMatrix().multiply(satellite_blockMatrix.toLocalMatrix())\n",
    "  n_components = List(model_blockMatrix.numRows(), model_blockMatrix.numCols(), satellite_blockMatrix.numRows(), satellite_blockMatrix.numCols()).min\n",
    "}\n",
    "\n",
    "//SC Matrix\n",
    "if (matrix_mode == 1) {\n",
    "  Mc.persist(StorageLevel.DISK_ONLY)\n",
    "  Sc.persist(StorageLevel.DISK_ONLY)\n",
    "  matrix_mul = Mc.toIndexedRowMatrix().multiply(Sc.toLocalMatrix())\n",
    "  n_components = List(Mc.numRows(), Mc.numCols(), Sc.numRows(), Sc.numCols()).min\n",
    "}\n",
    "\n",
    "//SR Matrix\n",
    "if (matrix_mode == 2) {\n",
    "  Mr.persist(StorageLevel.DISK_ONLY)\n",
    "  Sr.persist(StorageLevel.DISK_ONLY)\n",
    "  matrix_mul = Mr.toIndexedRowMatrix().multiply(Sr.toLocalMatrix())\n",
    "  n_components = List(Mr.numRows(), Mr.numCols(), Sr.numRows(), Sr.numCols()).min\n",
    "}\n",
    "\n",
    "n_components = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to save GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u_geotiff_hdfs_paths = Array(hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/u_tiffs/svd_u_0_3.tif, hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/u_tiffs/svd_u_1_3.tif, hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/u_tiffs/svd_u_2_3.tif)\n",
       "u_geotiff_tmp_paths = Array(/tmp/svd_u_0_3.tif, /tmp/svd_u_1_3.tif, /tmp/svd_u_2_3.tif)\n",
       "v_geotiff_hdfs_paths = Array(hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/v_tiffs/svd_v_0_3.tif, hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/v_tiffs/svd_v_1_3.tif, hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/v_tiffs/svd_v_2_3.tif)\n",
       "v_geotiff_tmp_paths = Array(/tmp/svd_v_0_3.tif, /tmp/svd_v_1_3.tif, /tmp/svd_v_2_3.tif)\n",
       "cmd = hadoop dfs -mkdir hdfs:///user/pheno/svd/spark/Bloo...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hadoop dfs -mkdir hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/v_tiffs/"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Paths to save GeoTiffs\n",
    "var u_geotiff_hdfs_paths :Array[String] = Array.fill[String](n_components.toInt)(\"\")\n",
    "var u_geotiff_tmp_paths :Array[String] = Array.fill[String](n_components.toInt)(\"\")\n",
    "var v_geotiff_hdfs_paths :Array[String] = Array.fill[String](n_components.toInt)(\"\")\n",
    "var v_geotiff_tmp_paths :Array[String] = Array.fill[String](n_components.toInt)(\"\")\n",
    "\n",
    "//Create dirs in HDFS\n",
    "var cmd = \"hadoop dfs -mkdir \" + out_path + \"u_tiffs/\"\n",
    "Process(cmd)!\n",
    "\n",
    "cmd = \"hadoop dfs -mkdir \" + out_path + \"v_tiffs/\"\n",
    "Process(cmd)!\n",
    "\n",
    "cfor(0)(_ < n_components, _ + 1) { k =>\n",
    "  u_geotiff_hdfs_paths(k) =  out_path + \"u_tiffs/svd_u_\" + k + \"_\" + n_components + matrix_mode_str + \".tif\"\n",
    "  u_geotiff_tmp_paths(k) = \"/tmp/svd_u_\" + k + \"_\" + n_components + matrix_mode_str + \".tif\"\n",
    "  if (fs.exists(new org.apache.hadoop.fs.Path(u_geotiff_hdfs_paths(k)))) {\n",
    "    println(\"There is already a GeoTiff with the path: \" + u_geotiff_hdfs_paths(k) + \". Please make either a copy or move it to another location, otherwise, it will be over-written.\")\n",
    "  }\n",
    "    \n",
    "  v_geotiff_hdfs_paths(k) =  out_path + \"v_tiffs/svd_v_\" + k + \"_\" + n_components + matrix_mode_str + \".tif\"\n",
    "  v_geotiff_tmp_paths(k) = \"/tmp/svd_v_\" + k + \"_\" + n_components + matrix_mode_str + \".tif\"\n",
    "  if (fs.exists(new org.apache.hadoop.fs.Path(v_geotiff_hdfs_paths(k)))) {\n",
    "    println(\"There is already a GeoTiff with the path: \" + v_geotiff_hdfs_paths(k) + \". Please make either a copy or move it to another location, otherwise, it will be over-written.\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 208:===================================================>   (41 + 3) / 44]                 (1175 + 144) / 2000]============================>                 (1285 + 144) / 2000][Stage 171:=========================================>       (1693 + 144) / 2000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "svd = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0014563987593792294    -0.0024703353837232...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@15076327,[1.2923542090759692E11,3.083065081621133E8,2.3033528005015823E8],-8.051231053858589E-21   2.0314743544613485E-20   -5.4470929547450735E-15  \n",
       "0.0016067529764857253    -1.9615361019119037E-4   2.4440313167959087E-4    \n",
       "0.0015946701465578213    4.594990813357084E-4     -1.27131792647146E-4     \n",
       "0.0015257591750188495    -1.1516506137695353E-4   4.757806412100489E-5     \n",
       "0.0015467867972772604    3.5001400383543143E-4    1.7776380768845697E-4    \n",
       "0.0015784500023011527    7.019261759657722E-4     2.9146377109652783E-4    \n",
       "0.0014563987593792294    -0.002470335383723202    0.0011291674754263425    \n",
       "5.223101216382444E-21    -1.2841205751705455E-20  3.4472675173625226E-15   \n",
       "-7.022981616856529E-21   1.6966037567050023E-20   -4.5731321646951146E-15  \n",
       "6.213146888095172E-21    -1.4769053932205133E-20  3.994921379437276E-15    \n",
       "-1.4005988581398866E-20  3.408154538323598E-20    -8.979296763901108E-15   \n",
       "-1.7837432191432697E-20  4.596625192668947E-20    -1.2113462720410842E-14  \n",
       "5.39331505207417E-4      5.27054361484347E-4      -1.880142629143662E-4    \n",
       "0.0015993341024977147    3.082198768346865E-4     1.876105791474283E-4     \n",
       "0.0015669487990077101    9.202790769520701E-5     2.6008982832244365E-4    \n",
       "0.0015757644465683232    1.0124605257879039E-4    2.688855133188503E-4     \n",
       "0.0015681841489847857    -3.0095040688783935E-5   2.5728656455293176E-4    \n",
       "6.128322208809462E-5     7.923676096598497E-4     -1.041780467531902E-4    \n",
       "-4.964597039883808E-22   1.2223188712085074E-21   -3.239627191115144E-16   \n",
       "-1.0484217870134873E-20  2.5735547353161487E-20   -6.909611679313999E-15   \n",
       "1.729717392793588E-20    -4.266001966508867E-20   1.1475651821855447E-14   \n",
       "-1.027284292821337E-20   2.5277715626228038E-20   -6.824951663049712E-15   \n",
       "4.985724915731599E-21    -1.2587601906655407E-20  3.324398423522689E-15    \n",
       "-1.4062110522245336E-20  3.4828460533990054E-20   -9.547168894562164E-15   \n",
       "0.0015778237987069013    3.0631033403236537E-4    1.8965435595771126E-4    \n",
       "0.001589933774543279     1.2121856871052628E-4    2.6911169203673786E-4    \n",
       "0.0016021767678790681    -1.2411093488719075E-4   -3.030647919490567E-5    \n",
       "3.062740111579499E-4     0.0013703832109195017    -0.001468767480132339    \n",
       "1.2088449894033151E-20   -2.9910117498522954E-20  7.957895343567636E-15    \n",
       "-1.8275036376608112E-20  4.736294496033747E-20    -1.2586711886207611E-14  \n",
       "1.3976758041064385E-20   -3.3247522674257666E-20  8.88575844270954E-15     \n",
       "-1.0951193296875823E-20  2.7705476875326367E-20   -7.318931640565459E-15   \n",
       "1.8514465600990095E-20   -4.620265891396146E-20   1.2237479044640504E-14   \n",
       "-5.211850327535444E-21   1.2720425348064891E-20   -3.4475762086876784E-15  \n",
       "2.0212303613777296E-20   -4.92560814745704E-20    1.3024602453591828E-14   \n",
       "-2.4794609574863248E-21  6.063883085448483E-21    -1.5910728107854143E-15  \n",
       "7.065000897339187E-23    -1.6720900742460984E-22  4.420246254595048E-17    \n",
       "4.064354350538228E-21    -9.82910119379099E-21    2.666195747420295E-15    \n",
       "-2.1841827969801026E-20  5.1193358404106017E-20   -1.3525523647695631E-14  \n",
       "9.965890293212886E-21    -2.5983200515257097E-20  6.9731613002621965E-15   \n",
       "-5.432140109238481E-21   1.3087937313015363E-20   -3.511501781818945E-15   \n",
       "-1.91888914970896E-20    4.5631002941104175E-20   -1.2391348115724852E-14  \n",
       "-1.0571727030766381E-20  2.5614248532320785E-20   -6.7784015435441465E-15  \n",
       "-1.177775341352891E-20   2.881769408538249E-20    -7.63281413700036E-15    \n",
       "-1.707261590544234E-20   4.2143856170782887E-20   -1.1236114731006528E-14  \n",
       "-1.899111959373164E-20   4.630734175934817E-20    -1.213239305801805E-14   \n",
       "1.1070093855099725E-20   -2.78344936960919E-20    7.418147309743971E-15    \n",
       "-1.1452214445673175E-20  2.6680684771793133E-20   -7.239264115484503E-15   \n",
       "8.79017577398206E-21     -2.220671881985823E-20   5.855388292665517E-15    \n",
       "4.25638573359928E-21     -1.0636064590101472E-20  2.835201483613028E-15    \n",
       "-1.0683254238637813E-20  2.6061648759408633E-20   -7.0639844439239386E-15  \n",
       "-7.084808391510278E-21   1.760564749277252E-20    -4.75947751461295E-15    \n",
       "2.6806836405774503E-21   -6.601152905715139E-21   1.73519556777209E-15     \n",
       "-9.767327869186358E-21   2.4559614575673857E-20   -6.549196441810807E-15   \n",
       "-1.5944581833796298E-20  3.8818329053448205E-20   -1.0420653357947268E-14  \n",
       "8.403702236311826E-21    -2.0410904987442866E-20  5.465261012271071E-15    \n",
       "-1.8386925119432408E-20  4.7832169587422506E-20   -1.2823322293253698E-14  \n",
       "1.4525815240165375E-20   -3.5588556906995243E-20  9.400408434119662E-15    \n",
       "6.86465821609936E-21     -1.679552416728763E-20   4.530991724800015E-15    \n",
       "-5.697370129197819E-21   1.3714726905824833E-20   -3.702390863908485E-15   \n",
       "2.08178010149238E-20     -5.180149231490558E-20   1.3762632636416795E-14   \n",
       "1.834482192791387E-20    -4.6439257969759544E-20  1.2505782687601276E-14   \n",
       "7.56737568717508E-21     -1.8367507939509405E-20  4.903679490007011E-15    \n",
       "5.585328536684528E-21    -1.381534678448963E-20   3.6590266697068755E-15   \n",
       "1.0686817990352353E-20   -2.635636289153174E-20   7.065440725425697E-15    \n",
       "7.26559894212315E-21     -1.7969416493837384E-20  4.783456073272063E-15    \n",
       "7.075916939876862E-21    -1.736845515561533E-20   4.616870518239012E-15    \n",
       "2.088641589822393E-21    -5.154682730775283E-21   1.3958200636441034E-15   \n",
       "-8.487894217904218E-21   2.0697387290410043E-20   -5.619044269882685E-15   \n",
       "4.3700067928833355E-21   -1.0783809179424438E-20  2.871984113403025E-15    \n",
       "-1.1276444775418703E-20  2.719811999397754E-20    -7.2738359037373E-15     \n",
       "5.129305373917183E-21    -1.2568885428135204E-20  3.3329892764452258E-15   \n",
       "2.008713942107985E-20    -4.971979227673963E-20   1.321768360993059E-14    \n",
       "5.411637204104116E-21    -1.3683306759287326E-20  3.645864311076264E-15    \n",
       "-1.242342397910324E-20   3.192528863259251E-20    -8.303844971540456E-15   \n",
       "-3.575928321403286E-21   8.955508144792108E-21    -2.4035535905634628E-15  \n",
       "1.8208818464523436E-20   -4.50544758943628E-20    1.1935245848397816E-14   \n",
       "-1.6699026129273154E-20  4.139948693190089E-20    -1.1065838549165554E-14  \n",
       "-1.421417200666801E-20   3.537808202015643E-20    -9.328927752246626E-15   \n",
       "... (483850 total))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svd: SingularValueDecomposition[IndexedRowMatrix, Matrix] = matrix_mul.computeSVD(n_components.toInt, computeU = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@15076327\n",
       "s = [1.2923542090759692E11,3.083065081621133E8,2.3033528005015823E8]\n",
       "V = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.2231012163...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-8.051231053858589E-21   2.0314743544613485E-20   -5.4470929547450735E-15  \n",
       "0.0016067529764857253    -1.9615361019119037E-4   2.4440313167959087E-4    \n",
       "0.0015946701465578213    4.594990813357084E-4     -1.27131792647146E-4     \n",
       "0.0015257591750188495    -1.1516506137695353E-4   4.757806412100489E-5     \n",
       "0.0015467867972772604    3.5001400383543143E-4    1.7776380768845697E-4    \n",
       "0.0015784500023011527    7.019261759657722E-4     2.9146377109652783E-4    \n",
       "0.0014563987593792294    -0.002470335383723202    0.0011291674754263425    \n",
       "5.223101216382444E-21    -1.2841205751705455E-20  3.4472675173625226E-15   \n",
       "-7.022981616856529E-21   1.6966037567050023E-20   -4.5731321646951146E-15  \n",
       "6.213146888095172E-21    -1.4769053932205133E-20  3.994921379437276E-15    \n",
       "-1.4005988581398866E-20  3.408154538323598E-20    -8.979296763901108E-15   \n",
       "-1.7837432191432697E-20  4.596625192668947E-20    -1.2113462720410842E-14  \n",
       "5.39331505207417E-4      5.27054361484347E-4      -1.880142629143662E-4    \n",
       "0.0015993341024977147    3.082198768346865E-4     1.876105791474283E-4     \n",
       "0.0015669487990077101    9.202790769520701E-5     2.6008982832244365E-4    \n",
       "0.0015757644465683232    1.0124605257879039E-4    2.688855133188503E-4     \n",
       "0.0015681841489847857    -3.0095040688783935E-5   2.5728656455293176E-4    \n",
       "6.128322208809462E-5     7.923676096598497E-4     -1.041780467531902E-4    \n",
       "-4.964597039883808E-22   1.2223188712085074E-21   -3.239627191115144E-16   \n",
       "-1.0484217870134873E-20  2.5735547353161487E-20   -6.909611679313999E-15   \n",
       "1.729717392793588E-20    -4.266001966508867E-20   1.1475651821855447E-14   \n",
       "-1.027284292821337E-20   2.5277715626228038E-20   -6.824951663049712E-15   \n",
       "4.985724915731599E-21    -1.2587601906655407E-20  3.324398423522689E-15    \n",
       "-1.4062110522245336E-20  3.4828460533990054E-20   -9.547168894562164E-15   \n",
       "0.0015778237987069013    3.0631033403236537E-4    1.8965435595771126E-4    \n",
       "0.001589933774543279     1.2121856871052628E-4    2.6911169203673786E-4    \n",
       "0.0016021767678790681    -1.2411093488719075E-4   -3.030647919490567E-5    \n",
       "3.062740111579499E-4     0.0013703832109195017    -0.001468767480132339    \n",
       "1.2088449894033151E-20   -2.9910117498522954E-20  7.957895343567636E-15    \n",
       "-1.8275036376608112E-20  4.736294496033747E-20    -1.2586711886207611E-14  \n",
       "1.3976758041064385E-20   -3.3247522674257666E-20  8.88575844270954E-15     \n",
       "-1.0951193296875823E-20  2.7705476875326367E-20   -7.318931640565459E-15   \n",
       "1.8514465600990095E-20   -4.620265891396146E-20   1.2237479044640504E-14   \n",
       "-5.211850327535444E-21   1.2720425348064891E-20   -3.4475762086876784E-15  \n",
       "2.0212303613777296E-20   -4.92560814745704E-20    1.3024602453591828E-14   \n",
       "-2.4794609574863248E-21  6.063883085448483E-21    -1.5910728107854143E-15  \n",
       "7.065000897339187E-23    -1.6720900742460984E-22  4.420246254595048E-17    \n",
       "4.064354350538228E-21    -9.82910119379099E-21    2.666195747420295E-15    \n",
       "-2.1841827969801026E-20  5.1193358404106017E-20   -1.3525523647695631E-14  \n",
       "9.965890293212886E-21    -2.5983200515257097E-20  6.9731613002621965E-15   \n",
       "-5.432140109238481E-21   1.3087937313015363E-20   -3.511501781818945E-15   \n",
       "-1.91888914970896E-20    4.5631002941104175E-20   -1.2391348115724852E-14  \n",
       "-1.0571727030766381E-20  2.5614248532320785E-20   -6.7784015435441465E-15  \n",
       "-1.177775341352891E-20   2.881769408538249E-20    -7.63281413700036E-15    \n",
       "-1.707261590544234E-20   4.2143856170782887E-20   -1.1236114731006528E-14  \n",
       "-1.899111959373164E-20   4.630734175934817E-20    -1.213239305801805E-14   \n",
       "1.1070093855099725E-20   -2.78344936960919E-20    7.418147309743971E-15    \n",
       "-1.1452214445673175E-20  2.6680684771793133E-20   -7.239264115484503E-15   \n",
       "8.79017577398206E-21     -2.220671881985823E-20   5.855388292665517E-15    \n",
       "4.25638573359928E-21     -1.0636064590101472E-20  2.835201483613028E-15    \n",
       "-1.0683254238637813E-20  2.6061648759408633E-20   -7.0639844439239386E-15  \n",
       "-7.084808391510278E-21   1.760564749277252E-20    -4.75947751461295E-15    \n",
       "2.6806836405774503E-21   -6.601152905715139E-21   1.73519556777209E-15     \n",
       "-9.767327869186358E-21   2.4559614575673857E-20   -6.549196441810807E-15   \n",
       "-1.5944581833796298E-20  3.8818329053448205E-20   -1.0420653357947268E-14  \n",
       "8.403702236311826E-21    -2.0410904987442866E-20  5.465261012271071E-15    \n",
       "-1.8386925119432408E-20  4.7832169587422506E-20   -1.2823322293253698E-14  \n",
       "1.4525815240165375E-20   -3.5588556906995243E-20  9.400408434119662E-15    \n",
       "6.86465821609936E-21     -1.679552416728763E-20   4.530991724800015E-15    \n",
       "-5.697370129197819E-21   1.3714726905824833E-20   -3.702390863908485E-15   \n",
       "2.08178010149238E-20     -5.180149231490558E-20   1.3762632636416795E-14   \n",
       "1.834482192791387E-20    -4.6439257969759544E-20  1.2505782687601276E-14   \n",
       "7.56737568717508E-21     -1.8367507939509405E-20  4.903679490007011E-15    \n",
       "5.585328536684528E-21    -1.381534678448963E-20   3.6590266697068755E-15   \n",
       "1.0686817990352353E-20   -2.635636289153174E-20   7.065440725425697E-15    \n",
       "7.26559894212315E-21     -1.7969416493837384E-20  4.783456073272063E-15    \n",
       "7.075916939876862E-21    -1.736845515561533E-20   4.616870518239012E-15    \n",
       "2.088641589822393E-21    -5.154682730775283E-21   1.3958200636441034E-15   \n",
       "-8.487894217904218E-21   2.0697387290410043E-20   -5.619044269882685E-15   \n",
       "4.3700067928833355E-21   -1.0783809179424438E-20  2.871984113403025E-15    \n",
       "-1.1276444775418703E-20  2.719811999397754E-20    -7.2738359037373E-15     \n",
       "5.129305373917183E-21    -1.2568885428135204E-20  3.3329892764452258E-15   \n",
       "2.008713942107985E-20    -4.971979227673963E-20   1.321768360993059E-14    \n",
       "5.411637204104116E-21    -1.3683306759287326E-20  3.645864311076264E-15    \n",
       "-1.242342397910324E-20   3.192528863259251E-20    -8.303844971540456E-15   \n",
       "-3.575928321403286E-21   8.955508144792108E-21    -2.4035535905634628E-15  \n",
       "1.8208818464523436E-20   -4.50544758943628E-20    1.1935245848397816E-14   \n",
       "-1.6699026129273154E-20  4.139948693190089E-20    -1.1065838549165554E-14  \n",
       "-1.421417200666801E-20   3.537808202015643E-20    -9.328927752246626E-15   \n",
       "... (483850 total)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val U: IndexedRowMatrix = svd.U // The U factor is a RowMatrix.\n",
    "val s: Vector = svd.s // The singular values are stored in a local dense vector.\n",
    "val V: Matrix = svd.V // The V factor is a local dense matrix.\n",
    "val S = Matrices.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 215:==================================================>(1997 + 3) / 2000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/U.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/U.csv"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val u_path = out_path + \"U\" + matrix_mode_str +\".csv\"\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(u_path))) {\n",
    "  cmd = \"hadoop dfs -rm -r \" + u_path\n",
    "  Process(cmd)!\n",
    "}\n",
    "val U_RDD = U.rows.sortBy(_.index).map(_.vector.toArray)\n",
    "U_RDD.cache()\n",
    "U_RDD.map(m => m.mkString(\",\")).repartition(1).saveAsTextFile(u_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 217:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "v_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/V.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/V.csv"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v_path = out_path + \"V\" + matrix_mode_str +\".csv\"\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(v_path))) {\n",
    "  cmd = \"hadoop dfs -rm -r \" + v_path\n",
    "  Process(cmd)!\n",
    "}\n",
    "sc.parallelize(V.rowIter.toVector.map(m => m.toArray.mkString(\",\")),4).saveAsTextFile(v_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s_path = hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/S.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/svd/spark/BloomGridmetSOST4Km3/S.csv"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s_path = out_path + \"S\" + matrix_mode_str +\".csv\"\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(s_path))) {\n",
    "  cmd = \"hadoop dfs -rm -r \" + s_path\n",
    "  Process(cmd)!\n",
    "}\n",
    "sc.parallelize(S.rowIter.toVector.map(m => m.toArray.mkString(\",\")),1).saveAsTextFile(s_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save GeoTiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GeoTiffs for U (dimension is M(A) x n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 243:=================================================>(1984 + 16) / 2000]7]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "[Stage 264:=================================================>(1984 + 16) / 2000]7]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "[Stage 285:==================================================>(1999 + 1) / 2000]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Elapsed time: 1566453903344ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 26171252685732\n",
       "mod_grid0_index_I = MapPartitionsRDD[370] at map at <console>:78\n",
       "U_RDD = MapPartitionsRDD[376] at map at <console>:81\n",
       "t1 = 27737706589076\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "27737706589076"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "val mod_grid0_index_I = model_grid0_index.zipWithIndex().map{ case (v,i) => (i,v)}\n",
    "mod_grid0_index_I.cache()\n",
    "model_grid0.cache()\n",
    "cfor(0)(_ < n_components, _ + 1) { k =>\n",
    "  //Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells\n",
    "  val kB = sc.broadcast(k)\n",
    "  //val U_k_RDD = U.rows.map(_.toArray.zipWithIndex.filter(_._2 == kB.value).sortBy(_._2).map{ case (v,i) => v}).flatMap(m => m)\n",
    "  val U_k_RDD = U_RDD.map(_(kB.value))\n",
    "  val cluster_cell_pos = ((U_k_RDD.zipWithIndex().map{ case (v,i) => (i,v)}).join(mod_grid0_index_I)).map{ case (k,(v,i)) => (v,i)}\n",
    "  cluster_cell_pos.cache()\n",
    "    \n",
    "  //Associate a Cluster_IDs to respective Grid_cell\n",
    "  val grid_clusters :RDD[ (Long, (Double, Option[Double]))] = model_grid0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i, c)})\n",
    "\n",
    "  //Convert all None to NaN\n",
    "  val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get)}\n",
    "\n",
    "  //Define a Tile\n",
    "  val cluster_cells :Array[Double] = grid_clusters_res.values.collect()\n",
    "  val cluster_cellsD = DoubleArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "  val geoTif = new SinglebandGeoTiff(cluster_cellsD, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "  cluster_cell_pos.unpersist()\n",
    "    \n",
    "  //Save to /tmp/\n",
    "  GeoTiffWriter.write(geoTif, u_geotiff_tmp_paths(k))\n",
    "\n",
    "  //Upload to HDFS\n",
    "  var cmd = \"hadoop dfs -copyFromLocal -f \" + u_geotiff_tmp_paths(k) + \" \" + u_geotiff_hdfs_paths(k)\n",
    "  Process(cmd)!\n",
    "\n",
    "  //Remove from /tmp/\n",
    "  cmd = \"rm -fr \" + u_geotiff_tmp_paths(k)\n",
    "  Process(cmd)!\n",
    "}\n",
    "U_RDD.unpersist()\n",
    "model_grid0.unpersist()\n",
    "mod_grid0_index_I.unpersist()\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 344:==================================================>(1999 + 1) / 2000]0]===================>                         (963 + 148) / 2000]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "[Stage 365:==================================================>(1996 + 4) / 2000]26]                    (440 + 147) / 2000]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "[Stage 386:==================================================>(1999 + 1) / 2000]6]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Elapsed time: 204158715556ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 28102241430963\n",
       "mod_grid0_index_I = MapPartitionsRDD[476] at map at <console>:85\n",
       "t1 = 28306400146519\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "28306400146519"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "val mod_grid0_index_I = satellite_grid0_index.zipWithIndex().map{ case (v,i) => (i,v)}\n",
    "mod_grid0_index_I.cache()\n",
    "model_grid0.cache()\n",
    "cfor(0)(_ < n_components, _ + 1) { k =>\n",
    "  //Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells\n",
    "  val kB = sc.broadcast(k)\n",
    "  //val U_k_RDD = U.rows.map(_.toArray.zipWithIndex.filter(_._2 == kB.value).sortBy(_._2).map{ case (v,i) => v}).flatMap(m => m)\n",
    "  val U_k_RDD = U_RDD.map(_(kB.value))\n",
    "  val cluster_cell_pos = ((U_k_RDD.zipWithIndex().map{ case (v,i) => (i,v)}).join(mod_grid0_index_I)).map{ case (k,(v,i)) => (v,i)}\n",
    "  cluster_cell_pos.cache()\n",
    "    \n",
    "  //Associate a Cluster_IDs to respective Grid_cell\n",
    "  val grid_clusters :RDD[ (Long, (Double, Option[Double]))] = satellite_grid0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i, c)})\n",
    "\n",
    "  //Convert all None to NaN\n",
    "  val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get)}\n",
    "\n",
    "  //Define a Tile\n",
    "  val cluster_cells :Array[Double] = grid_clusters_res.values.collect()\n",
    "  val cluster_cellsD = DoubleArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "  val geoTif = new SinglebandGeoTiff(cluster_cellsD, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "  cluster_cell_pos.unpersist()\n",
    "    \n",
    "  //Save to /tmp/\n",
    "  GeoTiffWriter.write(geoTif, u_geotiff_tmp_paths(k))\n",
    "\n",
    "  //Upload to HDFS\n",
    "  var cmd = \"hadoop dfs -copyFromLocal -f \" + u_geotiff_tmp_paths(k) + \" \" + u_geotiff_hdfs_paths(k)\n",
    "  Process(cmd)!\n",
    "\n",
    "  //Remove from /tmp/\n",
    "  cmd = \"rm -fr \" + u_geotiff_tmp_paths(k)\n",
    "  Process(cmd)!\n",
    "}\n",
    "U_RDD.unpersist()\n",
    "model_grid0.unpersist()\n",
    "mod_grid0_index_I.unpersist()\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GeoTiffs for V (dimension is M(B) x n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 298:==================================================>(1999 + 1) / 2000]26]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "[Stage 310:=================================================>(1990 + 10) / 2000]6]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "[Stage 322:==================================================>(1996 + 4) / 2000]0]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Elapsed time: 189805155287ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 27740358278758\n",
       "sat_grid0_index_I = MapPartitionsRDD[426] at map at <console>:78\n",
       "iter = empty iterator\n",
       "k = 3\n",
       "t1 = 27930163434045\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "27930163434045"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "val sat_grid0_index_I = satellite_grid0_index.zipWithIndex().map{ case (v,i) => (i,v)}\n",
    "val iter = V.colIter\n",
    "var k :Int = 0\n",
    "while (iter.hasNext) {\n",
    "  //Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells\n",
    "  val V_k_RDD = sc.parallelize(iter.next().toArray)\n",
    "  val cluster_cell_pos = ((V_k_RDD.zipWithIndex().map{ case (v,i) => (i,v)}).join(sat_grid0_index_I)).map{ case (k,(v,i)) => (v,i)}\n",
    "\n",
    "  //Associate a Cluster_IDs to respective Grid_cell\n",
    "  val grid_clusters :RDD[ (Long, (Double, Option[Double]))] = satellite_grid0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i, c)})\n",
    "\n",
    "  //Convert all None to NaN\n",
    "  val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get)}\n",
    "\n",
    "  //Define a Tile\n",
    "  val cluster_cells :Array[Double] = grid_clusters_res.values.collect()\n",
    "  val cluster_cellsD = DoubleArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "  val geoTif = new SinglebandGeoTiff(cluster_cellsD, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "\n",
    "  //Save to /tmp/\n",
    "  GeoTiffWriter.write(geoTif, v_geotiff_tmp_paths(k))\n",
    "\n",
    "  //Upload to HDFS\n",
    "  var cmd = \"hadoop dfs -copyFromLocal -f \" + v_geotiff_tmp_paths(k) + \" \" + v_geotiff_hdfs_paths(k)\n",
    "  Process(cmd)!\n",
    "\n",
    "  //Remove from /tmp/\n",
    "  cmd = \"rm -fr \" + v_geotiff_tmp_paths(k)\n",
    "  Process(cmd)!\n",
    "    \n",
    "  k += 1\n",
    "}\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1684"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols_rows._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
