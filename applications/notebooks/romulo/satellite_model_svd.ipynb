{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD between a Model and Satellite data\n",
    "\n",
    "This notebook shows how to multiply two matrices and calculate SVD. Each matrix is created out a set of GeoTiffs for a series of years. Both matrices should have the same dimension.\n",
    "\n",
    "For demonstration we will use from a model (spring-index) and from a satellite (AVHRR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n",
    "\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile, Tile, UByteCellType}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.hadoop.io.SequenceFile.Writer\n",
    "import org.apache.hadoop.io.{SequenceFile, _}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "import scala.sys.process._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of operation\n",
    "\n",
    "Here the user can define the mode of operation.\n",
    "* **rdd_offline_mode**: If false it means the notebook will create all data from scratch and store protected_extent and num_cols_rows into HDFS. Otherwise, these data structures are read from HDFS.\n",
    "\n",
    "It is also possible to define which directory of GeoTiffs is to be used and on which **band** to run Kmeans. The options are\n",
    "* **all** which are a multi-band (**8 bands**) GeoTiffs\n",
    "* Or choose single band ones:\n",
    "    0. Onset_Greenness_Increase\n",
    "    1. Onset_Greenness_Maximum\n",
    "    2. Onset_Greenness_Decrease\n",
    "    3. Onset_Greenness_Minimum\n",
    "    4. NBAR_EVI_Onset_Greenness_Minimum\n",
    "    5. NBAR_EVI_Onset_Greenness_Maximum\n",
    "    6. NBAR_EVI_Area\n",
    "    7. Dynamics_QC\n",
    "\n",
    "<span style=\"color:red\">Note that when using a range **kemans offline mode** is not possible and it will be reset to **online mode**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation setup\n",
    "<a id='mode_of_operation_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_rdd_offline_mode = true\n",
       "model_matrix_offline_mode = true\n",
       "satellite_rdd_offline_mode = true\n",
       "satellite_matrix_offline_mode = true\n",
       "model_path = hdfs:///user/hadoop/spring-index/\n",
       "model_dir = BloomGridmet\n",
       "satellite_path = hdfs:///user/hadoop/spring-index/\n",
       "satellite_dir = LeafGridmet\n",
       "out_path = hdfs:///user/pheno/svd/spark/BloomGridmetLeafGridmet/\n",
       "band_num = 0\n",
       "satellite_first_year = 1989\n",
       "satellite_last_year = 2014\n",
       "model_first_year = 1989\n",
       "model_last_year = 2014\n",
       "toBeMasked = false\n",
       "mask_path = hdfs:///user/hadoop/usa_mask_low.tif\n",
       "save_rdds = true\n",
       "save_matrix = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var model_rdd_offline_mode = true\n",
    "var model_matrix_offline_mode = true\n",
    "var satellite_rdd_offline_mode = true\n",
    "var satellite_matrix_offline_mode = true\n",
    "\n",
    "//Using spring-index model\n",
    "var model_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var model_dir = \"BloomGridmet\"\n",
    "\n",
    "//Using AVHRR Satellite data\n",
    "//var satellite_path = \"hdfs:///user/hadoop/avhrr/\"\n",
    "//var satellite_dir = \"SOSTLowPR\"\n",
    "var satellite_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var satellite_dir = \"LeafGridmet\"\n",
    "\n",
    "var out_path = \"hdfs:///user/pheno/svd/spark/\" + model_dir + satellite_dir + \"/\"\n",
    "//First band is 0\n",
    "var band_num = 0\n",
    "\n",
    "//Years between (inclusive) 1989 - 2014\n",
    "var satellite_first_year = 1989\n",
    "var satellite_last_year = 2014\n",
    "\n",
    "//Years between (inclusive) 1980 - 2015\n",
    "var model_first_year = 1989\n",
    "var model_last_year = 2014\n",
    "\n",
    "//Mask\n",
    "val toBeMasked = false\n",
    "val mask_path = \"hdfs:///user/hadoop/usa_mask_low.tif\"\n",
    "\n",
    "val save_rdds = true\n",
    "val save_matrix = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">DON'T MODIFY ANY PIECE OF CODE FROM HERE ON!!!</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of operation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "[Stage 2327:==================================================>(998 + 2) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_907625744_41, ugi=pheno (auth:SIMPLE)]]\n",
       "model_grid0_path = hdfs:///user/pheno/svd/spark/BloomGridmetLeafGridmet/BloomGridmet_grid0\n",
       "model_grid0_index_path = hdfs:///user/pheno/svd/spark/BloomGridmetLeafGridmet/BloomGridmet_grid0_index\n",
       "model_grid_path = hdfs:///user/pheno/svd/spark/BloomGridmetLeafGridmet/BloomGridmet_grid\n",
       "satellite_grid_path = hdfs:///user/pheno/svd/spark/BloomGridmetLeafGridmet...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mask_str: String = \"\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/svd/spark/BloomGridmetLeafGridmet/LeafGridmet_grid"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Check offline modes\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "//Paths to store data structures for Offline runs\n",
    "var mask_str = \"\"\n",
    "if (toBeMasked)\n",
    "  mask_str = \"_mask\"\n",
    "var model_grid0_path = out_path + model_dir + \"_grid0\"\n",
    "var model_grid0_index_path = out_path + model_dir + \"_grid0_index\"\n",
    "\n",
    "var model_grid_path = out_path + model_dir + \"_grid\"\n",
    "var satellite_grid_path = out_path + satellite_dir + \"_grid\"\n",
    "var model_matrix_path = out_path + model_dir + \"_matrix\"\n",
    "var satellite_matrix_path = out_path + satellite_dir + \"_matrix\"\n",
    "var metadata_path = out_path + model_dir + \"_metadata\"\n",
    "\n",
    "var sc_path = out_path + model_dir + \"_sc\"\n",
    "var mc_path = out_path + model_dir + \"_mc\"\n",
    "\n",
    "val model_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(model_grid_path))\n",
    "val model_matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(model_matrix_path))\n",
    "val satellite_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(satellite_grid_path))\n",
    "val satellite_matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(satellite_matrix_path))\n",
    "\n",
    "if (model_rdd_offline_mode != model_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + model_rdd_offline_exists.toString())\n",
    "  model_rdd_offline_mode = model_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (model_matrix_offline_mode != model_matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + model_matrix_offline_exists.toString())\n",
    "  model_matrix_offline_mode = model_matrix_offline_exists\n",
    "}\n",
    "\n",
    "var model_skip_rdd = false\n",
    "if (model_matrix_offline_exists) {\n",
    "    println(\"Since we have a matrix, the load of the grids RDD will be skipped!!!\")\n",
    "    model_skip_rdd = true\n",
    "}\n",
    "\n",
    "if (satellite_rdd_offline_mode != satellite_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + satellite_rdd_offline_exists.toString())\n",
    "  satellite_rdd_offline_mode = satellite_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (satellite_matrix_offline_mode != satellite_matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + satellite_matrix_offline_exists.toString())\n",
    "  satellite_matrix_offline_mode = satellite_matrix_offline_exists\n",
    "}\n",
    "\n",
    "var satellite_skip_rdd = false\n",
    "if (satellite_matrix_offline_exists) {\n",
    "    println(\"Since we have a matrix, the load of the grids RDD will be skipped!!!\")\n",
    "    satellite_skip_rdd = true\n",
    "}\n",
    "\n",
    "var corr_tif = out_path + \"_\" + satellite_dir + \"_\" + model_dir + \".tif\"\n",
    "var corr_tif_tmp = \"/tmp/svd_\" + satellite_dir + \"_\" + model_dir + \".tif\"\n",
    "\n",
    "//Years\n",
    "val model_years = 1989 to 2014\n",
    "val satellite_years = 1989 to 2014\n",
    "\n",
    "if (!satellite_years.contains(satellite_first_year) || !(satellite_years.contains(satellite_last_year))) {\n",
    "  println(\"Invalid range of years for \" + satellite_dir + \". I should be between \" + satellite_first_year + \" and \" + satellite_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "if (!model_years.contains(model_first_year) || !(model_years.contains(model_last_year))) {\n",
    "  println(\"Invalid range of years for \" + model_dir + \". I should be between \" + model_first_year + \" and \" + model_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "if ( ((satellite_last_year - model_first_year) > (model_last_year - model_first_year)) || ((satellite_last_year - model_first_year) > (model_last_year - model_first_year))) {\n",
    "  println(\"The range of years for each data set should be of the same length.\");\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "var model_years_range = (model_years.indexOf(model_first_year), model_years.indexOf(model_last_year))\n",
    "var satellite_years_range = (satellite_years.indexOf(satellite_first_year), satellite_years.indexOf(satellite_last_year))\n",
    "\n",
    "//Global variables\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var model_grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var model_grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var model_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var model_grids: RDD[Array[Double]] = sc.emptyRDD\n",
    "val rows = sc.parallelize(Array[Double]()).map(m => Vectors.dense(m))\n",
    "var model_summary: MultivariateStatisticalSummary = new RowMatrix(rows).computeColumnSummaryStatistics()\n",
    "var model_std :Array[Double] = new Array[Double](0)\n",
    "\n",
    "var satellite_grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_grids: RDD[Array[Double]] = sc.emptyRDD\n",
    "var satellite_summary: MultivariateStatisticalSummary = new RowMatrix(rows).computeColumnSummaryStatistics()\n",
    "var satellite_std :Array[Double] = new Array[Double](0)\n",
    "\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var cellT :CellType = UByteCellType\n",
    "var mask_tile0 :Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile\n",
    "var satellite_cells_size :Long = 0\n",
    "var model_cells_size :Long = 0\n",
    "var t0 : Long = 0\n",
    "var t1 : Long = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to (de)serialize any structure into Array[Byte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serialize: (value: Any)Array[Byte]\n",
       "deserialize: (bytes: Array[Byte])Any\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def serialize(value: Any): Array[Byte] = {\n",
    "    val out_stream: ByteArrayOutputStream = new ByteArrayOutputStream()\n",
    "    val obj_out_stream = new ObjectOutputStream(out_stream)\n",
    "    obj_out_stream.writeObject(value)\n",
    "    obj_out_stream.close\n",
    "    out_stream.toByteArray\n",
    "}\n",
    "\n",
    "def deserialize(bytes: Array[Byte]): Any = {\n",
    "    val obj_in_stream = new ObjectInputStream(new ByteArrayInputStream(bytes))\n",
    "    val value = obj_in_stream.readObject\n",
    "    obj_in_stream.close\n",
    "    value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GeoTiffs\n",
    "\n",
    "Using GeoTrellis all GeoTiffs of a directory will be loaded into a RDD. Using the RDD, we extract a grid from the first file to lated store the Kmeans cluster_IDS, we build an Index for populate such grid and we filter out here all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 198799ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 10986953876449\n",
       "pattern = tif\n",
       "satellite_filepath = hdfs:///user/hadoop/spring-index/LeafGridmet\n",
       "model_filepath = hdfs:///user/hadoop/spring-index//BloomGridmet\n",
       "t1 = 10986954075248\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10986954075248"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "//Load Mask\n",
    "if (toBeMasked) {\n",
    "  val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mask_path).values\n",
    "  val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "  mask_tile0 = (mask_tiles_withIndex.filter(m => m._1==0).filter(m => !m._1.isNaN).values.collect())(0)\n",
    "}\n",
    "\n",
    "//Local variables\n",
    "val pattern: String = \"tif\"\n",
    "val satellite_filepath: String = satellite_path + satellite_dir\n",
    "val model_filepath: String = model_path + \"/\" + model_dir\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells is: 1414560                                                     \n",
      "Elapsed time: 9940263382ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 10990578985885\n",
       "satellite_grids_withIndex = MapPartitionsRDD[1075] at map at <console>:198\n",
       "satellite_grids = MapPartitionsRDD[1077] at values at <console>:201\n",
       "satellite_summary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@470166fc\n",
       "satellite_std = [D@1ab30351\n",
       "satellite_grid0_index = MapPartitionsRDD[1085] at flatMap at <console>:208\n",
       "satellite_cells_size = 1414560\n",
       "t1 = 11000519249267\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11000519249267"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "if (satellite_rdd_offline_mode) {\n",
    "  satellite_grids_RDD = sc.objectFile(satellite_grid_path)\n",
    "} else {\n",
    "  //Lets load MODIS Singleband GeoTiffs and return RDD just with the tiles.\n",
    "  val satellite_geos_RDD = sc.hadoopGeoTiffRDD(satellite_filepath, pattern)\n",
    "  val satellite_tiles_RDD = satellite_geos_RDD.values\n",
    "\n",
    "  if (toBeMasked) {\n",
    "    val mask_tile_broad :Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "    satellite_grids_RDD = satellite_tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble().filter(_ != -1000))\n",
    "  } else {\n",
    "    satellite_grids_RDD = satellite_tiles_RDD.map(m => m.toArrayDouble())\n",
    "  }\n",
    "\n",
    "  //Store in HDFS\n",
    "  if (save_rdds) {\n",
    "      satellite_grids_RDD.saveAsObjectFile(satellite_grid_path)\n",
    "  }\n",
    "}\n",
    "val satellite_grids_withIndex = satellite_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "satellite_grids = satellite_grids_withIndex.filterByRange(satellite_years_range._1, satellite_years_range._2).values\n",
    "satellite_grids.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//Collect Stats:\n",
    "satellite_summary = Statistics.colStats(satellite_grids.map(m => Vectors.dense(m)))\n",
    "satellite_std = satellite_summary.variance.toArray.map(m => scala.math.sqrt(m))\n",
    "\n",
    "var satellite_grid0_index: RDD[Double] = satellite_grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m)\n",
    "satellite_cells_size = satellite_grid0_index.count().toInt\n",
    "println(\"Number of cells is: \" + satellite_cells_size)\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 41297494747ns                                                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 11006417639401\n",
       "model_grids_withIndex = MapPartitionsRDD[1118] at map at <console>:279\n",
       "model_grids = MapPartitionsRDD[1120] at values at <console>:282\n",
       "model_summary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@55d0e57c\n",
       "model_tile0_index = MapPartitionsRDD[1128] at flatMap at <console>:289\n",
       "model_cells_size = 1414560\n",
       "t1 = 11047715134148\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11047715134148"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "\n",
    "if (model_rdd_offline_mode) {\n",
    "  model_grids_RDD = sc.objectFile(model_grid_path)\n",
    "  model_grid0 = sc.objectFile(model_grid0_path)\n",
    "  model_grid0_index = sc.objectFile(model_grid0_index_path)\n",
    "  val metadata = sc.sequenceFile(metadata_path, classOf[IntWritable], classOf[BytesWritable]).map(_._2.copyBytes()).collect()\n",
    "  projected_extent = deserialize(metadata(0)).asInstanceOf[ProjectedExtent]\n",
    "  num_cols_rows = (deserialize(metadata(1)).asInstanceOf[Int], deserialize(metadata(2)).asInstanceOf[Int])\n",
    "  cellT = deserialize(metadata(3)).asInstanceOf[CellType]\n",
    "} else {\n",
    "  if (band_num != 0) {\n",
    "    val model_geos_RDD = sc.hadoopMultibandGeoTiffRDD(model_filepath, pattern)\n",
    "    val model_tiles_RDD = model_geos_RDD.values\n",
    "\n",
    "    //Retrieve the number of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = model_tiles_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "    val projected_extents_withIndex = model_geos_RDD.keys.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "    projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    val band_numB: Broadcast[Int] = sc.broadcast(band_num)\n",
    "    if (toBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "      grids_RDD = model_tiles_RDD.map(m => m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "    } else {\n",
    "      grids_RDD = model_tiles_RDD.map(m => m.band(band_numB.value).toArrayDouble())\n",
    "    }\n",
    "  } else {\n",
    "    val model_geos_RDD = sc.hadoopGeoTiffRDD(model_filepath, pattern)\n",
    "    val model_tiles_RDD = model_geos_RDD.values\n",
    "\n",
    "    //Retrieve the number of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = model_tiles_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "    val projected_extents_withIndex = model_geos_RDD.keys.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "    projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "    if (toBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "      grids_RDD = model_tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "    } else {\n",
    "      grids_RDD = model_tiles_RDD.map(m => m.toArrayDouble())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  //Get Index for each Cell\n",
    "  val grids_withIndex = grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  if (toBeMasked) {\n",
    "    model_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => m._1 != -1000.0).map { case (v, i) => (i) }\n",
    "  } else {\n",
    "    model_grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i) }\n",
    "  }\n",
    "\n",
    "  //Get the Tile's grid\n",
    "  model_grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "  //Lets filter out NaN\n",
    "  if (toBeMasked) {\n",
    "    model_grids_RDD = grids_RDD.map(m => m.filter(m => m != -1000.0))\n",
    "  } else {\n",
    "    model_grids_RDD = grids_RDD\n",
    "  }\n",
    "\n",
    "  //Store data in HDFS\n",
    "  model_grids_RDD.saveAsObjectFile(model_grid_path)\n",
    "  model_grid0.saveAsObjectFile(model_grid0_path)\n",
    "  model_grid0_index.saveAsObjectFile(model_grid0_index_path)\n",
    "\n",
    "  val writer: SequenceFile.Writer = SequenceFile.createWriter(conf,\n",
    "    Writer.file(metadata_path),\n",
    "    Writer.keyClass(classOf[IntWritable]),\n",
    "    Writer.valueClass(classOf[BytesWritable])\n",
    "  )\n",
    "\n",
    "  writer.append(new IntWritable(1), new BytesWritable(serialize(projected_extent)))\n",
    "  writer.append(new IntWritable(2), new BytesWritable(serialize(num_cols_rows._1)))\n",
    "  writer.append(new IntWritable(3), new BytesWritable(serialize(num_cols_rows._2)))\n",
    "  writer.append(new IntWritable(4), new BytesWritable(serialize(cellT)))\n",
    "  writer.hflush()\n",
    "  writer.close()\n",
    "}\n",
    "\n",
    "val model_grids_withIndex = model_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "model_grids = model_grids_withIndex.filterByRange(model_years_range._1, model_years_range._2).values\n",
    "model_grids.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//Collect Stats:\n",
    "model_summary = Statistics.colStats(model_grids.map(m => Vectors.dense(m)))\n",
    "//model_std = model_summary.variance.toArray.map(m => scala.math.sqrt(m))\n",
    "\n",
    "var model_tile0_index: RDD[Double] = model_grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m)\n",
    "model_cells_size = model_tile0_index.count().toInt\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2349:====================================================> (36 + 1) / 37]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "satellite_cells_sizeB = Broadcast(571)\n",
       "satellite_mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@56d1a612\n",
       "sat_byColumnAndRow = MapPartitionsRDD[1134] at flatMap at <console>:162\n",
       "satellite_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@41714a00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@41714a00"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Satellite\n",
    "val satellite_cells_sizeB = sc.broadcast(satellite_cells_size)\n",
    "val satellite_mat: RowMatrix = new RowMatrix(satellite_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(satellite_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "\n",
    "// Split the matrix into one number per line.\n",
    "val sat_byColumnAndRow = satellite_mat.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val satellite_blockMatrix: BlockMatrix = new CoordinateMatrix(sat_byColumnAndRow).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2382:==================================================>(991 + 9) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sc_exists = false\n",
       "Sc = org.apache.spark.mllib.linalg.distributed.BlockMatrix@af400ca\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@af400ca"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//SC\n",
    "val sc_exists = fs.exists(new org.apache.hadoop.fs.Path(sc_path))\n",
    "var Sc :BlockMatrix = null\n",
    "if (sc_exists) {\n",
    "  val rdd_indexed_rows :RDD[IndexedRow]= sc.objectFile(sc_path)\n",
    "  Sc = new IndexedRowMatrix(rdd_indexed_rows).toBlockMatrix()\n",
    "} else {\n",
    "  val satellite_M_1_Gc = sc.parallelize(Array[Vector](satellite_summary.mean)).map(m => Vectors.dense(m.toArray))\n",
    "  val satellite_M_1_Gc_RowM: RowMatrix = new RowMatrix(satellite_M_1_Gc)\n",
    "  val sat_M_1_Gc_byColumnAndRow = satellite_M_1_Gc_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val satellite_M_1_Gc_blockMatrix = new CoordinateMatrix(sat_M_1_Gc_byColumnAndRow).toBlockMatrix()\n",
    "\n",
    "  val sat_matrix_Nt_1 = new Array[Double](satellite_grids.count().toInt)\n",
    "  satellite_grids.unpersist(false)\n",
    "  for (i <- 0 until sat_matrix_Nt_1.length)\n",
    "    sat_matrix_Nt_1(i) = 1\n",
    "  val satellite_M_Nt_1 = sc.parallelize(sat_matrix_Nt_1).map(m => Vectors.dense(m))\n",
    "  val satellite_M_Nt_1_RowM: RowMatrix = new RowMatrix(satellite_M_Nt_1)\n",
    "  val sat_M_Nt_1_byColumnAndRow = satellite_M_Nt_1_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val satellite_M_Nt_1_blockMatrix = new CoordinateMatrix(sat_M_Nt_1_byColumnAndRow).toBlockMatrix()\n",
    "  val satellite_M_Nt_Gc_blockMatrix = satellite_M_Nt_1_blockMatrix.multiply(satellite_M_1_Gc_blockMatrix)\n",
    "\n",
    "  //Sc = satellite_blockMatrix.subtract(satellite_M_Nt_Gc_blockMatrix)\n",
    "  val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = satellite_blockMatrix.toCoordinateMatrix().toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(satellite_M_Nt_Gc_blockMatrix.toCoordinateMatrix().toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "  Sc = (new CoordinateMatrix(joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1*m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))).toBlockMatrix() \n",
    "\n",
    "  //save to disk\n",
    "  Sc.toIndexedRowMatrix().rows.saveAsObjectFile(sc_path)\n",
    "}\n",
    "Sc.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_cells_sizeB = Broadcast(595)\n",
       "model_mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@273dfe93\n",
       "mod_byColumnAndRow = MapPartitionsRDD[1202] at flatMap at <console>:162\n",
       "model_blockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@6eebebcb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@6eebebcb"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Model\n",
    "val model_cells_sizeB = sc.broadcast(model_cells_size)\n",
    "val model_mat: RowMatrix = new RowMatrix(model_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(model_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1))))\n",
    "\n",
    "// Split the matrix into one number per line.\n",
    "val mod_byColumnAndRow = model_mat.rows.zipWithIndex.map {\n",
    "  case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "    case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "  }\n",
    "}.flatMap(x => x)\n",
    "val model_blockMatrix: BlockMatrix = new CoordinateMatrix(mod_byColumnAndRow).transpose().toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2417:==================================================>(996 + 4) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mc_exists = false\n",
       "Mc = org.apache.spark.mllib.linalg.distributed.BlockMatrix@370c2642\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@370c2642"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//MC\n",
    "val mc_exists = fs.exists(new org.apache.hadoop.fs.Path(mc_path))\n",
    "var Mc :BlockMatrix = null\n",
    "if (mc_exists) {\n",
    "  val rdd_indexed_rows :RDD[IndexedRow]= sc.objectFile(mc_path)\n",
    "  Mc = new IndexedRowMatrix(rdd_indexed_rows).toBlockMatrix()\n",
    "} else {\n",
    "  val model_M_1_Gc = sc.parallelize(Array[Vector](model_summary.mean)).map(m => Vectors.dense(m.toArray))\n",
    "  val model_M_1_Gc_RowM: RowMatrix = new RowMatrix(model_M_1_Gc)\n",
    "  val mod_M_1_Gc_byColumnAndRow = model_M_1_Gc_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val model_M_1_Gc_blockMatrix = new CoordinateMatrix(mod_M_1_Gc_byColumnAndRow).toBlockMatrix()\n",
    "\n",
    "  val model_matrix_Nt_1 = new Array[Double](model_grids.count().toInt)\n",
    "  model_grids.unpersist(false)\n",
    "\n",
    "  for (i <- 0 until model_matrix_Nt_1.length)\n",
    "    model_matrix_Nt_1(i) = 1\n",
    "  val model_M_Nt_1 = sc.parallelize(model_matrix_Nt_1).map(m => Vectors.dense(m))\n",
    "  val model_M_Nt_1_RowM: RowMatrix = new RowMatrix(model_M_Nt_1)\n",
    "  val mod_M_Nt_1_byColumnAndRow = model_M_Nt_1_RowM.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val model_M_Nt_1_blockMatrix = new CoordinateMatrix(mod_M_Nt_1_byColumnAndRow).toBlockMatrix()\n",
    "  val model_M_Nt_Gc_blockMatrix = model_M_Nt_1_blockMatrix.multiply(model_M_1_Gc_blockMatrix)\n",
    "  val model_M_Gc_Nt_blockMatrix = model_M_Nt_Gc_blockMatrix.transpose\n",
    "  \n",
    "  //Mc = model_blockMatrix.subtract(model_M_Gc_Nt_blockMatrix)\n",
    "  val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = model_blockMatrix.toCoordinateMatrix().toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(model_M_Gc_Nt_blockMatrix.toCoordinateMatrix().toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "  Mc = (new CoordinateMatrix(joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1*m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))).toBlockMatrix()\n",
    "\n",
    "  //save to disk\n",
    "  Mc.toIndexedRowMatrix().rows.saveAsObjectFile(mc_path)\n",
    "}\n",
    "Mc.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2441:==================================================>   (30 + 2) / 32]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix_mul = org.apache.spark.mllib.linalg.distributed.BlockMatrix@6032e8cc\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.BlockMatrix@6032e8cc"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Matrix Multiplication\n",
    "//val matrix_mul = model_blockMatrix.multiply(satellite_blockMatrix)\n",
    "val matrix_mul = Mc.multiply(Sc)\n",
    "matrix_mul.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "//val resRowMatrix: RowMatrix = new RowMatrix(matrix_mul.toIndexedRowMatrix().rows.sortBy(_.index).map(_.vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1414560\n",
      "493832\n"
     ]
    }
   ],
   "source": [
    "println(matrix_mul.numCols)\n",
    "println(matrix_mul.numRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2466:>                                                     (0 + 34) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 29 in stage 2466.0 failed 4 times, most recent failure: Lost task 29.3 in stage 2466.0 (TID 200652, 145.100.59.233, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
       "Driver stacktrace:\n",
       "StackTrace: Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1988)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.multiplyGramianMatrixBy(RowMatrix.scala:93)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$16.apply(RowMatrix.scala:268)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$16.apply(RowMatrix.scala:268)\n",
       "  at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:268)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:194)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//SVD\n",
    "val svd: SingularValueDecomposition[RowMatrix, Matrix] = matrix_mul.toCoordinateMatrix().toRowMatrix().computeSVD(10, computeU = true)\n",
    "\n",
    "//val U: RowMatrix = svd.U // The U factor is a RowMatrix.\n",
    "//val s: Vector = svd.s // The singular values are stored in a local dense vector.\n",
    "//val V: Matrix = svd.V // The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U = org.apache.spark.mllib.linalg.distributed.RowMatrix@13a81f4\n",
       "s = [1.7294690472100357E13,3.5417440832692556E9,2.218091441578333E9,1.4102137597250912E9,1.0501195763116548E9,7.490420148329376E8,5.105869757765809E8,4.219423656858187E8,3.515090255611161E8,2.966443139452226E8]\n",
       "V = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "-0.00718766236...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.008214439989956223   -0.0024723820007321044  ... (10 total)\n",
       "-0.00821443998995622    -0.0024723820007328673  ...\n",
       "-0.007214179882276797   -0.004419063578345915   ...\n",
       "-0.0074317588149111925  -0.004702615520864071   ...\n",
       "-0.00737423750464065    -0.004590520013245656   ...\n",
       "-0.007080193275179696   -0.00374039318980654    ...\n",
       "-0.007080193275179695   -0.003740393189807042   ...\n",
       "-0.007187662361606361   -0.005214828820355853   ...\n",
       "-0.007196118383966567   -0.005165290038956709   ...\n",
       "-0.00719611838396657    -0.005165290038956005   ...\n",
       "-0.007196118383966571   -0.005165290038956264   ...\n",
       "-0.007393471035251325   -0.005381283570195753   ...\n",
       "-0.007196118383966569   -0.005165290038956245   ...\n",
       "-0.007223990876863853   -0.00422548179952084    ...\n",
       "-0.007043827597506955   -0.004331001303692338   ...\n",
       "-0.007043827597506951   -0.004331001303692099   ...\n",
       "-0.007035059608884771   -0.003689546775446188   ...\n",
       "-0.0072138165626801955  -0.005279059954993707   ...\n",
       "-0.0072519146908152365  -0.004725102219028031   ...\n",
       "-0.007251914690815234   -0.004725102219027509   ...\n",
       "-0.007620149989090083   -0.006322139009499673   ...\n",
       "-0.007782267010916147   -0.005945979779992119   ...\n",
       "-0.007601956034467512   -0.006599079425944074   ...\n",
       "-0.007620149989090081   -0.006322139009500203   ...\n",
       "-0.00762014998909008    -0.0063221390095010025  ...\n",
       "-0.00791282422426551    -0.0042938375137338775  ...\n",
       "-0.007534767826722704   -0.0046769762621223015  ...\n",
       "-0.007476943386303139   -0.0040675040920763435  ...\n",
       "-0.007476943386303144   -0.004067504092076485   ...\n",
       "-0.007260961162804731   -0.0049115173212434145  ...\n",
       "-0.007362141310467533   -0.0051806121554544195  ...\n",
       "-0.007364295989053258   -0.0050157006012136185  ...\n",
       "-0.007206733975354012   -0.0046333498977684014  ...\n",
       "-0.007206733975354011   -0.0046333498977689895  ...\n",
       "-0.007278658535847191   -0.003931641481054757   ...\n",
       "-0.007242473590160143   -0.005000269199303983   ...\n",
       "-0.007317238990117628   -0.005709687479298257   ...\n",
       "-0.0073172389901176305  -0.00570968747929817    ...\n",
       "-0.007308628886722519   -0.0052336254004587916  ...\n",
       "-0.007215956320607845   -0.0046353633979383324  ...\n",
       "-0.007260341286453987   -0.00458858865133539    ...\n",
       "-0.007260341286453982   -0.004588588651335816   ...\n",
       "-0.007270170942572244   -0.004632256894905598   ...\n",
       "-0.00718868437357805    -0.005081230476654533   ...\n",
       "-0.00730690888950451    -0.004658732869918392   ...\n",
       "-0.007287890541040389   -0.003962248978478505   ...\n",
       "-0.007287890541040386   -0.003962248978478147   ...\n",
       "-0.007352678129126756   -0.006081480794963577   ...\n",
       "-0.007503581289506829   -0.004649521336938309   ...\n",
       "-0.0076556110045774275  -0.005845656217880814   ...\n",
       "-0.007655611004577426   -0.005845656217880913   ...\n",
       "-0.0074355773516051864  -0.00446435515572654    ...\n",
       "-0.007407527512479568   -0.003584174213267224   ...\n",
       "-0.0073795123911930315  -0.003801070856135202   ...\n",
       "-0.0073887774377316695  -0.0034367973187000786  ...\n",
       "-0.007388777437731669   -0.0034367973187006354  ...\n",
       "-0.007076744864340285   -0.005381266048590632   ...\n",
       "-0.007286068105041298   -0.004881162623860054   ...\n",
       "-0.007361045984116008   -0.003420045088664161   ...\n",
       "-0.007361045984116006   -0.0034200450886637005  ...\n",
       "-0.007305508661334962   -0.004761844994555425   ...\n",
       "-0.007141050454479883   -0.005738072992697399   ...\n",
       "-0.00713179280523441    -0.005877018194223964   ...\n",
       "-0.007095663730608611   -0.005017076839788583   ...\n",
       "-0.007150240093379292   -0.005616009261972162   ...\n",
       "-0.007193227042133395   -0.005522342756385361   ...\n",
       "-0.007131646798233326   -0.005392831525617595   ...\n",
       "-0.0070961765853073165  -0.004732398587482455   ...\n",
       "-0.007096176585307314   -0.004732398587482992   ...\n",
       "-0.007132900716399681   -0.004586210040286677   ...\n",
       "-0.007132900716399683   -0.004586210040286691   ...\n",
       "-0.007150816755254485   -0.005401447507586449   ...\n",
       "-0.00715081675525448    -0.005401447507587351   ...\n",
       "-0.007150816755254485   -0.005401447507586295   ...\n",
       "-0.007150816755254484   -0.005401447507586739   ...\n",
       "-0.007150816755254482   -0.005401447507586882   ...\n",
       "-0.007142305177471765   -0.00493093377855215    ...\n",
       "-0.007142305177471769   -0.004930933778552603   ...\n",
       "-0.0070428361631046385  -0.0037195933179921224  ...\n",
       "... (30870 total)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val U: RowMatrix = svd.U // The U factor is a RowMatrix.\n",
    "val s: Vector = svd.s // The singular values are stored in a local dense vector.\n",
    "val V: Matrix = svd.V // The V factor is a local dense matrix.\n",
    "val S = Matrices.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2496:==================================================>(998 + 2) / 1000]"
     ]
    }
   ],
   "source": [
    "U.rows.map(m => m.toArray.mkString(\",\")).repartition(1).saveAsTextFile(out_path + \"U.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(V.rowIter.toVector.map(m => m.toArray.mkString(\",\")),1).saveAsTextFile(out_path + \"V.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(S.rowIter.toVector.map(m => m.toArray.mkString(\",\")),1).saveAsTextFile(out_path + \"S.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
