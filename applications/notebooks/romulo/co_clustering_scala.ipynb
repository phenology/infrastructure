{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n",
    "import java.util.Random\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, Tile, UByteCellType}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.hadoop.io.SequenceFile.Writer\n",
    "import org.apache.hadoop.io.{SequenceFile, _}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry, RowMatrix}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_offline_mode = true\n",
       "matrix_offline_mode = true\n",
       "dir_path = hdfs:///user/hadoop/spring-index/light/\n",
       "offline_dir_path = hdfs:///user/pheno/spring-index/light/\n",
       "geoTiff_dir = LeafFinal\n",
       "band_num = 3\n",
       "model_first_year = 1980\n",
       "model_last_year = 1983\n",
       "toBeMasked = true\n",
       "mask_path = hdfs:///user/hadoop/usa_state_masks/california.tif\n",
       "numIterations = 75\n",
       "minClusters = 100\n",
       "maxClusters = 120\n",
       "stepClusters = 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Operation mode\n",
    "var rdd_offline_mode = true\n",
    "var matrix_offline_mode = true\n",
    "\n",
    "//GeoTiffs to be read from \"hdfs:///user/hadoop/spring-index/\"\n",
    "var dir_path = \"hdfs:///user/hadoop/spring-index/light/\"\n",
    "var offline_dir_path = \"hdfs:///user/pheno/spring-index/light/\"\n",
    "var geoTiff_dir = \"LeafFinal\"\n",
    "var band_num = 3\n",
    "\n",
    "//Years between (inclusive) 1989 - 2014\n",
    "var model_first_year = 1980\n",
    "var model_last_year = 1983\n",
    "\n",
    "//Mask\n",
    "val toBeMasked = true\n",
    "val mask_path = \"hdfs:///user/hadoop/usa_state_masks/california.tif\"\n",
    "\n",
    "//Kmeans number of iterations and clusters\n",
    "var numIterations = 75\n",
    "var minClusters = 100\n",
    "var maxClusters = 120\n",
    "var stepClusters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of operation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "single_band = false\n",
       "mask_str = _mask\n",
       "grid0_path = hdfs:///user/pheno/spring-index/light/LeafFinal/grid0_3_mask\n",
       "grid0_index_path = hdfs:///user/pheno/spring-index/light/LeafFinal/grid0_index_3_mask\n",
       "grids_noNaN_path = hdfs:///user/pheno/spring-index/light/LeafFinal/grids_noNaN_3_mask\n",
       "metadata_path = hdfs:///user/pheno/spring-index/light/LeafFinal/metadata_3_mask\n",
       "grids_matrix_path = hdfs:///user/pheno/spring-index/light/LeafFinal/grids_matrix_3_mask\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fs: org.apache.hadoop.fs.File...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Validation, do not modify these lines.\n",
    "var single_band = false\n",
    "if (geoTiff_dir == \"BloomFinal\" || geoTiff_dir == \"LeafFinal\") {\n",
    "  single_band = false\n",
    "} else if (geoTiff_dir == \"LastFreeze\" || geoTiff_dir == \"DamageIndex\") {\n",
    "  single_band = true\n",
    "  if (band_num > 0) {\n",
    "    println(\"Since LastFreezze and DamageIndex are single band, we will use band 0!!!\")\n",
    "    band_num  = 0\n",
    "  }\n",
    "} else {\n",
    "  println(\"Directory unknown, please set either BloomFinal, LeafFinal, LastFreeze or DamageIndex!!!\")\n",
    "}\n",
    "\n",
    "if (minClusters > maxClusters) {\n",
    "  maxClusters = minClusters\n",
    "  stepClusters = 1\n",
    "}\n",
    "if (stepClusters < 1) {\n",
    "  stepClusters = 1\n",
    "}\n",
    "\n",
    "//Paths to store data structures for Offline runs\n",
    "var mask_str = \"\"\n",
    "if (toBeMasked)\n",
    "  mask_str = \"_mask\"\n",
    "var grid0_path = offline_dir_path + geoTiff_dir + \"/grid0\" + \"_\" + band_num + mask_str\n",
    "var grid0_index_path = offline_dir_path + geoTiff_dir + \"/grid0_index\" + \"_\" + band_num + mask_str\n",
    "var grids_noNaN_path = offline_dir_path + geoTiff_dir + \"/grids_noNaN\" + \"_\" + band_num + mask_str\n",
    "var metadata_path = offline_dir_path + geoTiff_dir + \"/metadata\" + \"_\" + band_num + mask_str\n",
    "var grids_matrix_path = offline_dir_path + geoTiff_dir + \"/grids_matrix\" + \"_\" + band_num + mask_str\n",
    "\n",
    "//Check offline modes\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "val rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(grid0_path))\n",
    "val matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(grids_matrix_path))\n",
    "\n",
    "if (rdd_offline_mode != rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + rdd_offline_exists.toString())\n",
    "  rdd_offline_mode = rdd_offline_exists\n",
    "}\n",
    "if (matrix_offline_mode != matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + matrix_offline_exists.toString())\n",
    "  matrix_offline_mode = matrix_offline_exists\n",
    "}\n",
    "\n",
    "if (!fs.exists(new org.apache.hadoop.fs.Path(mask_path))) {\n",
    "  println(\"The mask path: \" + mask_path + \" is invalid!!!\")\n",
    "}\n",
    "\n",
    "//Years\n",
    "val model_years = 1980 to 1983\n",
    "\n",
    "if (!model_years.contains(model_first_year) || !(model_years.contains(model_last_year))) {\n",
    "  println(\"Invalid range of years for \" + geoTiff_dir + \". I should be between \" + model_first_year + \" and \" + model_last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "//Global variables\n",
    "var model_years_range = (model_years.indexOf(model_first_year), model_years.indexOf(model_last_year))\n",
    "var projected_extent = new ProjectedExtent(new Extent(0, 0, 0, 0), CRS.fromName(\"EPSG:3857\"))\n",
    "var grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var grids_noNaN_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var num_cols_rows: (Int, Int) = (0, 0)\n",
    "var cellT: CellType = UByteCellType\n",
    "var grids_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var mask_tile0: Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile\n",
    "var grid_cells_size: Long = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to (de)serialize any structure into Array[Byte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serialize: (value: Any)Array[Byte]\n",
       "deserialize: (bytes: Array[Byte])Any\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def serialize(value: Any): Array[Byte] = {\n",
    "  val out_stream: ByteArrayOutputStream = new ByteArrayOutputStream()\n",
    "  val obj_out_stream = new ObjectOutputStream(out_stream)\n",
    "  obj_out_stream.writeObject(value)\n",
    "  obj_out_stream.close\n",
    "  out_stream.toByteArray\n",
    "}\n",
    "\n",
    "def deserialize(bytes: Array[Byte]): Any = {\n",
    "  val obj_in_stream = new ObjectInputStream(new ByteArrayInputStream(bytes))\n",
    "  val value = obj_in_stream.readObject\n",
    "  obj_in_stream.close\n",
    "  value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GeoTiffs\n",
    "\n",
    "Using GeoTrellis all GeoTiffs of a directory will be loaded into a RDD. Using the RDD, we extract a grid from the first file to lated store the Kmeans cluster_IDS, we build an Index for populate such grid and we filter out here all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 12153278619ns                                                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 39860015631537\n",
       "pattern = tif\n",
       "filepath = hdfs:///user/hadoop/spring-index/light/LeafFinal\n",
       "grid_cells_size = 510852\n",
       "t1 = 39872168910156\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39872168910156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var t0 = System.nanoTime()\n",
    "//Load Mask\n",
    "if (toBeMasked) {\n",
    "  val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mask_path).values\n",
    "  val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  mask_tile0 = (mask_tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "}\n",
    "\n",
    "//Local variables\n",
    "val pattern: String = \"tif\"\n",
    "val filepath: String = dir_path + geoTiff_dir\n",
    "\n",
    "if (rdd_offline_mode) {\n",
    "  grids_noNaN_RDD = sc.objectFile(grids_noNaN_path)\n",
    "  grid0 = sc.objectFile(grid0_path)\n",
    "  grid0_index = sc.objectFile(grid0_index_path)\n",
    "\n",
    "  val metadata = sc.sequenceFile(metadata_path, classOf[IntWritable], classOf[BytesWritable]).map(_._2.copyBytes()).collect()\n",
    "  projected_extent = deserialize(metadata(0)).asInstanceOf[ProjectedExtent]\n",
    "  num_cols_rows = (deserialize(metadata(1)).asInstanceOf[Int], deserialize(metadata(2)).asInstanceOf[Int])\n",
    "  cellT = deserialize(metadata(3)).asInstanceOf[CellType]\n",
    "} else {\n",
    "  if (single_band) {\n",
    "    //Lets load a Singleband GeoTiffs and return RDD just with the tiles.\n",
    "    var tiles_RDD: RDD[Tile] = sc.hadoopGeoTiffRDD(filepath, pattern).values\n",
    "\n",
    "    //Retrive the numbre of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = tiles_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    if (toBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "      grids_RDD = tiles_RDD.map(m => m.localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "    } else {\n",
    "      grids_RDD = tiles_RDD.map(m => m.toArrayDouble())\n",
    "    }\n",
    "  } else {\n",
    "    //Lets load Multiband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopMultibandGeoTiffRDD(filepath, pattern).values\n",
    "\n",
    "    //Retrive the numbre of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = tiles_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "    num_cols_rows = (tile0.cols, tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "\n",
    "    //Lets read the average of the Spring-Index which is stored in the 4th band\n",
    "    val band_numB: Broadcast[Int] = sc.broadcast(band_num)\n",
    "    if (toBeMasked) {\n",
    "      val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "      grids_RDD = tiles_RDD.map(m => m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())\n",
    "    } else {\n",
    "      grids_RDD = tiles_RDD.map(m => m.band(band_numB.value).toArrayDouble())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "  val projected_extents_withIndex = sc.hadoopGeoTiffRDD(filepath, pattern).keys.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "  //Get Index for each Cell\n",
    "  val grids_withIndex = grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  if (toBeMasked) {\n",
    "    grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => m._1 != -1000.0).map { case (v, i) => (i) }\n",
    "  } else {\n",
    "    grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i) }\n",
    "\n",
    "  }\n",
    "  //Get the Tile's grid\n",
    "  grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i, v) }\n",
    "\n",
    "  //Lets filter out NaN\n",
    "  if (toBeMasked) {\n",
    "    grids_noNaN_RDD = grids_RDD.map(m => m.filter(m => m != -1000.0))\n",
    "  } else {\n",
    "    grids_noNaN_RDD = grids_RDD\n",
    "  }\n",
    "  //Store data in HDFS\n",
    "  grid0.saveAsObjectFile(grid0_path)\n",
    "  grid0_index.saveAsObjectFile(grid0_index_path)\n",
    "  grids_noNaN_RDD.saveAsObjectFile(grids_noNaN_path)\n",
    "\n",
    "  val grids_noNaN_RDD_withIndex = grids_noNaN_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  grids_noNaN_RDD = grids_noNaN_RDD_withIndex.filterByRange(model_years_range._1, model_years_range._2).values\n",
    "\n",
    "  val writer: SequenceFile.Writer = SequenceFile.createWriter(conf,\n",
    "    Writer.file(metadata_path),\n",
    "    Writer.keyClass(classOf[IntWritable]),\n",
    "    Writer.valueClass(classOf[BytesWritable])\n",
    "  )\n",
    "\n",
    "  writer.append(new IntWritable(1), new BytesWritable(serialize(projected_extent)))\n",
    "  writer.append(new IntWritable(2), new BytesWritable(serialize(num_cols_rows._1)))\n",
    "  writer.append(new IntWritable(3), new BytesWritable(serialize(num_cols_rows._2)))\n",
    "  writer.append(new IntWritable(4), new BytesWritable(serialize(cellT)))\n",
    "  writer.hflush()\n",
    "  writer.close()\n",
    "}\n",
    "grid_cells_size = grid0_index.count().toInt\n",
    "var t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 33515314ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 39873883577290\n",
       "grids_matrix = MapPartitionsRDD[21] at objectFile at <console>:65\n",
       "grid_cells_sizeB = Broadcast(8)\n",
       "t1 = 39873917092604\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39873917092604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "//Global variables\n",
    "var grids_matrix: RDD[Vector] = sc.emptyRDD\n",
    "val grid_cells_sizeB = sc.broadcast(grid_cells_size)\n",
    "\n",
    "if (matrix_offline_mode) {\n",
    "  grids_matrix = sc.objectFile(grids_matrix_path)\n",
    "} else {\n",
    "  //Dense Vector\n",
    "  //val mat: RowMatrix = new RowMatrix(grids_noNaN_RDD.map(m => Vectors.dense(m)))\n",
    "  //Sparse Vector\n",
    "  val mat: RowMatrix = new RowMatrix(grids_noNaN_RDD.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(grid_cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1)))) // Split the matrix into one number per line.\n",
    "  val byColumnAndRow = mat.rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "\n",
    "  val matt: CoordinateMatrix = new CoordinateMatrix(byColumnAndRow)\n",
    "  val matt_T = matt.transpose()\n",
    "  //grids_matrix = matt_T.toRowMatrix().rows\n",
    "  grids_matrix = matt_T.toIndexedRowMatrix().rows.sortBy(_.index).map(_.vector)\n",
    "  grids_matrix.saveAsObjectFile(grids_matrix_path)\n",
    "}\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diag\n",
    "```\n",
    "/*\n",
    "# Create an identity matrix with num of rows and cols equal to numRowC or numColC\n",
    "# Use the diagMask to create a matrix with rows for which the indice is in diagMask.\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diag( dim :Int, diagMaskRDD :RDD[Long]) : CoordinateMatrix = {\n",
    "  /*Build Identity matrix*/\n",
    "  val rows :Array[Long] = Array.fill(dim)(0)\n",
    "  val dimB = sc.broadcast(dim)\n",
    "  val rowsRDD :RDD[Long] = sc.parallelize(rows)\n",
    "  val idenMat = rowsRDD.zipWithIndex().map{\n",
    "    case (v,rowIndex) => (rowIndex, Array.fill(dimB.value)(v).zipWithIndex.map{\n",
    "      case (v, colIndex) => if (rowIndex == colIndex) (colIndex, 1) else (colIndex, 0)\n",
    "    })\n",
    "  }//.flatMap(m => m)\n",
    "  val diaMat = new CoordinateMatrix(idenMat.join(diagMaskRDD.zipWithIndex()).map{case (i,(m,rID)) => m.map{case (colIndex, v) => new MatrixEntry(rID, colIndex,v)}}.flatMap(m => m))\n",
    "  return diaMat\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average\n",
    "```\n",
    "//CALCUALTE AVERAGE\n",
    "/*\n",
    "calculate_average <- function(Left, Z, Right, W, epsilon) {\n",
    "  if (is.null(W)) {\n",
    "    #A 2D array, i.e., a Matrix for which each cell is set with the value 1\n",
    "    W <- array(1, dim(Z))\n",
    "  } else {\n",
    "    # Element-wise multiplication\n",
    "      Z <- W * Z\n",
    "  }\n",
    "  # t(Left) is Matrix transpose\n",
    "  # %*% means matrix mutiplication\n",
    "  # mean(matrix) gives a single value which is the mean of all values in the matrix\n",
    "  # y=mean(x,'r') (or, equivalently, y=mean(x,1)) is the rowwise mean.\n",
    "  # y=mean(x,'c') (or, equivalently, y=mean(x,2)) is the columnwise mean.\n",
    "  # Right + means(matrix) is a element-wise addition\n",
    "    numerator <- t(Left) %*% Z %*% Right + mean(Z) * epsilon\n",
    "\n",
    "  denominator <- t(Left) %*% W %*% Right + epsilon\n",
    "  return(numerator/denominator)\n",
    "}\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_average: (Left: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, Z: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, Right: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, W: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, epsilon: Double)org.apache.spark.mllib.linalg.distributed.CoordinateMatrix\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_average (Left :CoordinateMatrix, Z :CoordinateMatrix, Right :CoordinateMatrix, W :CoordinateMatrix, epsilon :Double): CoordinateMatrix = {\n",
    "  var _W :CoordinateMatrix = null\n",
    "  var _Z : CoordinateMatrix = null\n",
    "  var res : CoordinateMatrix = null\n",
    "   \n",
    "  if (W == null) {\n",
    "    val byColumnAndRow = Z.toRowMatrix().rows.zipWithIndex.map {\n",
    "      case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "        case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, 1)\n",
    "      }\n",
    "    }.flatMap(x => x)\n",
    "    _W = new CoordinateMatrix(byColumnAndRow)\n",
    "    _Z = Z\n",
    "  } else {\n",
    "    //We assume that both rows fit in memory\n",
    "    val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = W.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(Z.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "    _Z = new CoordinateMatrix(joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1*m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "    _W = W\n",
    "  }\n",
    "\n",
    "  val leftT = Left.transpose\n",
    "  val leftT_Z_right = leftT.toBlockMatrix().multiply(_Z.toBlockMatrix().multiply(Right.toBlockMatrix()))\n",
    "  val mean_Z_epsilon = _Z.toRowMatrix().rows.map(m => m.toArray.sum/m.size).reduce( (a,b) => a+b)/_Z.numRows().toDouble * epsilon\n",
    "  val mean_Z_epsilonB = sc.broadcast(mean_Z_epsilon)\n",
    "  val numerator = leftT_Z_right.toIndexedRowMatrix().rows.map( m => m.vector.toArray.map(m => m+mean_Z_epsilonB.value))\n",
    "\n",
    "  val leftT_w_right = leftT.toBlockMatrix().multiply(_W.toBlockMatrix().multiply(Right.toBlockMatrix()))\n",
    "  val epsilonB = sc.broadcast(epsilon)\n",
    "  val denominator = leftT_w_right.toIndexedRowMatrix().rows.map( m => m.vector.toArray.map(m => m+epsilonB.value))\n",
    "   \n",
    "  //We assume the two rows fit in memory\n",
    "  val numerator_denominator :RDD[ (Long, (Array[Double], Array[Double]))] = numerator.zipWithUniqueId().map{ case (v,i) => (i,v)}.join(denominator.zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "  res = new CoordinateMatrix(numerator_denominator.map{ case (row_index,(a,b)) => a.zip(b).map(m => m._1 / m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "\n",
    "  //mean_Z_epsilonB.destroy()\n",
    "  //epsilonB.destroy()\n",
    "  return res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coCavg\n",
    "\n",
    "```\n",
    "/*\n",
    "coCavg <- function(dist, row_col, R, Z, C, W, epsilon) {\n",
    "  CoCavg <- calculate_average(R, Z, C, W, epsilon)\n",
    "  if (row_col==\"row\") {\n",
    "    #Creates a list and names the elements. Such names can then be used to access them in an easy way.\n",
    "    return(list(Zrowc = array(dist, dim(Z)), Zrowv = CoCavg %*% t(C)))\n",
    "  } else if (row_col==\"col\") {\n",
    "    return(list(Zcolc = array(dist, dim(Z)), Zcolv = R %*% CoCavg))\n",
    "  }\n",
    "}\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coCavg: (dist: Double, row_col: String, Right: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, Z: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, C: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, W: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, epsilon: Double)(org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, org.apache.spark.mllib.linalg.distributed.CoordinateMatrix)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def coCavg (dist :Double, row_col :String, Right: CoordinateMatrix, Z: CoordinateMatrix, C: CoordinateMatrix, W: CoordinateMatrix, epsilon: Double) :(CoordinateMatrix,CoordinateMatrix) = {\n",
    "  val CoCavg = calculate_average(Right, Z, C, W, epsilon)\n",
    "  val byColumnAndRow = Z.toRowMatrix().rows.zipWithIndex.map {\n",
    "    case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "      case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, dist)\n",
    "    }\n",
    "  }.flatMap(x => x)\n",
    "  val a = new CoordinateMatrix(byColumnAndRow)\n",
    "  if (row_col.equals(\"row\")) {\n",
    "    (a, CoCavg.toBlockMatrix().multiply(C.toBlockMatrix().transpose).toCoordinateMatrix())\n",
    "  } else {\n",
    "    (a, Right.toBlockMatrix().multiply(CoCavg.toBlockMatrix().transpose).toCoordinateMatrix())\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measure\n",
    "```\n",
    "/*\n",
    "similarity_measure <- function(dist, Z, X, Y, W, epsilon) {\n",
    "  if (is.null(W))\n",
    "  W <- array(1, dim(Z))\n",
    "  if (dist==0) {\n",
    "    # rowSums sum values of Raster objects.\n",
    "    # rep vector several times, but with each we repeat the values, in this case has many as the Z rows\n",
    "      #> rep(1:4, 2)\n",
    "    #  [1] 1 2 3 4 1 2 3 4\n",
    "    # > rep(1:4, each = 2)       # not the same.\n",
    "    #  [1] 1 1 2 2 3 3 4 4\n",
    "    euc <- function(i) rowSums(W * (Z - X - rep(Y[i,], each = dim(Z)[1]))^2)\n",
    "    return(sapply(1:dim(Y)[1], euc))\n",
    "  } else if (dist==1) {\n",
    "    # log(t(Y + epsilon)): sum epsilon to all elements of Y, transpose it and do the log to each element of the transpose matrix\n",
    "    return((W * X) %*% t(Y + epsilon) - (W * Z) %*% log(t(Y + epsilon)))\n",
    "  }\n",
    "}\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "euc: (i: Long, Z_X: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, Y: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, W: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, each: Int)org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def euc (i: Long, Z_X: CoordinateMatrix, Y: CoordinateMatrix, W: CoordinateMatrix, numReps :Int) :RDD[MatrixEntry] = {\n",
    "  val iB = sc.broadcast(i)\n",
    "  val Y_row_i = Y.toRowMatrix().rows.zipWithIndex().filter(_._2 == iB.value).map(_._1.toArray).flatMap(m => m)\n",
    "  var res: RDD[MatrixEntry] = sc.emptyRDD\n",
    "\n",
    "  /*\n",
    "  To represent the rep, i.e., repeat the vector numReps we need to copy each vector value NumReps.\n",
    "  It will create a matrix (Y_row_i.size x numReps). This means we need to transpose the matrix.\n",
    "  Another option is to create a numReps x 1 matrix with value 1 and multiply by Y_row_i matrix (1 x Y_row_i.size)\n",
    "  To avoid transpose we can create the tuples and then do a groupby rowIdx (we need to make sure each row\n",
    "  array is sorted by colIdx).\n",
    "   */\n",
    "  val numRepsB = sc.broadcast(numReps)\n",
    "  val Y_row_i_RDD :RDD[Array[Double]] = Y_row_i.map( m => Array.fill(numRepsB.value)(m))\n",
    "  val Y_row_i_mat = Y_row_i_RDD.zipWithIndex().map{ case (a, colIdx) => a.zipWithIndex.map{ case (v, rowIdx) => (rowIdx, colIdx, v)}}\n",
    "  val rep_Y_row_i = Y_row_i_mat.flatMap( m => m).groupBy(_._1).map{ case (rowIdx, it) => it.toArray.sortBy(_._2).map(_._3)}\n",
    "\n",
    "  val Z_X_rep_Y_joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = Z_X.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(rep_Y_row_i.zipWithUniqueId().map{ case (v,i) => (i,v)})\n",
    "  val Z_X_rep_Y = new CoordinateMatrix(Z_X_rep_Y_joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => math.pow(m._1-m._2,2)).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "\n",
    "  val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = W.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(Z_X_rep_Y.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "  res = joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1-m._2)}.map( m => (iB.value,m.sum)).zipWithIndex.map{ case ((row_index, v),col_index) => new MatrixEntry(row_index, col_index,v)}\n",
    "\n",
    "  numRepsB.destroy()\n",
    "  iB.destroy()\n",
    "  return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "similarity_measure: (dist: Double, Z: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, X: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, Y: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, W: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, epsilon: Double)org.apache.spark.mllib.linalg.distributed.CoordinateMatrix\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def similarity_measure(dist :Double, Z : CoordinateMatrix, X: CoordinateMatrix, Y: CoordinateMatrix, W: CoordinateMatrix, epsilon :Double) :CoordinateMatrix = {\n",
    "  var _W :CoordinateMatrix = null\n",
    "  var res :CoordinateMatrix = null\n",
    "\n",
    "  if (W == null) {\n",
    "    val byColumnAndRow = Z.toRowMatrix().rows.zipWithIndex.map {\n",
    "      case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "        case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, 1)\n",
    "      }\n",
    "    }.flatMap(x => x)\n",
    "\n",
    "    _W = new CoordinateMatrix(byColumnAndRow)\n",
    "  } else {\n",
    "    _W = W\n",
    "  }\n",
    "  if (dist == 0) {\n",
    "    val Z_X_joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = Z.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(X.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "    val X_Z = new CoordinateMatrix(Z_X_joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1-m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "    val Z_rows = Z.numRows().toInt\n",
    "\n",
    "    var resRDD :RDD[MatrixEntry] = sc.emptyRDD\n",
    "\n",
    "    /*\n",
    "    The apply creates a result for each element of the vector.\n",
    "    Hence, each iteration creates a column for the new table.\n",
    "    */\n",
    "    for (i <- 0 until (Y.numRows().toInt)) {\n",
    "      if (resRDD.isEmpty()) {\n",
    "        resRDD = euc(i, X_Z, Y, _W, Z_rows)\n",
    "      } else {\n",
    "        resRDD = resRDD.union(euc(i, X_Z, Y, _W, Z_rows))\n",
    "      }\n",
    "    }\n",
    "    res = new CoordinateMatrix(resRDD)\n",
    "  } else {\n",
    "    val W_X_joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = _W.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(X.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "    val W_X = new CoordinateMatrix(W_X_joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1*m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "\n",
    "    val epsilonB = sc.broadcast(epsilon)\n",
    "    val Y_epsilonT = new CoordinateMatrix(Y.toIndexedRowMatrix().rows.map(m => m.vector.toArray.map(m => m+epsilonB.value)).zipWithIndex().map{ case (a, row_index) => a.zipWithIndex.map{ case (v, col_index) => new MatrixEntry(row_index, col_index, v)}}.flatMap(m => m)).transpose()\n",
    "\n",
    "    val W_Z_joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = _W.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(Z.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "    val W_Z = new CoordinateMatrix(W_Z_joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1*m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "\n",
    "    val log_Y_epsilonT = new CoordinateMatrix(Y_epsilonT.toIndexedRowMatrix().rows.map(m => m.vector.toArray.map(m => math.log(m))).zipWithIndex().map{ case (a, row_index) => a.zipWithIndex.map{ case (v, col_index) => new MatrixEntry(row_index, col_index, v)}}.flatMap(m => m)).transpose()\n",
    "\n",
    "    val W_X_Y_epsilonT = W_X.toBlockMatrix().multiply(Y_epsilonT.toBlockMatrix()).toCoordinateMatrix()\n",
    "    val W_Z_log_Y_epsilonT = W_Z.toBlockMatrix().multiply(log_Y_epsilonT.toBlockMatrix()).toCoordinateMatrix()\n",
    "\n",
    "    val joined_mat :RDD[ (Long, (Array[Double], Array[Double]))] = W_X_Y_epsilonT.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)}.join(W_Z_log_Y_epsilonT.toRowMatrix().rows.map(_.toArray).zipWithUniqueId().map{case (v,i) => (i,v)})\n",
    "    res = new CoordinateMatrix(joined_mat.map {case (row_index, (a,b)) => a.zip(b).map(m => m._1-m._2).zipWithIndex.map{ case (v,col_index) => new MatrixEntry(row_index, col_index,v)}}.flatMap(m => m))\n",
    "    epsilonB.destroy()\n",
    "  }\n",
    "\n",
    "  return res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Cluster\n",
    "```\n",
    "/*\n",
    "assign_cluster <- function(dist, Z, X, Y, W, epsilon) {\n",
    "  D <- similarity_measure(dist, Z, X, Y, W, epsilon)\n",
    "  # apply(Matrix, <row or col>, func) -> <row or col> 1 is row-wise, 2 is col-wise\n",
    "  # sapply is like lapply (it applies a function to each element of a list and the result is also a list) consumes data as a vector.\n",
    "  # dim(D)[1] gives number of rows.\n",
    "  # sort a each row increasing order and return index, we get the indice of the highest value\n",
    "    id <- sapply(1:dim(D)[1], function(i) sort(D[i,], index.return = TRUE)$ix[1])\n",
    "  res <- sapply(1:dim(D)[1], function(i) sort(D[i,])[1]^(2-dist))\n",
    "\n",
    "  # Create an identity matrix, diag(dim(Y)[1]), which has num_rows and num_cols = dim(Y)[1], i.e., number of rows of Y, and set diagonal to 1.\n",
    "  # dim(Y)[1])[id,] -> Give me the row from the identity matrix which has indice \"id\"\n",
    "  return(list(Cluster = diag(dim(Y)[1])[id,], Error = sum(res)))\n",
    "}\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assign_cluster: (dist: Int, Z: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, X: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, Y: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, W: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, epsilon: Double)(org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry], Double)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def assign_cluster (dist: Int, Z :CoordinateMatrix, X: CoordinateMatrix, Y: CoordinateMatrix, W: CoordinateMatrix, epsilon :Double) :(CoordinateMatrix, Double) = {\n",
    "  val D = similarity_measure(dist, Z, X, Y, W, epsilon)\n",
    "  val id = D.toRowMatrix().rows.map(_.toArray.zipWithIndex.sortBy(_._1).map(_._2).head)\n",
    "  val dist2 :Int = 2-dist\n",
    "  val dist2B = sc.broadcast(dist2)\n",
    "  val res = D.toRowMatrix().rows.map( m => math.pow(m.toArray.sorted.head, dist2B.value))\n",
    "\n",
    "  //dist2B.destroy()\n",
    "  (diag(Y.numRows().toInt, id.map(_.toLong)), res.reduce((a,b) => a+b))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/*\n",
    "bbac <- function(Z, numRowC, numColC, W = NULL, distance = \"euclidean\", errobj = 1e-6, niters = 100, nruns = 5, epsilon = 1e-8) {\n",
    "  error <- Inf\n",
    "  error_now <- Inf\n",
    "  dist <- pmatch(tolower(distance), c(\"euclidean\",\"divergence\")) - 1\n",
    "\n",
    "  for (r in 1:nruns) {\n",
    "    # Initialization of R and C\n",
    "    #Create an identity matrix\n",
    "    # Get row which is a random sample (betwewn numRowC and num of rows) with replacement\n",
    "    R <- diag(numRowC)[base::sample(numRowC, dim(Z)[1], replace = TRUE),]\n",
    "    C <- diag(numColC)[base::sample(numColC, dim(Z)[2], replace = TRUE),]\n",
    "\n",
    "    for (s in 1:niters) {\n",
    "      # Row estimation\n",
    "        rs <- coCavg(dist, \"row\", R, Z, C, W, epsilon)\n",
    "      ra <- assign_cluster(dist, Z, rs$Zrowc, rs$Zrowv, W, epsilon)\n",
    "      R  <- ra$Cluster\n",
    "\n",
    "      # Column estimation\n",
    "        cs <- coCavg(dist, \"col\", R, Z, C, W, epsilon)\n",
    "      ca <- assign_cluster(dist, t(Z), t(cs$Zcolc), t(cs$Zcolv), W, epsilon)\n",
    "      C  <- ca$Cluster\n",
    "\n",
    "      #\n",
    "      if (abs(ca$Error - error_now) < errobj) {\n",
    "        status <- paste(\"converged in\",s,\"iterations\")\n",
    "        return(list(R = R, C = C, status = status))\n",
    "      }\n",
    "\n",
    "      # Update objective value\n",
    "      error_now <- ca$Error\n",
    "\n",
    "    }\n",
    "\n",
    "    # Keep pair with min error\n",
    "    if (error_now < error) {\n",
    "      R_star <- R\n",
    "      C_star <- C\n",
    "      error <- error_now\n",
    "    }\n",
    "  }\n",
    "\n",
    "  status <- paste(\"reached maximum of\", niters, \"iterations\")\n",
    "  return(list(R = R_star, C = C_star, status = status))\n",
    "}\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bbac: (Z: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, numRowC: Int, numColC: Int, W: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, distance: String, errobj: Double, niters: Int, nruns: Int, epsilon: Double)(org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, org.apache.spark.mllib.linalg.distributed.CoordinateMatrix, String)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bbac (Z :CoordinateMatrix, numRowC: Int, numColC :Int, W :CoordinateMatrix, distance :String, errobj :Double, niters :Int, nruns :Int, epsilon :Double) :(CoordinateMatrix, CoordinateMatrix, String) = {\n",
    "  var error :Double = Double.MaxValue\n",
    "  var error_now :Double = Double.MaxValue\n",
    "  var status :String = \"\"\n",
    "  var R_star :CoordinateMatrix = null\n",
    "  var C_star :CoordinateMatrix = null\n",
    "  var R :CoordinateMatrix = null\n",
    "  var C :CoordinateMatrix = null\n",
    "  var dim = 0\n",
    "  var rndUpper = 0\n",
    "  var rndLength = 0\n",
    "  var rnd :Random = null\n",
    "  var diagMask :Array[Long] = null\n",
    "  var diagMaskRDD :RDD[Long] = sc.emptyRDD\n",
    "  val dist = if (distance.toLowerCase.equals(\"euclidean\")) 0 else 1 // \"divergence\"\n",
    "\n",
    "  for (r <- 0 until nruns) {\n",
    "    //# Define an array of size (dim(Z)[1] or dim(Z)[2]) filled with random numbers (with replacement) between 1 and numRowC\n",
    "    //[base::sample(numRowC, dim(Z)[1], replace = TRUE),]\n",
    "    dim = numRowC\n",
    "    rndUpper = numRowC.toInt\n",
    "    rndLength = Z.numRows().toInt\n",
    "    rnd = new Random\n",
    "\n",
    "    /*Build Mask, i.e., [base::sample(numRowC, dim(Z)[1], replace = TRUE),]*/\n",
    "    //In scala random gives numbers between 0 inclusive and Upper exclusive.\n",
    "    diagMask = Array.fill(rndLength)(rnd.nextInt(rndUpper))\n",
    "    diagMaskRDD = sc.parallelize(diagMask)\n",
    "    R = diag(dim, diagMaskRDD)\n",
    "\n",
    "    dim = numColC\n",
    "    rndUpper = numColC.toInt\n",
    "    rndLength = Z.numCols().toInt\n",
    "    rnd = new Random\n",
    "\n",
    "    /*Build Mask, i.e., [base::sample(numRowC, dim(Z)[1], replace = TRUE),]*/\n",
    "    //In scala random gives numbers between 0 inclusive and Upper exclusive.\n",
    "    diagMask = Array.fill(rndLength)(rnd.nextInt(rndUpper))\n",
    "    diagMaskRDD = sc.parallelize(diagMask)\n",
    "    C = diag (dim, diagMaskRDD)\n",
    "\n",
    "    for (s <- 0 until niters) {\n",
    "      //Row estimation\n",
    "      val rs = coCavg(dist, \"row\", R, Z, C, W, epsilon)\n",
    "      val ra = assign_cluster(dist, Z, rs._1, rs._2, W, epsilon)\n",
    "      R  = ra._1\n",
    "\n",
    "      //Column estimation\n",
    "      val cs = coCavg(dist, \"col\", R, Z, C, W, epsilon)\n",
    "      val ca = assign_cluster(dist, Z.transpose(), cs._1.transpose(), cs._2.transpose(), W, epsilon)\n",
    "      C  = ca._1\n",
    "\n",
    "      if (math.abs(ca._2 - error_now) < errobj) {\n",
    "        val status = \"converged in \" + s + \" iterations\"\n",
    "        return (R, C, status)\n",
    "      }\n",
    "\n",
    "      //Update objective value\n",
    "      error_now = ca._2\n",
    "    }\n",
    "\n",
    "    //Keep pair with min error\n",
    "    if (error_now < error) {\n",
    "      R_star = R\n",
    "      C_star = C\n",
    "      error = error_now\n",
    "    }\n",
    "  }\n",
    "  status = \"reached maximum of \" + niters + \" iterations\"\n",
    "  (R_star, C_star, status)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 131:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NullPointerException\n",
       "Message: null\n",
       "StackTrace:   at euc(<console>:56)\n",
       "  at $anonfun$similarity_measure$1.apply$mcVI$sp(<console>:73)\n",
       "  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n",
       "  at similarity_measure(<console>:71)\n",
       "  at assign_cluster(<console>:51)\n",
       "  at $anonfun$bbac$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(<console>:85)\n",
       "  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n",
       "  at $anonfun$bbac$1.apply$mcVI$sp(<console>:82)\n",
       "  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n",
       "  at bbac(<console>:72)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//bbac <- function(Z, numRowC, numColC, W = NULL, distance = \"euclidean\", errobj = 1e-6, niters = 100, nruns = 5, epsilon = 1e-8) {\n",
    "val matrixDF = new CoordinateMatrix(grids_matrix.map(_.toArray).zipWithIndex().map{ case (a, row_index) => a.zipWithIndex.map{case (v,col_index) => new MatrixEntry(row_index, col_index, v)}}.flatMap(m => m))\n",
    "val numRowC = 200\n",
    "val numColC = 3\n",
    "var W :CoordinateMatrix = null\n",
    "val distance = \"euclidean\" //Or divergence\n",
    "val errobj :Double = 1e-6  //1e-6\n",
    "val niters = 1\n",
    "val nruns = 1\n",
    "val epsilon :Double = 1e-8 //1e-8\n",
    "\n",
    "var R:CoordinateMatrix = null\n",
    "var C:CoordinateMatrix = null\n",
    "var status :String = \"\"\n",
    "\n",
    "val res_bbac = bbac(matrixDF, numRowC, numColC, W, distance, errobj, niters, nruns, epsilon)\n",
    "R = res_bbac._1\n",
    "C = res_bbac._2\n",
    "status = res_bbac._3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
