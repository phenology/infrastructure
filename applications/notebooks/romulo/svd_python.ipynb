{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVD Python\n",
    "\n",
    "\n",
    "In this NoteBook the reader finds code to read a GeoTiff file, single- or multi-band, from HDFS. It reads the GeoTiff as a **ByteArray** and then stores the GeoTiff in memory using **MemFile** from **RasterIO** python package. Then scipy is used to determine the SVD of a matrix multiplication between two phenology products.\n",
    "\n",
    "With this example the user can load GeoTiffs from HDFS and then explore all the features of Python packages such as [rasterio](https://github.com/mapbox/rasterio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/data/local/jupyterhub/modules/python\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from hdfs import InsecureClient\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#from osgeo import gdal\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "from scipy.linalg import norm, qr, svd\n",
    "from lowrankproduct import lowrankproduct\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"plot_GeoTiff\"\n",
    "masterURL=\"spark://pheno0.phenovari-utwente.surf-hosted.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A new Spark Context will be created.\")\n",
    "\n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugMode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(msg):\n",
    "    if (debugMode):\n",
    "        print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_client():\n",
    "    return InsecureClient(\"pheno0.phenovari-utwente.surf-hosted.nl:50070\", user=\"pheno\",\n",
    "         root=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(directoryPath):\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Running getDataSet(directoryPath)\")\n",
    "    dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"directoryPath: \" + directoryPath)\n",
    "    dprint(\"-------------------------------\")\n",
    "    files = sc.binaryFiles(directoryPath)\n",
    "    fileList = files.keys().collect()\n",
    "    dprint(\"Found files: \" + str(fileList))\n",
    "    dataArray = []\n",
    "    for f in fileList:\n",
    "        data = files.lookup(f)\n",
    "        dataByteArray = bytearray(data[0])\n",
    "        memfile = MemoryFile(dataByteArray)\n",
    "        dataset = memfile.open()\n",
    "        #relevantBand = np.uint8(dataset.read()[0])\n",
    "        relevantBand = np.array(dataset.read()[0])\n",
    "        memfile.close()\n",
    "        dprint(\"relevantBand.shape: \" + str(relevantBand.shape))\n",
    "        flattenedDataSet = relevantBand.flatten()\n",
    "        dprint(\"flattenedDataSet.shape: \" + str(flattenedDataSet.shape))\n",
    "        dataArray.append(flattenedDataSet)\n",
    "    #Pandas appends a vectors as a column to a DataFrame\n",
    "    dataSet = pandas.DataFrame(dataArray).T\n",
    "    maxDimension = max(dataSet.shape)\n",
    "    minDimension = min(dataSet.shape)\n",
    "    dataSetWithIndex = dataSet.reset_index()\n",
    "    dataSetWithoutNan = dataSetWithIndex.dropna(axis = 0, thresh = minDimension)\n",
    "    dataSetIndex = dataSetWithoutNan.index\n",
    "    dataSetWithoutIndex = np.array(dataSetWithoutNan.drop(\"index\", axis = 1))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"Ending getDataSet(directoryPath)\")\n",
    "    dprint(\"-------------------------------\")\n",
    "    return dataSetWithoutIndex, dataSetIndex, maxDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDifferenceUpToSign(vector1, vector2): # Necesarry because algorithm sometimes gives back the negative of the expected result\n",
    "    normDifference = norm(vector1 - vector2)\n",
    "    if normDifference > 1:\n",
    "            normDifference = norm(vector1 + vector2)\n",
    "    return normDifference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMode(resultDir, fileName, i, U, s, V): \n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDir + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    U.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    decompositionFile = open(inFile, \"a\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    s[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    V.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(resultDir, fileName, res):\n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDir + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    res.T.tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest(dataDirectory1, dataDirectory2, resultDir):\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Running test\")\n",
    "    dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")\n",
    "\n",
    "    dataSet1, dataSetIndex1, maxDimension1 = getDataSet(dataDirectory1)\n",
    "    dataSet2, dataSetIndex2, maxDimension2 = getDataSet(dataDirectory2)\n",
    "    dprint(\"dataSet1.shape: \" + str(dataSet1.shape))\n",
    "    dprint(\"dataSet2.shape: \" + str(dataSet2.shape))\n",
    "    maxDimension = max(max(dataSet1.shape), max(dataSet2.shape))\n",
    "    minDimension = min(min(dataSet1.shape), min(dataSet2.shape))\n",
    "    doFullSVD = maxDimension <= 33000\n",
    "    lowRankQ1, lowRankQ2, lowRankProduct = lowrankproduct(dataSet1, dataSet2, p = 0, i = 2, ifgram = False, iffast = True)\n",
    "    lowRankU, lowRankS, lowRankVt = svd(lowRankProduct, full_matrices = False)\n",
    "    lowRankV = lowRankVt.T\n",
    "    lowRankU2 = lowRankQ1 @ lowRankU\n",
    "    lowRankV2 = lowRankQ2 @ lowRankV\n",
    "    new_index1 = pandas.Index(range(maxDimension1), name = \"index\")\n",
    "    new_index2 = pandas.Index(range(maxDimension2), name = \"index\")\n",
    "    lowRankU3 = np.array(pandas.DataFrame(lowRankU2).reindex(dataSetIndex1).reindex(new_index1))\n",
    "    lowRankV3 = np.array(pandas.DataFrame(lowRankV2).reindex(dataSetIndex2).reindex(new_index2))\n",
    "    dprint(\"lowRankU.shape: \" + str(lowRankU.shape))\n",
    "    dprint(\"lowRankS.shape: \" + str(lowRankS.shape))\n",
    "    dprint(\"lowRankV.shape: \" + str(lowRankV.shape))\n",
    "    dprint(\"lowRankU2.shape: \" + str(lowRankU2.shape))\n",
    "    dprint(\"lowRankV2.shape: \" + str(lowRankV2.shape))\n",
    "    dprint(\"lowRankU3.shape: \" + str(lowRankU3.shape))\n",
    "    dprint(\"lowRankV3.shape: \" + str(lowRankV3.shape))\n",
    "    dprint(\"Singular values of low-rank product: \")\n",
    "    dprint(lowRankS)\n",
    "    dprint(\"lowRankU2.T[0][:minDimension]: \")\n",
    "    dprint(lowRankU2.T[0][:minDimension])\n",
    "    dprint(\"lowRankV2.T[0][:minDimension]: \")\n",
    "    dprint(lowRankV2.T[0][:minDimension])\n",
    "    if doFullSVD:\n",
    "        fullProduct = dataSet1 @ dataSet2.T\n",
    "        dprint(\"fullProduct shape \" + str(fullProduct.shape))\n",
    "        dprint(\"ncomponents \" + str(minDimension))\n",
    "        fullU, fullS, fullVt = randomized_svd(fullProduct, n_components = minDimension)\n",
    "        fullV = fullVt.T\n",
    "        dprint(\"fullU.shape: \" + str(fullU.shape))\n",
    "        dprint(\"fullS.shape: \" + str(fullS.shape))\n",
    "        dprint(\"fullV.shape: \" + str(fullV.shape))\n",
    "        dprint(\"Singular values of full product: \")\n",
    "        dprint(fullS)\n",
    "        dprint(\"fullU.T[0][:minDimension]: \")\n",
    "        dprint(fullU.T[0][:minDimension])\n",
    "        dprint(\"fullV.T[0][:minDimension]: \")\n",
    "        dprint(fullV.T[0][:minDimension])\n",
    "    for i in range(len(lowRankS)):\n",
    "        iString = str(i + 1).zfill(2)\n",
    "        if doFullSVD:\n",
    "            u = fullU.T[i]\n",
    "            v = fullV.T[i]\n",
    "        else:\n",
    "            u = dataSet1 @ (dataSet2.T @ lowRankV2.T[i]) / lowRankS[i]\n",
    "            v = dataSet2 @ (dataSet1.T @ lowRankU2.T[i]) / lowRankS[i]\n",
    "        dprint(\"Norm difference u\" + iString + \": \" + str(normDifferenceUpToSign(lowRankU2.T[i], u)))\n",
    "        dprint(\"Norm difference v\" + iString + \": \" + str(normDifferenceUpToSign(lowRankV2.T[i], v)))\n",
    "        writeMode(resultDir, \"ModeWithoutNan\" + iString + \".txt\", i, lowRankU2, lowRankS, lowRankV2)\n",
    "        writeMode(resultDir, \"ModeWithNan\" + iString + \".txt\", i, lowRankU3, lowRankS, lowRankV3)\n",
    "    #lowRankU2.T.tofile(resultDirectory + \"/U.csv\", sep = \",\")\n",
    "    writeCSV(resultDir, \"U.csv\", lowRankU2)\n",
    "    #lowRankS.tofile(resultDirectory + \"/s.csv\", sep = \",\")\n",
    "    writeCSV(resultDir, \"s.csv\", lowRankS)\n",
    "    #lowRankV2.T.tofile(resultDirectory + \"/V.csv\", sep = \",\")\n",
    "    writeCSV(resultDir, \"V.csv\", lowRankV2)\n",
    "\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Ending test\")\n",
    "    dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 1\n",
      "Start time: 2017-10-27 15:53:33.988076\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 15:56:02.866955\n",
      "---------------------------\n",
      "dataSet1.shape: (483850, 37)\n",
      "dataSet2.shape: (483850, 37)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 37\n",
      "l: 37\n",
      "lowRankQ1.shape: (483850, 37)\n",
      "lowRankQ2.shape: (483850, 37)\n",
      "B1.shape: (37, 37)\n",
      "B2.shape: (37, 37)\n",
      "lowRankProduct.shape: (37, 37)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 16:00:47.594624\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 1\n",
      "End time: 2017-10-27 16:05:17.443404\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 1\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomGridmetLeafGridmet/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 1\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 2\n",
      "Start time: 2017-10-27 13:16:20.092836\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 13:16:55.997545\n",
      "---------------------------\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31089, 26)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 26\n",
      "l: 26\n",
      "lowRankQ1.shape: (31089, 26)\n",
      "lowRankQ2.shape: (31089, 26)\n",
      "B1.shape: (26, 26)\n",
      "B2.shape: (26, 26)\n",
      "lowRankProduct.shape: (26, 26)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 13:17:06.911020\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 2\n",
      "End time: 2017-10-27 13:19:24.926910\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 2\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafFinalLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowLeafFinalLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 2\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 3\n",
      "Start time: 2017-10-27 13:06:04.778482\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 13:06:41.716855\n",
      "---------------------------\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31524, 26)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 26\n",
      "l: 26\n",
      "lowRankQ1.shape: (31089, 26)\n",
      "lowRankQ2.shape: (31524, 26)\n",
      "B1.shape: (26, 26)\n",
      "B2.shape: (26, 26)\n",
      "lowRankProduct.shape: (26, 26)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 13:06:51.317871\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 3\n",
      "End time: 2017-10-27 13:09:08.199955\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 3\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowSOSTLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 3\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 4\n",
      "Start time: 2017-10-27 13:11:26.834135\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 13:12:03.052125\n",
      "---------------------------\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31524, 26)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 26\n",
      "l: 26\n",
      "lowRankQ1.shape: (31089, 26)\n",
      "lowRankQ2.shape: (31524, 26)\n",
      "B1.shape: (26, 26)\n",
      "B2.shape: (26, 26)\n",
      "lowRankProduct.shape: (26, 26)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 13:12:14.168737\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 4\n",
      "End time: 2017-10-27 13:14:30.913878\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 4\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/LeafFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/LeafFinalLowSOSTLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 4\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD test ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31524, 26)\n",
      "(31524, 26)\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31524, 26)\n"
     ]
    }
   ],
   "source": [
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowSOSTLow/\"\n",
    "\n",
    "dataSet1, dataSetIndex1, maxDimension1 = getDataSet(dataDirectory1)\n",
    "dataSet2, dataSetIndex2, maxDimension2 = getDataSet(dataDirectory2)\n",
    "\n",
    "print(\"dataSet1.shape: \" + str(dataSet1.shape))\n",
    "print(\"dataSet2.shape: \" + str(dataSet2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullProduct = dataSet1 @ dataSet2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31089, 31524)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullProduct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDimension = min(min(dataSet1.shape), min(dataSet2.shape))\n",
    "randU, randS, randVt = randomized_svd(fullProduct, n_components=minDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normU, normS, normVt = svd(fullProduct, full_matrices = True)\n",
    "normU, normS, normVt = svd(fullProduct, full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
