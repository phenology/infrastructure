{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVD Python\n",
    "\n",
    "\n",
    "In this NoteBook the reader finds code to read a GeoTiff file, single- or multi-band, from HDFS. It reads the GeoTiff as a **ByteArray** and then stores the GeoTiff in memory using **MemFile** from **RasterIO** python package. Then scipy is used to determine the SVD of a matrix multiplication between two phenology products.\n",
    "\n",
    "With this example the user can load GeoTiffs from HDFS and then explore all the features of Python packages such as [rasterio](https://github.com/mapbox/rasterio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#from osgeo import gdal\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "from scipy.linalg import norm, qr, svd\n",
    "#from lowrankproduct import lowrankproduct\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new Spark Context will be created.\n"
     ]
    }
   ],
   "source": [
    "appName = \"plot_GeoTiff\"\n",
    "masterURL=\"spark://pheno0.phenovari-utwente.surf-hosted.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A new Spark Context will be created.\")\n",
    "    \n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(directoryPath):\n",
    "    files = sc.binaryFiles(directoryPath)\n",
    "    dataArray = []\n",
    "    for f in files.keys().collect():\n",
    "        data = files.lookup(f)\n",
    "        dataByteArray = bytearray(data[0])\n",
    "        memfile = MemoryFile(dataByteArray)\n",
    "        dataset = memfile.open()\n",
    "        #relevantBand = np.uint8(dataset.read()[0])\n",
    "        relevantBand = np.array(dataset.read()[0])\n",
    "        memfile.close()\n",
    "        #print(\"relevantBand.shape: \" + str(relevantBand.shape))\n",
    "        flattenedDataSet = relevantBand.flatten()\n",
    "        #print(\"flattenedDataSet.shape: \" + str(flattenedDataSet.shape))\n",
    "        dataArray.append(flattenedDataSet)\n",
    "    \n",
    "    #Pandas appends a vectors as a column to a DataFrame\n",
    "    dataSet = pandas.DataFrame(dataArray).T\n",
    "    print(dataSet.shape)\n",
    "    maxDimension = max(dataSet.shape)\n",
    "    minDimension = min(dataSet.shape)\n",
    "    dataSetWithIndex = dataSet.reset_index()\n",
    "    dataSetWithoutNan = dataSetWithIndex.dropna(axis = 0, thresh = minDimension)\n",
    "    dataSetIndex = dataSetWithoutNan.index\n",
    "    dataSetWithoutIndex = np.array(dataSetWithoutNan.drop(\"index\", axis = 1))\n",
    "    return dataSetWithoutIndex, dataSetIndex, maxDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet1, dataSetIndex1, maxDimension1 = getDataSet(dataDirectory1)\n",
    "dataSet2, dataSetIndex2, maxDimension2 = getDataSet(dataDirectory2)\n",
    "print(\"dataSet1.shape: \" + str(dataSet1.shape))\n",
    "print(\"dataSet2.shape: \" + str(dataSet2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullProduct = dataSet1 @ dataSet2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullProduct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDimension = min(min(dataSet1.shape), min(dataSet2.shape))\n",
    "randU, randS, randVt = randomized_svd(fullProduct, n_components=minDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normU, normS, normVt = svd(fullProduct, full_matrices = False)\n",
    "normU, normS, normVt = svd(fullProduct, full_matrices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
