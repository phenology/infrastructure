{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compare SVD with R-SVD\n",
    "\n",
    "With this example the user can load GeoTiffs from HDFS and then explore all the features of Python packages such as [rasterio](https://github.com/mapbox/rasterio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/data/local/jupyterhub/modules/python\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "from scipy.linalg import norm, qr, svd\n",
    "from productsvd import qrproductsvd\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  new Spark Context will be created.\n"
     ]
    }
   ],
   "source": [
    "appName = \"compare_SVD_RSVD\"\n",
    "#masterURL=\"spark://pheno0.phenovari-utwente.surf-hosted.nl:7077\"\n",
    "masterURL=\"spark://emma0.emma.nlesc.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A  new Spark Context will be created.\")\n",
    "    \n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "This configuration determines whether functions print logs during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugMode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(first, second):\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(msg):\n",
    "    if (debugMode):\n",
    "        print(str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) + \" | \" + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressBar(message, value, endvalue, bar_length = 20):\n",
    "    if (debugMode):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        sys.stdout.write(\"\\r\" \n",
    "                         + str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) \n",
    "                         + \" | \" \n",
    "                         + message \n",
    "                         + \": [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100)))\n",
    "                        )\n",
    "        if value == endvalue:\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDirectory = \"hdfs:///user/emma/svd/spark/BloomGridmetLeafGridmet3/\"\n",
    "EigenVal_path = \"S.csv\"\n",
    "svd_eigenvalues = sc.textFile(resultDirectory + EigenVal_path)\n",
    "EigenVector_path = \"U.csv\"\n",
    "svd_U_vector = sc.textFile(resultDirectory + EigenVector_path)\n",
    "EigenVector_path = \"V.csv\"\n",
    "svd_V_vector = sc.textFile(resultDirectory + EigenVector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1.2051734846237376E11', '0.0', '0.0'], ['0.0', '1.573150206334602E8', '0.0'], ['0.0', '0.0', '9.354714854448391E7']]\n"
     ]
    }
   ],
   "source": [
    "print(str(svd_eigenvalues.map(lambda line: (line.split(','))).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDirectory = \"hdfs:///user/emma/svd/BloomGridmetLeafGridmet/\"\n",
    "EigenVal_path = \"s.csv\"\n",
    "rsvd_eigenvalues = sc.textFile(resultDirectory + EigenVal_path)\n",
    "EigenVector_path = \"U.csv\"\n",
    "rsvd_U_vector = sc.textFile(resultDirectory + EigenVector_path)\n",
    "EigenVector_path = \"V.csv\"\n",
    "rsvd_V_vector = sc.textFile(resultDirectory + EigenVector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['178024103475.1254', '119077846.9692295', '93428727.56734921', '72721591.82347083', '65841756.54620246', '59404710.40743541', '51631282.069282435', '49513903.9558733', '47108951.885175385', '38968643.61893467', '37964497.6647184', '30864611.430567198', '29702882.837573484', '28846459.46894772', '26868885.05725242', '25634563.017505467', '21866384.033159643', '21278468.657219097', '19666948.232925273', '19320181.569853082', '17596932.24841928', '17076063.727075674', '16459195.201867793', '15707948.425088547', '15403646.200111331', '14583482.312037304', '13950787.830484996', '13511287.22838203', '13102844.783520581', '12662716.742046334', '12304538.159589903', '11636398.91255648', '10780714.878158744', '10023492.753480507', '9844759.910824766', '9408355.309497282', '9022289.716617534']]\n"
     ]
    }
   ],
   "source": [
    "print(str(rsvd_eigenvalues.map(lambda line: (line.split(','))).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_U = svd_U_vector.map(lambda line: (line.split(','))).collect()\n",
    "svd_U_np = np.asarray(svd_U).astype(float)\n",
    "\n",
    "rsvd_U = rsvd_U_vector.map(lambda line: (line.split(','))).collect()\n",
    "rsvd_U_np = np.asarray(rsvd_U).astype(float)\n",
    "rsvd_U_np_T = np.reshape(rsvd_U_np,(37,483850)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pc in range(0,3):\n",
    "    print(norm(svd_U_np[:,pc] - rsvd_U_np_T[:,pc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_V = svd_V_vector.map(lambda line: (line.split(','))).collect()\n",
    "svd_V_np = np.asarray(svd_U).astype(float)\n",
    "\n",
    "rsvd_V = rsvd_V_vector.map(lambda line: (line.split(','))).collect()\n",
    "rsvd_V_np = np.asarray(rsvd_V).astype(float)\n",
    "rsvd_V_np_T = np.reshape(rsvd_V_np,(37,483850)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pc in range(0,3):\n",
    "    print(norm(svd_V_np[:,pc] - rsvd_V_np_T[:,pc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
