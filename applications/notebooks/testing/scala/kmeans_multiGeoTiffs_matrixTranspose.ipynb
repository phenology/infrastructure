{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans over a set of GeoTiffs\n",
    "\n",
    "This notebook loads a set of GeoTiffs into a **RDD** of Tiles, with each Tile being a band in the GeoTiff. Each GeoTiff file contains **SpringIndex-** or **LastFreeze-** value for one year over the entire USA.\n",
    "\n",
    "Kmeans takes years as dimensions. Hence, the matrix has cells as rows and the years as columns. To cluster on all years, the matrix needs to be transposed. The notebook has two flavors of matrix transpose, locally by the Spark-driver or distributed using the Spark-workers. Once transposed the matrix is converted to a **RDD** of dense vectors to be used by **Kmeans** algorithm from **Spark-MLlib**. The end result is a grid where each cell has a cluster ID which is then saved into a SingleBand GeoTiff. By saving the result into a GeoTiff, the reader can plot it using a Python notebook as the one defined in the [python examples](../examples/python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.{CellType, ArrayTile, DoubleArrayTile, Tile}\n",
    "import geotrellis.raster.io.geotiff._\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{GeoTiff, SinglebandGeoTiff}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry, RowMatrix}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "//Spire is a numeric library for Scala which is intended to be generic, fast, and precise.\n",
    "import spire.syntax.cfor._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multiple GeoTiffs into a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================================> (35 + 1) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "single_band = false\n",
       "local_mode = false\n",
       "num_cols_rows = (7808,3892)\n",
       "cellT = float64raw\n",
       "grids_RDD = MapPartitionsRDD[8] at map at <console>:91\n",
       "pattern = tif\n",
       "filepath = hdfs:///user/hadoop/spring-index/BloomFinal/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/hadoop/spring-index/BloomFinal/"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val single_band = false\n",
    "val local_mode = false\n",
    "\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var cellT :CellType = CellType.fromName(\"uint8raw\")\n",
    "var grids_RDD :RDD[Array[Double]] = sc.emptyRDD\n",
    "\n",
    "val pattern: String = \"tif\"\n",
    "var filepath: String = \"\"\n",
    "\n",
    "if (single_band) {\n",
    "    //Single band GeoTiff\n",
    "    filepath = \"hdfs:///user/hadoop/spring-index/LastFreeze/\"\n",
    "} else {\n",
    "    //Multi band GeoTiff\n",
    "    filepath = \"hdfs:///user/hadoop/spring-index/BloomFinal/\"\n",
    "}\n",
    "\n",
    "if (single_band) {\n",
    "    //Lets load a Singleband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopGeoTiffRDD(filepath, pattern).values\n",
    "    \n",
    "    //Retrive the numbre of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "    num_cols_rows = (tile0.cols,tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "    \n",
    "    grids_RDD = tiles_RDD.map(m => m.toArrayDouble())\n",
    "} else {\n",
    "    //Lets load Multiband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopMultibandGeoTiffRDD(filepath, pattern).values\n",
    "    \n",
    "    //Retrive the numbre of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "    num_cols_rows = (tile0.cols,tile0.rows)\n",
    "    cellT = tile0.cellType\n",
    "    \n",
    "    //Lets read the average of the Spring-Index which is stored in the 4th band\n",
    "    grids_RDD = tiles_RDD.map(m => m.band(3).toArrayDouble())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata and create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================================> (34 + 1) / 35]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "projected_extent = ProjectedExtent(Extent(-126.30312894720473, 14.29219617034159, -56.162671563152486, 49.25462702827337),geotrellis.proj4.CRS$$anon$3@41d0d1b7)\n",
       "grid0 = MapPartitionsRDD[31] at map at <console>:68\n",
       "grid0_index = MapPartitionsRDD[26] at map at <console>:65\n",
       "grids_noNaN_RDD = MapPartitionsRDD[32] at map at <console>:71\n",
       "projected_extents_withIndex = MapPartitionsRDD[16] at map at <console>:60\n",
       "projected_extent = ProjectedExtent(Extent(-126.30312894720473, 14.29219617034159, -56.162671563152486, 49.25462702827337),geotrellis.pro...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ProjectedExtent(Extent(-126.30312894720473, 14.29219617034159, -56.162671563152486, 49.25462702827337),geotrellis.proj4.CRS$$anon$3@41d0d1b7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Global variables\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var grids_noNaN_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "\n",
    "//Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "val projected_extents_withIndex = sc.hadoopGeoTiffRDD(filepath, pattern).keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect())(0)\n",
    "\n",
    "//Get Index for each Cell\n",
    "val grids_withIndex = grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => !m._1.isNaN).map { case (v, i) => (i) }\n",
    "\n",
    "//Get the Tile's grid\n",
    "grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "//Lets filter out NaN\n",
    "grids_noNaN_RDD = grids_RDD.map(m => m.filter(!_.isNaN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix transpose\n",
    "\n",
    "We need to do a Matrix transpose to have clusters per cell and not per year. With a GeoTiff representing a single year, the loaded data looks liks this:\n",
    "```\n",
    "bands_RDD.map(s => Vectors.dense(s)).cache()\n",
    "\n",
    "//The vectors are rows and therefore the matrix will look like this:\n",
    "[\n",
    "Vectors.dense(0.0, 1.0, 2.0),\n",
    "Vectors.dense(3.0, 4.0, 5.0),\n",
    "Vectors.dense(6.0, 7.0, 8.0),\n",
    "Vectors.dense(9.0, 0.0, 1.0)\n",
    "]\n",
    "```\n",
    "\n",
    "The information was gathered from the blog [how to convert a matrix to a RDD of vectors](http://jacob119.blogspot.nl/2015/11/how-to-convert-matrix-to-rddvector-in.html) and a stackoverflow post on [how to transpose an rdd in spark](https://stackoverflow.com/questions/29390717/how-to-transpose-an-rdd-in-spark).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8:=======================================================> (35 + 1) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grids_vec = MapPartitionsRDD[43] at map at IndexedRowMatrix.scala:90\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[43] at map at IndexedRowMatrix.scala:90"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var grids_vec: RDD[Vector] = sc.emptyRDD\n",
    "\n",
    "//A) For small memory footprint RDDs we can simply bring it to the Driver node and transpose it    \n",
    "if (local_mode) {\n",
    "    //First transpose and then parallelize otherwise you get:\n",
    "    //error: polymorphic expression cannot be instantiated to expected type;\n",
    "    val grids_noNaN_RDD_T = grids_noNaN_RDD.collect().transpose\n",
    "    \n",
    "    //Convert to a RDD\n",
    "    val transposed = sc.parallelize(grids_noNaN_RDD_T)\n",
    "    \n",
    "    //Create a RDD of dense vectors and cache it\n",
    "    grids_vec = transposed.map(m => Vectors.dense(m)).cache()\n",
    "\n",
    "//B) For large memory footpring RDDs we need to run in distributed mode\n",
    "} else {\n",
    "    val mat :RowMatrix = new RowMatrix(grids_noNaN_RDD.map(m => Vectors.dense(m)))\n",
    "\n",
    "    // Split the matrix into one number per line.\n",
    "    val byColumnAndRow = mat.rows.zipWithIndex.map {\n",
    "        case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "            case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "        }   \n",
    "    }.flatMap(x => x)\n",
    "    \n",
    "    val matt: CoordinateMatrix = new CoordinateMatrix(byColumnAndRow)\n",
    "    val matt_T = matt.transpose()\n",
    "    grids_vec = matt_T.toRowMatrix().rows\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tranposed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (single_band) {\n",
    "    val matrix_T_path = \"hdfs:///user/emma/spring_index/LastFreeze/matrix_T\"\n",
    "    grids_vec.saveAsObjectFile(matrix_T_path)\n",
    "} else {\n",
    "    val matrix_T_path = \"hdfs:///user/emma/spring_index/BloomFinal/matrix_T\"\n",
    "    grids_vec.saveAsObjectFile(matrix_T_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transposed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (single_band) {\n",
    "    val matrix_T_path = \"hdfs:///user/emma/spring_index/LastFreeze/matrix_T\"\n",
    "    grids_vec :RDD[Vector] = sc.ObjectFile(matrix_T_path)\n",
    "} else {\n",
    "    val matrix_T_path = \"hdfs:///user/emma/spring_index/BloomFinal/matrix_T\"\n",
    "    grids_vec :RDD[Vector] = sc.ObjectFile(matrix_T_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kmeans training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9:===============================>                         (20 + 8) / 36]"
     ]
    }
   ],
   "source": [
    "val numClusters = 20\n",
    "val numIterations = 25\n",
    "val clusters = {\n",
    "    KMeans.train(grids_vec, numClusters, numIterations)\n",
    "}\n",
    "\n",
    "// Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "val WSSSE = clusters.computeCost(grids_vec)\n",
    "println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n",
    "\n",
    "//Un-persist it to save memory\n",
    "grids_vec.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster model's result management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Lets save the model into HDFS. If the file already exists it will abort and report error.\n",
    "if (single_band) {\n",
    "    val model_path = \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model_\" + numClusters + \"_\" + numIterations\n",
    "    clusters.save(sc, model_path)\n",
    "} else {\n",
    "    val model_path = \"hdfs:///user/emma/spring_index/BloomFinal/all_kmeans_model_\" + numClusters + \"_\" + numIterations\n",
    "    clusters.save(sc, model_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-load the KmeansModel\n",
    "\n",
    "In case the notebook was left running in the background the next task helps the user to re-load the Kmeans Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (30 + 2) / 32]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clusters = org.apache.spark.mllib.clustering.KMeansModel@7a09538b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@7a09538b"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var clusters :KMeansModel = new KMeansModel(Array.empty :Array[Vector])\n",
    "\n",
    "if (single_band) {\n",
    "    val model_path = \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model_\" + numClusters + \"_\" + numIterations\n",
    "    clusters = KMeansModel.load(sc, model_path)\n",
    "} else {\n",
    "    val model_path = \"hdfs:///user/emma/spring_index/BloomFinal/all_kmeans_model_\" + numClusters + \"_\" + numIterations\n",
    "    clusters = KMeansModel.load(sc, model_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Kmeans clustering\n",
    "\n",
    "Run Kmeans and obtain the clusters per each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Cache it so kmeans is more efficient\n",
    "grids_vec.cache()\n",
    "\n",
    "val kmeans_res = clusters.predict(grids_vec)\n",
    "\n",
    "//Un-persist it to save memory\n",
    "grids_vec.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the cluster ID for the first 50 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kmeans_res_out = Array()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmeans_res_out = kmeans_res.take(50)\n",
    "kmeans_res_out.foreach(println)\n",
    "\n",
    "println(kmeans_res_out.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign cluster ID to each grid cell\n",
    "\n",
    "To assign the clusterID to each grid cell it is necessary to get the indices of gird cells they belong to. The process is not straight forward because the ArrayDouble used for the creation of each dense Vector does not contain the NaN values, therefore there is not a direct between the indices in the Tile's grid and the ones in **kmeans_res** (kmeans result).\n",
    "\n",
    "To join the two RDDS the knowledge was obtaing from a stackoverflow post on [how to perform basic joins of two rdd tables in spark using python](https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//The zip operator would be the most appropriated operator for this operation.\n",
    "//However, it requires the RRDs to have the same number of partitions and each partition have the same number of records.\n",
    "//val cluster_cell_pos = res.zip(band0_index)\n",
    "\n",
    "//Since we can't use Zip, we index each RDD and then we join them.\n",
    "val cluster_cell_pos = ((kmeans_res.zipWithIndex().map{ case (v,i) => (i,v)}).join(grid0_index.zipWithIndex().map{ case (v,i) => (i,v)})).map{ case (k,(v,i)) => (v,i)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate a Cluster_IDs to respective Grid_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val grid_clusters = grid0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i.toLong, c)})\n",
    "\n",
    "//Convert all None to NaN\n",
    "val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get.toDouble)}//.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the results in a SingleBand GeoTiff\n",
    "\n",
    "The Grid with the cluster IDs is stored in a SinglBand GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cluster_cells :Array[Double] = grid_clusters_res.values.collect()\n",
    "\n",
    "//Define a Tile\n",
    "val cluster_cellsD = DoubleArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "val cluster_tile = geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "cfor(0)(_ < num_cols_rows._1, _ + 1) { col =>\n",
    "    cfor(0)(_ < num_cols_rows._2, _ + 1) { row =>\n",
    "        val v = cluster_cellsD.get(col, row)\n",
    "        cluster_tile.setDouble(col, row, v)\n",
    "    }\n",
    "}\n",
    "\n",
    "//Create GeoTiff\n",
    "val geoTif = new SinglebandGeoTiff(cluster_tile, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a GeoTiff to **/tmp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var output:String = \"\"\n",
    "val tmp_output = \"/tmp/clusters_\" + numClusters + \"_\" + numIterations + \".tif\"\n",
    "GeoTiffWriter.write(geoTif, tmp_output)\n",
    "\n",
    "// Lets show the result.\n",
    "println(\"Cluster Centers: \")\n",
    "//clusters.clusterCenters.foreach(println)\n",
    "\n",
    "//Lets save the model into HDFS. If the file already exists it will abort and report error.\n",
    "if (single_band) {\n",
    "    val model_path = \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model_\" + numClusters + \"_\" + numIterations\n",
    "    clusters.save(sc, model_path)\n",
    "} else {\n",
    "    val model_path = \"hdfs:///user/emma/spring_index/BloomFinal/all_kmeans_model_\" + numClusters + \"_\" + numIterations\n",
    "    clusters.save(sc, model_path)\n",
    "}\n",
    "\n",
    "if (single_band) {\n",
    "    output = \"/user/emma/spring-index/LastFreeze/clusters_\" + numClusters + \"_\" + numIterations + \".tif\"\n",
    "} else {\n",
    "    output = \"/user/emma/spring-index/BloomFinal/clusters_\" + numClusters + \"_\" + numIterations + \".tif\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a GeoTiff to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cmd = \"hadoop dfs -copyFromLocal -f \" + tmp_output + \" \" + output\n",
    "Process(cmd)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
