{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.io.{IOUtils, SequenceFile}\n",
    "import sys.process._\n",
    "\n",
    "import scala.sys.process.Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test"
     ]
    }
   ],
   "source": [
    "print(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file hdfs:///user/hadoop/TSF_process.x64 is found! :))\n",
      " ------------------------------------------------------------------------\n",
      "   TSF_process\n",
      "   Program for processing time-series data from images or ASCII files\n",
      "   Arguments: settings_file no_of_processors\n",
      " \n",
      "   TIMESAT version 3.3                         \n",
      "   Copyright Per Jonsson and Lars Eklundh           \n",
      "   per.jonsson@mah.se, lars.eklundh@nateko.lu.se    \n",
      "   Feb. 2017                                         \n",
      " ------------------------------------------------------------------------\n",
      "  Error opening image file list\n",
      " /media/vik/799E6DA576F23642/Data NDVI 1km/Img_only/listfile.txt                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                \n",
      "0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_dir_path_hdfs = hdfs:///user/hadoop/\n",
       "file_name = TSF_process.x64\n",
       "file_name_dir = hdfs:///user/hadoop/TSF_process.x64\n",
       "file_name_dir_hdfs = hdfs:/user/hadoop/TSF_process.x64\n",
       "file_name_settings = 200_200_px_USA.set\n",
       "file_name_dir_settings = hdfs:///user/hadoop/200_200_px_USA.set\n",
       "file_name_settings_hdfs = hdfs:/user/hadoop/200_200_px_USA.set\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1754559330_42, ug...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1754559330_42, ugi=mycluster (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var _dir_path_hdfs = \"hdfs:///user/hadoop/\"\n",
    "var file_name = \"TSF_process.x64\"\n",
    "var file_name_dir :String = _dir_path_hdfs + file_name\n",
    "var file_name_dir_hdfs = new org.apache.hadoop.fs.Path(file_name_dir)\n",
    "\n",
    "var file_name_settings = \"200_200_px_USA.set\"\n",
    "var file_name_dir_settings :String = _dir_path_hdfs + file_name_settings\n",
    "var file_name_settings_hdfs = new org.apache.hadoop.fs.Path(file_name_dir_settings)\n",
    "\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "if (fs.exists(file_name_dir_hdfs) && fs.exists(file_name_settings_hdfs)) {\n",
    "    println(\"The file \" + file_name_dir + \" is found! :))\")\n",
    "}else{\n",
    "    println(\"Put the file in user/hadoop/mycluster0 on the HDFS\")\n",
    "}\n",
    "\n",
    "// Connect to Spark\n",
    "var appName = \"phenology_timesat\"\n",
    "var masterURL = \"spark://mycluster0.mydomain:7077\"\n",
    "\n",
    "//A context needs to be created if it does not already exist\n",
    "val conf_spark = new SparkConf().setAppName(appName).setMaster(masterURL)\n",
    "val sc_spark = new SparkContext(conf_spark)\n",
    "\n",
    "\n",
    "// Test\n",
    "// val info = List((\"viktor\", 24), (\"joe\", 30), (\"jack\", 30))\n",
    "// val infoRDD = sc_spark.parallelize(info)\n",
    "// infoRDD.collect().foreach(println)\n",
    "\n",
    "//METHODOLAGY:\n",
    "// The basic idea:\n",
    "//put the executable into HDFS or your local and use addFile to add it into driver, \n",
    "// which will also copy them into workers. \n",
    "//Execute(use SparkFiles.get to get the path from the work executor) to that partition using Process.\n",
    "//To control the input partitionaing: \n",
    "//Data partitioning file as a RDD, and use mapPartitionsWithIndex function to save each partition (????)\n",
    "\n",
    "sc_spark.addFile(file_name_dir)\n",
    "sc_spark.addFile(file_name_dir_settings)\n",
    "\n",
    "val sparkScriptPath: String =  org.apache.spark.SparkFiles.get(file_name)\n",
    "val sparkSettingsPath: String =  org.apache.spark.SparkFiles.get(file_name_settings)\n",
    "\n",
    "//Execute the external system call\n",
    "val exitCode = Seq(sparkScriptPath,sparkSettingsPath, \"1\").!\n",
    "print(exitCode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_process_single_timmesat: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrrrrrrrrr\n"
     ]
    }
   ],
   "source": [
    "def start_process_single_timmesat(): Unit = {\n",
    "  println(\"barrrrrrrrrr\")\n",
    "}\n",
    "\n",
    "start_process_single_timmesat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
