{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.io.{IOUtils, SequenceFile}\n",
    "import sys.process._\n",
    "\n",
    "import scala.sys.process.Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test"
     ]
    }
   ],
   "source": [
    "print(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file hdfs:///user/hadoop/TSF_process.x64 is found! :))\n",
      "The file hdfs:/user/hadoop/200_200_px_USA_cluster.set is found! :))\n",
      "/data/local/spark/tmp/spark-a2a6ed92-c91a-42e5-ab1f-584f005b9b01/userFiles-09308da0-35dc-4164-bd0a-b37d0d424897/listfile.txt ------------------------------------------------------------------------\n",
      "   TSF_process\n",
      "   Program for processing time-series data from images or ASCII files\n",
      "   Arguments: settings_file no_of_processors\n",
      " \n",
      "   TIMESAT version 3.3                         \n",
      "   Copyright Per Jonsson and Lars Eklundh           \n",
      "   per.jonsson@mah.se, lars.eklundh@nateko.lu.se    \n",
      "   Feb. 2017                                         \n",
      " ------------------------------------------------------------------------\n",
      "  Error opening image file\n",
      " hdfs:///user/hadoop/Img_only/g2_BIOPAR_NDVI_201406010000_NOAM_PROBAV_V2.1_NDVI.\n",
      " img                                                                            \n",
      "                                                                                \n",
      "                                                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_dir_path_hdfs = hdfs:///user/hadoop/\n",
       "file_name = TSF_process.x64\n",
       "file_name_dir = hdfs:///user/hadoop/TSF_process.x64\n",
       "file_name_dir_hdfs = hdfs:/user/hadoop/TSF_process.x64\n",
       "file_name_settings = 200_200_px_USA_cluster.set\n",
       "file_name_dir_settings = hdfs:///user/hadoop/200_200_px_USA_cluster.set\n",
       "file_name_settings_hdfs = hdfs:/user/hadoop/200_200_px_USA_cluster.set\n",
       "file_name_list_files = listfile.txt\n",
       "file_name_dir_list_files = hdfs:///user/hadoop/Img_only/listfile.txt\n",
       "file_name_list_hdfs = hdfs:/user/hadoop/Img_only/listfile.txt\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var _dir_path_hdfs = \"hdfs:///user/hadoop/\"\n",
    "var file_name = \"TSF_process.x64\"\n",
    "var file_name_dir :String = _dir_path_hdfs + file_name\n",
    "var file_name_dir_hdfs = new org.apache.hadoop.fs.Path(file_name_dir)\n",
    "\n",
    "var file_name_settings = \"200_200_px_USA_cluster.set\"\n",
    "var file_name_dir_settings :String = _dir_path_hdfs + file_name_settings\n",
    "var file_name_settings_hdfs = new org.apache.hadoop.fs.Path(file_name_dir_settings)\n",
    "\n",
    "var file_name_list_files = \"listfile.txt\"\n",
    "var file_name_dir_list_files :String = _dir_path_hdfs + \"Img_only/\" + file_name_list_files\n",
    "var file_name_list_hdfs = new org.apache.hadoop.fs.Path(file_name_dir_list_files)\n",
    "\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "if (fs.exists(file_name_dir_hdfs) && fs.exists(file_name_settings_hdfs)) {\n",
    "    println(\"The file \" + file_name_dir + \" is found! :))\")\n",
    "       println(\"The file \" + file_name_settings_hdfs + \" is found! :))\")\n",
    "}else{\n",
    "    println(\"Put the file in user/hadoop/mycluster0 on the HDFS\")\n",
    "}\n",
    "\n",
    "// Connect to Spark\n",
    "var appName = \"phenology_timesat\"\n",
    "var masterURL = \"spark://mycluster0.mydomain:7077\"\n",
    "\n",
    "//A context needs to be created if it does not already exist\n",
    "val conf_spark = new SparkConf().setAppName(appName).setMaster(masterURL)\n",
    "val sc_spark = new SparkContext(conf_spark)\n",
    "\n",
    "\n",
    "// Test\n",
    "// val info = List((\"viktor\", 24), (\"joe\", 30), (\"jack\", 30))\n",
    "// val infoRDD = sc_spark.parallelize(info)\n",
    "// infoRDD.collect().foreach(println)\n",
    "\n",
    "def update_list_path_location_on_worker(): Unit = {\n",
    "    \n",
    "    sc_spark.addFile(file_name_dir_list_files)\n",
    "    \n",
    "    val sparkListPath: String =  org.apache.spark.SparkFiles.get(file_name_list_files)\n",
    "    print (sparkListPath)\n",
    "\n",
    "    // for each job Spark creates new set of temporary files\n",
    "    //dinamically change the list_of_files path for the worker in the settings file; line number=6\n",
    "    val settings: RDD[String] = sc_spark.textFile(file_name_dir_settings)\n",
    "    \n",
    "    val filteredRdd = settings.zipWithIndex().collect { case (r, i) if (i != 5) => r\n",
    "                                                        case (r, i) if (i == 5) => sparkListPath + \" %Data\" }\n",
    "\n",
    "    if (fs.exists(file_name_settings_hdfs+\"_new\")) {\n",
    "        fs.delete(file_name_settings_hdfs+\"_new\") \n",
    "    }\n",
    "\n",
    "    filteredRdd.coalesce(1).saveAsTextFile(file_name_dir_settings+\"_new\")   \n",
    "}\n",
    "\n",
    "update_list_path_location_on_worker()\n",
    "\n",
    "\n",
    "\n",
    "//METHODOLAGY:\n",
    "// The basic idea:\n",
    "//put the executable into HDFS  and use addFile to add it into driver, \n",
    "// which will also copy them into workers. \n",
    "//Execute(use SparkFiles.get to get the path from the work executor) to that partition using Process.\n",
    "//To control the input partitionaing: \n",
    "//Data partitioning file as a RDD, and use mapPartitionsWithIndex function to save each partition (????)\n",
    "\n",
    "\n",
    "\n",
    "sc_spark.addFile(file_name_dir)\n",
    "sc_spark.addFile(file_name_dir_settings+ \"_new/part-00000\") //ugly\n",
    "\n",
    "val sparkScriptPath: String =  org.apache.spark.SparkFiles.get(file_name)\n",
    "val sparkSettingsPath: String =  org.apache.spark.SparkFiles.get(\"part-00000\")\n",
    "\n",
    "// //Execute the external system call\n",
    "val exitCode = Seq(sparkScriptPath,sparkSettingsPath, \"1\").!\n",
    "\n",
    "//TODO: How to have access from each of the workers to the .img files on HDFS ??\n",
    "//Transport each file to the worker with .addFile ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
