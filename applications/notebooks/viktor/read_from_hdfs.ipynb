{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.io.{IOUtils, SequenceFile}\n",
    "import sys.process._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test"
     ]
    }
   ],
   "source": [
    "print(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file hdfs:///user/hadoop/mycluster0/listfile.txt is found! :))\n",
      "(viktor,24)                                                                     \n",
      "(joe,30)\n",
      "(jack,30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_dir_path_hdfs = hdfs:///user/hadoop/mycluster0/\n",
       "file_name = listfile.txt\n",
       "file_name_dir = hdfs:///user/hadoop/mycluster0/listfile.txt\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_207783209_39, ugi=mycluster (auth:SIMPLE)]]\n",
       "appName = phenolohy_timesat\n",
       "masterURL = spark://mycluster0.mydomain:7077\n",
       "conf_spark = org.apache.spark.SparkConf@51de6bf8\n",
       "sc_spark = org.apache.spark.SparkContext@2ba34c24\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "in...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://192.168.0.101:4041)\" target=\"new_tab\">Spark UI: app-20180315231501-0012</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark app-20180315231501-0012: Some(http://192.168.0.101:4041)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var _dir_path_hdfs = \"hdfs:///user/hadoop/mycluster0/\"\n",
    "var file_name = \"listfile.txt\"\n",
    "var file_name_dir :String = _dir_path_hdfs + file_name\n",
    "\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(file_name_dir))) {\n",
    "    println(\"The file \" + file_name_dir + \" is found! :))\")\n",
    "}else{\n",
    "    println(\"Put the file in user/hadoop/mycluster0 on the HDFS\")\n",
    "}\n",
    "\n",
    "// Connect to Spark\n",
    "var appName = \"phenolohy_timesat\"\n",
    "var masterURL = \"spark://mycluster0.mydomain:7077\"\n",
    "\n",
    "//A context needs to be created if it does not already exist\n",
    "val conf_spark = new SparkConf().setAppName(appName).setMaster(masterURL)\n",
    "val sc_spark = new SparkContext(conf_spark)\n",
    "\n",
    "// Test\n",
    "val info = List((\"viktor\", 24), (\"joe\", 30), (\"jack\", 30))\n",
    "val infoRDD = sc_spark.parallelize(info)\n",
    "infoRDD.collect().foreach(println)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_process_single_timmesat: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrrrrrrrrr\n"
     ]
    }
   ],
   "source": [
    "def start_process_single_timmesat(): Unit = {\n",
    "  println(\"barrrrrrrrrr\")\n",
    "}\n",
    "\n",
    "start_process_single_timmesat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
