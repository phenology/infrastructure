{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Product SVD in Python\n",
    "\n",
    "\n",
    "In this NoteBook the reader finds code to read a GeoTiff file, single- or multi-band, from HDFS. It reads the GeoTiff as a **ByteArray** and then stores the GeoTiff in memory using **MemFile** from **RasterIO** python package. Subsequently, the Python module _productsvd_ is used to determine the SVD of two phenology datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/data/local/jupyterhub/modules/python\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from hdfs import InsecureClient\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#from osgeo import gdal\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "from scipy.linalg import norm, qr, svd\n",
    "from productsvd import qrproductsvd\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugMode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"plot_GeoTiff\"\n",
    "masterURL = \"spark://pheno0.phenovari-utwente.surf-hosted.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A new Spark Context will be created.\")\n",
    "\n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(msg):\n",
    "    if (debugMode):\n",
    "        print(str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) + \" | \" + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressBar(message, value, endvalue, bar_length = 20):\n",
    "    if (debugMode):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        sys.stdout.write(\"\\r\" \n",
    "                         + str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) \n",
    "                         + \" | \" \n",
    "                         + message \n",
    "                         + \": [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100)))\n",
    "                        )\n",
    "        if value == endvalue:\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_client():\n",
    "    return InsecureClient(\"pheno0.phenovari-utwente.surf-hosted.nl:50070\", user=\"pheno\",\n",
    "         root=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(directoryPath):\n",
    "    dprint(\"Running getDataSet(directoryPath)\")\n",
    "    \n",
    "    files = sc.binaryFiles(directoryPath + \"/*.tif\")\n",
    "    fileList = files.keys().collect()\n",
    "    dprint(\"Number of files: \" + str(len(fileList)))\n",
    "    dataSet = []\n",
    "    plotShapes = []\n",
    "    flattenedShapes = []\n",
    "    for i, f in enumerate(fileList):\n",
    "        progressBar(\"Reading files\", i + 1, len(fileList))\n",
    "        data = files.lookup(f)\n",
    "        dataByteArray = bytearray(data[0])\n",
    "        memfile = MemoryFile(dataByteArray)\n",
    "        dataset = memfile.open()\n",
    "        relevantBand = np.array(dataset.read()[0])\n",
    "        memfile.close()\n",
    "        plotShapes.append(relevantBand.shape)\n",
    "        flattenedDataSet = relevantBand.flatten()\n",
    "        flattenedShapes.append(flattenedDataSet.shape)\n",
    "        dataSet.append(flattenedDataSet)\n",
    "    dataSet = np.array(dataSet).T\n",
    "    dprint(\"dataSet.shape: \" + str(dataSet.shape))\n",
    "    \n",
    "    dprint(\"Ending getDataSet(directoryPath)\")\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMask(filePath):\n",
    "    dprint(\"Running getMask(filePath)\")\n",
    "    \n",
    "    mask_data = sc.binaryFiles(filePath).take(1)\n",
    "    mask_byteArray = bytearray(mask_data[0][1])\n",
    "    mask_memfile = MemoryFile(mask_byteArray)\n",
    "    mask_dataset = mask_memfile.open()\n",
    "    mask_data = np.array(mask_dataset.read()[0])\n",
    "    mask_memfile.close()\n",
    "    dprint(\"mask_data.shape: \" + str(mask_data.shape))\n",
    "    \n",
    "    dprint(\"Ending getMask(filePath)\")\n",
    "    return mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataSet(dataSet, maskData):\n",
    "    dprint(\"Running filterDataSet(dataSet, maskIndex)\")\n",
    "    \n",
    "    maskIndex = np.nonzero(maskData.flatten())[0]\n",
    "    dataSetFiltered = dataSet[maskIndex]\n",
    "    dprint(\"dataSetFiltered.shape: \" + str(dataSetFiltered.shape))\n",
    "    \n",
    "    dprint(\"Ending filterDataSet(dataSet, maskIndex)\")\n",
    "    return dataSetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateNorms(dataSet1, dataSet2, U, s, V):\n",
    "    dprint(\"Running validateNorms(dataSet1, dataSet2, U, s, V)\")\n",
    "    \n",
    "    length = len(s)\n",
    "    norms = []\n",
    "    for i in range(length):\n",
    "        progressBar(\"Validating norms\", i + 1, length)\n",
    "        u = dataSet1 @ (dataSet2.T @ V.T[i]) / s[i]\n",
    "        v = dataSet2 @ (dataSet1.T @ U.T[i]) / s[i]\n",
    "        norms.append(norm(U.T[i] - u))\n",
    "        norms.append(norm(V.T[i] - v))\n",
    "    dprint(\"Largest norm difference: \" + str(max(norms)))\n",
    "    \n",
    "    dprint(\"Ending validateNorms(dataSet1, dataSet2, U, s, V)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMode(resultDirectory, fileName, i, U, s, V):\n",
    "    dprint(\"Running writeMode(resultDirectory, fileName, i, U, s, V)\")\n",
    "    \n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDirectory + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    U.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    decompositionFile = open(inFile, \"a\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    s[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    V.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending writeMode(resultDirectory, fileName, i, U, s, V)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(resultDirectory, fileName, res):\n",
    "    dprint(\"Running writeCSV(resultDirectory, fileName, res)\")\n",
    "    \n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDirectory + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    res.T.tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending writeCSV(resultDirectory, fileName, res)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMode(resultDirectory, fileName, singularVector, maskData):\n",
    "    dprint(\"Running plotMode(resultDirectory, fileName, singularVector, maskData)\")\n",
    "    \n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDirectory + fileName\n",
    "    \n",
    "    data = np.zeros(maskData.shape[0] * maskData.shape[1])\n",
    "    maskIndex = np.nonzero(maskData.flatten())[0]\n",
    "    np.put(data, maskIndex, singularVector)\n",
    "    data = np.reshape(data, maskData.shape)\n",
    "    plt.figure(1)\n",
    "    cmap = plt.cm.get_cmap('YlGn')\n",
    "    img = plt.imshow(data.T, cmap = 'YlGn')\n",
    "    plt.colorbar(orientation = 'horizontal')\n",
    "    plt.clim(float(np.min(data.T)), float(np.max(data.T)))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(inFile)\n",
    "    plt.clf()\n",
    "\n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending plotMode(resultDirectory, fileName, singularVector, shape)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory):\n",
    "    dprint(\"Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\")\n",
    "\n",
    "    dataSet1 = getDataSet(dataDirectory1)\n",
    "    dataSet2 = getDataSet(dataDirectory2)\n",
    "    \n",
    "    maskData = getMask(maskFile)\n",
    "    \n",
    "    dataSetFiltered1 = filterDataSet(dataSet1, maskData)\n",
    "    dataSetFiltered2 = filterDataSet(dataSet2, maskData)\n",
    "    \n",
    "    U, s, Vt = qrproductsvd(dataSetFiltered1, dataSetFiltered2)\n",
    "    V = Vt.T\n",
    "    dprint(\"U.shape: \" + str(U.shape))\n",
    "    dprint(\"s.shape: \" + str(s.shape))\n",
    "    dprint(\"V.shape: \" + str(V.shape))\n",
    "    dprint(\"Singular values of product: \")\n",
    "    dprint(str(s))\n",
    "    \n",
    "    validateNorms(dataSetFiltered1, dataSetFiltered2, U, s, V)\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        progressBar(\"Writing modes\", i + 1, len(s))\n",
    "        iString = str(i + 1).zfill(2)\n",
    "        writeMode(resultDirectory, \"Mode\" + iString + \".txt\", i, U, s, V)\n",
    "        plotMode(resultDirectory, \"ModeU\" + iString + \".pdf\", U.T[i], maskData)\n",
    "        plotMode(resultDirectory, \"ModeV\" + iString + \".pdf\", V.T[i], maskData)\n",
    "    writeCSV(resultDirectory, \"U.csv\", U)\n",
    "    writeCSV(resultDirectory, \"s.csv\", s)\n",
    "    writeCSV(resultDirectory, \"V.csv\", V)\n",
    "    \n",
    "    dprint(\"Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 1\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "maskFile = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomGridmetLeafGridmet/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 1\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 2\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOST4Km/\"\n",
    "maskFile = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomGridmetSOST4Km/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 2\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 3\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOST4Km/\"\n",
    "maskFile = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/LeafGridmetSOST4Km/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 3\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
