{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Product SVD in Python\n",
    "\n",
    "\n",
    "In this NoteBook, the reader will find code to load GeoTiff files, single- or multi-band, from HDFS. It reads the GeoTiffs as a **ByteArray**s and then stores the GeoTiffs in memory using **MemFile** from the **RasterIO** Python package. Subsequently, a statistical analysis is performed on each pair of datasets. In particular, the Python module _productsvd_ is used to determine the SVD of the product of the two phenology datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Initialization\n",
    "This section initializes the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependencies\n",
    "Here, all necessary libraries are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/data/local/jupyterhub/modules/python\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from hdfs import InsecureClient\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#from osgeo import gdal\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "import scipy.linalg\n",
    "from productsvd import qrproductsvd\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "This configuration determines whether functions print logs during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugMode = True\n",
    "maxModes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Spark\n",
    "Here, the Spark context is loaded, which allows for a connection to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new Spark Context will be created.\n"
     ]
    }
   ],
   "source": [
    "appName = \"plot_GeoTiff\"\n",
    "masterURL = \"spark://emma0.emma.nlesc.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A new Spark Context will be created.\")\n",
    "\n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "This section defines various functions used in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Support functions\n",
    "These functions support other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(msg):\n",
    "    if (debugMode):\n",
    "        print(str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) + \" | \" + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressBar(message, value, endvalue, bar_length = 20):\n",
    "    if (debugMode):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        sys.stdout.write(\"\\r\" \n",
    "                         + str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) \n",
    "                         + \" | \" \n",
    "                         + message \n",
    "                         + \": [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100)))\n",
    "                        )\n",
    "        if value == endvalue:\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_client():\n",
    "    return InsecureClient(\"emma0.emma.nlesc.nl:50070\", user=\"pheno\",\n",
    "         root=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Read functions\n",
    "These functions allow for the reading of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(directoryPath, bandNum):\n",
    "    dprint(\"Running getDataSet(directoryPath)\")\n",
    "    \n",
    "    files = sc.binaryFiles(directoryPath + \"/*.tif\")\n",
    "    fileList = files.keys().collect()\n",
    "    dprint(\"Number of files: \" + str(len(fileList)))\n",
    "    dataSet = []\n",
    "    plotShapes = []\n",
    "    flattenedShapes = []\n",
    "    for i, f in enumerate(fileList):\n",
    "        progressBar(\"Reading files\", i + 1, len(fileList))\n",
    "        data = files.lookup(f)\n",
    "        dataByteArray = bytearray(data[0])\n",
    "        memfile = MemoryFile(dataByteArray)\n",
    "        dataset = memfile.open()\n",
    "        relevantBand = np.array(dataset.read()[bandNum])\n",
    "        memfile.close()\n",
    "        plotShapes.append(relevantBand.shape)\n",
    "        flattenedDataSet = relevantBand.flatten()\n",
    "        flattenedShapes.append(flattenedDataSet.shape)\n",
    "        dataSet.append(flattenedDataSet)\n",
    "    dataSet = np.array(dataSet).T\n",
    "    dprint(\"dataSet.shape: \" + str(dataSet.shape))\n",
    "    \n",
    "    dprint(\"Ending getDataSet(directoryPath)\")\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMask(filePath):\n",
    "    dprint(\"Running getMask(filePath)\")\n",
    "    \n",
    "    mask_data = sc.binaryFiles(filePath).take(1)\n",
    "    mask_byteArray = bytearray(mask_data[0][1])\n",
    "    mask_memfile = MemoryFile(mask_byteArray)\n",
    "    mask_dataset = mask_memfile.open()\n",
    "    maskTransform = mask_dataset.transform\n",
    "    mask_data = np.array(mask_dataset.read()[0])\n",
    "    mask_memfile.close()\n",
    "    dprint(\"mask_data.shape: \" + str(mask_data.shape))\n",
    "    \n",
    "    dprint(\"Ending getMask(filePath)\")\n",
    "    return mask_data, maskTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Utility functions\n",
    "These functions analyse and manipulate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataSet(dataSet, maskData):\n",
    "    dprint(\"Running filterDataSet(dataSet, maskIndex)\")\n",
    "    \n",
    "    maskIndex = np.nonzero(np.nan_to_num(maskData.flatten()))[0]\n",
    "    dataSetFiltered = dataSet[maskIndex]\n",
    "    dprint(\"dataSetFiltered.shape: \" + str(dataSetFiltered.shape))\n",
    "    \n",
    "    dprint(\"Ending filterDataSet(dataSet, maskIndex)\")\n",
    "    return dataSetFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateNorms(dataSet1, dataSet2, U, s, V):\n",
    "    dprint(\"Running validateNorms(dataSet1, dataSet2, U, s, V)\")\n",
    "    \n",
    "    length = len(s)\n",
    "    norms = []\n",
    "    for i in range(length):\n",
    "        progressBar(\"Validating norms\", i + 1, length)\n",
    "        u = dataSet1 @ (dataSet2.T @ V.T[i]) / s[i]\n",
    "        v = dataSet2 @ (dataSet1.T @ U.T[i]) / s[i]\n",
    "        norms.append(scipy.linalg.norm(U.T[i] - u))\n",
    "        norms.append(scipy.linalg.norm(V.T[i] - v))\n",
    "    dprint(\"Largest norm difference: \" + str(max(norms)))\n",
    "    \n",
    "    dprint(\"Ending validateNorms(dataSet1, dataSet2, U, s, V)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Write functions\n",
    "These functions write data and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSVs(resultDirectory, U, s, V):\n",
    "    dprint(\"Running writeCSV(resultDirectory, U, s, V)\")\n",
    "    \n",
    "    for i, vectorData in enumerate([U, s, V]):\n",
    "        progressBar(\"Writing CSV\", i + 1, 3)\n",
    "        fileName = [\"U\", \"s\", \"V\"][i] + \".csv\"\n",
    "        inFile = \"/tmp/\" + fileName\n",
    "        outFile = resultDirectory + fileName\n",
    "        decompositionFile = open(inFile, \"w\")\n",
    "        vectorData.T.tofile(decompositionFile, sep = \",\")\n",
    "        decompositionFile.close()\n",
    "        #Upload to HDFS\n",
    "        subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])\n",
    "        #Remove from /tmp/\n",
    "        subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending writeCSV(resultDirectory, U, s, V)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSingularValues(resultDirectory, s):\n",
    "    dprint(\"Running plotSingularValues(resultDirectory, s)\")\n",
    "    \n",
    "    fileName = \"s.pdf\"\n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDirectory + fileName\n",
    "    x = range(len(s))\n",
    "    total = s.T @ s\n",
    "    cumulativeValue = 0\n",
    "    valueList = []\n",
    "    cumulativeList = []\n",
    "    for i in x:\n",
    "        value = np.square(s[i]) / total\n",
    "        valueList.append(value)\n",
    "        cumulativeValue = cumulativeValue + value\n",
    "        cumulativeList.append(cumulativeValue)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(x, valueList, \"g^\")\n",
    "    ax2.plot(x, cumulativeList, \"ro\")\n",
    "    ax1.set_xlabel(\"Singular values\")\n",
    "    ax1.set_ylabel(\"Variance explained\", color = \"g\")\n",
    "    ax2.set_ylabel(\"Cumulative variance explained\", color = \"r\")\n",
    "    plt.savefig(inFile)\n",
    "    plt.clf()\n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending plotSingularValues(resultDirectory, s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeModes(resultDirectory, U, s, V):\n",
    "    dprint(\"Running writeModes(resultDirectory, U, s, V)\")\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        progressBar(\"Writing modes\", i + 1, len(s))\n",
    "        fileName = \"Mode\" + str(i + 1).zfill(2) + \".txt\"\n",
    "        inFile = \"/tmp/\" + fileName\n",
    "        outFile = resultDirectory + fileName\n",
    "        decompositionFile = open(inFile, \"w\")\n",
    "        U.T[i].tofile(decompositionFile, sep = \",\")\n",
    "        decompositionFile.close()\n",
    "        decompositionFile = open(inFile, \"a\")\n",
    "        decompositionFile.write(\"\\n\")\n",
    "        s[i].tofile(decompositionFile, sep = \",\")\n",
    "        decompositionFile.write(\"\\n\")\n",
    "        V.T[i].tofile(decompositionFile, sep = \",\")\n",
    "        decompositionFile.close()\n",
    "        #Upload to HDFS\n",
    "        subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "        #Remove from /tmp/\n",
    "        subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending writeModes(resultDirectory, U, s, V)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModes(resultDirectory, U, s, V, maskData, maskTransform):\n",
    "    dprint(\"Running plotModes(resultDirectory, U, s, V, maskData, maskTransform)\")\n",
    "    \n",
    "    plotTemplate = np.full(maskData.shape[0] * maskData.shape[1], np.nan, dtype=np.float64)\n",
    "    maskIndex = np.nonzero(np.nan_to_num(maskData.flatten()))[0]\n",
    "        \n",
    "    for i in range(min(maxModes, len(s))):\n",
    "        progressBar(\"Plotting modes\", i + 1, min(maxModes, len(s)))\n",
    "        for vectorData, vectorName in zip([U, V], [\"U\", \"V\"]):\n",
    "            data = np.copy(plotTemplate)\n",
    "            np.put(data, maskIndex, vectorData.T[i])\n",
    "            data = np.reshape(data, maskData.shape, )\n",
    "                        \n",
    "            fileName = \"Mode\" + vectorName + str(i + 1).zfill(2) + \".tif\"\n",
    "            inFile = \"/tmp/\" + fileName\n",
    "            outFile = resultDirectory + fileName\n",
    "            rasterioPlot = rasterio.open(inFile, \"w\", driver = \"GTiff\", width = data.shape[1], height = data.shape[0], count = 1, dtype = data.dtype, crs = \"EPSG:4326\", transform = maskTransform) #, compress=\"deflate\")\n",
    "            rasterioPlot.write(data, 1)\n",
    "            rasterioPlot.close()\n",
    "            #Upload to HDFS\n",
    "            subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "            #Remove from /tmp/\n",
    "            subprocess.run(['rm', '-fr', inFile])\n",
    "    \n",
    "    dprint(\"Ending plotModes(resultDirectory, U, s, V, maskData, maskTransform)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analysis function\n",
    "This function combines all the necessary steps for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory):\n",
    "    dprint(\"Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\")\n",
    "\n",
    "    dataSet1 = getDataSet(dataDirectory1, bandNum1)\n",
    "    dataSet2 = getDataSet(dataDirectory2, bandNum2)\n",
    "    \n",
    "    if (dataSet2.shape[1] == 26 and dataSet1.shape[1] != 26): # Hack to align time-dimension of SOS with Bloom and Leaf\n",
    "        dataSet1 = dataSet1[:, 8:34]\n",
    "    \n",
    "    dataSet1 = dataSet1[:, 8:34]\n",
    "    dataSet2 = dataSet2[:, 8:34]\n",
    "    maskData, maskTransform = getMask(maskFile)\n",
    "    \n",
    "    dataSetFiltered1 = filterDataSet(dataSet1, maskData)\n",
    "    dataSetFiltered2 = filterDataSet(dataSet2, maskData)\n",
    "    \n",
    "    U, s, Vt = qrproductsvd(dataSetFiltered1, dataSetFiltered2)\n",
    "    V = Vt.T\n",
    "    dprint(\"U.shape: \" + str(U.shape))\n",
    "    dprint(\"s.shape: \" + str(s.shape))\n",
    "    dprint(\"V.shape: \" + str(V.shape))\n",
    "    dprint(\"Singular values of product: \")\n",
    "    dprint(str(s))\n",
    "    \n",
    "    validateNorms(dataSetFiltered1, dataSetFiltered2, U, s, V)\n",
    "    \n",
    "    #plotSingularValues(resultDirectory, s)\n",
    "    #writeModes(resultDirectory, U, s, V)\n",
    "    plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
    "    #writeCSVs(resultDirectory, U, s, V)\n",
    "    \n",
    "    dprint(\"Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "In this section, the various analyses are initiated. Each analysis uses a different pair of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 1\n",
    "This analysis focusses on Bloom and Leaf data from the USA from 1980 to 2016 at a 4K spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-14 15:54:30 | -------------------------------\n",
      "2017-12-14 15:54:30 | Running analysis 1\n",
      "2017-12-14 15:54:30 | -------------------------------\n",
      "2017-12-14 15:54:32 | Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-14 15:54:32 | Running getDataSet(directoryPath)\n",
      "2017-12-14 15:54:34 | Number of files: 37\n",
      "2017-12-14 15:55:14 | Reading files: [------------------->] 100%\n",
      "2017-12-14 15:55:16 | dataSet.shape: (1414560, 37)\n",
      "2017-12-14 15:55:16 | Ending getDataSet(directoryPath)\n",
      "2017-12-14 15:55:16 | Running getDataSet(directoryPath)\n",
      "2017-12-14 15:55:17 | Number of files: 37\n",
      "2017-12-14 15:56:00 | Reading files: [------------------->] 100%\n",
      "2017-12-14 15:56:02 | dataSet.shape: (1414560, 37)\n",
      "2017-12-14 15:56:02 | Ending getDataSet(directoryPath)\n",
      "2017-12-14 15:56:02 | Running getMask(filePath)\n",
      "2017-12-14 15:56:02 | mask_data.shape: (840, 1684)\n",
      "2017-12-14 15:56:02 | Ending getMask(filePath)\n",
      "2017-12-14 15:56:02 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-14 15:56:02 | dataSetFiltered.shape: (483850, 26)\n",
      "2017-12-14 15:56:02 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-14 15:56:02 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-14 15:56:03 | dataSetFiltered.shape: (483850, 26)\n",
      "2017-12-14 15:56:03 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-14 15:56:04 | U.shape: (483850, 26)\n",
      "2017-12-14 15:56:04 | s.shape: (26,)\n",
      "2017-12-14 15:56:04 | V.shape: (483850, 26)\n",
      "2017-12-14 15:56:04 | Singular values of product: \n",
      "2017-12-14 15:56:04 | [  1.20254550e+11   1.57409271e+08   1.13839270e+08   8.15249398e+07\n",
      "   6.20629660e+07   4.76097433e+07   3.69459962e+07   2.93977551e+07\n",
      "   2.24526763e+07   1.97582525e+07   1.72573723e+07   1.42100462e+07\n",
      "   1.36322155e+07   1.23687366e+07   1.15852298e+07   1.01421050e+07\n",
      "   9.46837146e+06   8.97110212e+06   8.43427548e+06   7.53652133e+06\n",
      "   7.11084735e+06   6.39710718e+06   5.99329557e+06   5.29402425e+06\n",
      "   4.92051730e+06   4.56489109e+06]\n",
      "2017-12-14 15:56:04 | Running validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-14 15:56:06 | Validating norms: [------------------->] 100%\n",
      "2017-12-14 15:56:06 | Largest norm difference: 3.228485161363993e-11\n",
      "2017-12-14 15:56:06 | Ending validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-14 15:56:06 | Running plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-14 15:56:18 | Plotting modes: [------------------->] 100%\n",
      "2017-12-14 15:56:23 | Ending plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-14 15:56:23 | Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-14 15:56:23 | -------------------------------\n",
      "2017-12-14 15:56:23 | Ending analysis 1\n",
      "2017-12-14 15:56:23 | -------------------------------\n"
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 1\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "bandNum1 = 3\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "bandNum2 = 3\n",
    "maskFile = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/BloomGridmetLeafGridmet/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 1\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-14 16:18:15 | -------------------------------\n",
      "2017-12-14 16:18:15 | Running analysis 1.b\n",
      "2017-12-14 16:18:15 | -------------------------------\n",
      "2017-12-14 16:18:18 | Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-14 16:18:18 | Running getDataSet(directoryPath)\n",
      "2017-12-14 16:18:19 | Number of files: 37\n",
      "2017-12-14 16:18:21 | Reading files: [>                   ] 5%"
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 1.b\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "bandNum1 = 3\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "bandNum2 = 3\n",
    "maskFile = \"hdfs:///user/hadoop/usa_state_masks/california_4km.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/BloomGridmetLeafGridmetCali/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 1\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2\n",
    "This analysis focusses on Bloom and SOS data from the USA from 1980 to 2016 at a 4K spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-13 12:24:12 | -------------------------------\n",
      "2017-12-13 12:24:12 | Running analysis 2\n",
      "2017-12-13 12:24:12 | -------------------------------\n",
      "2017-12-13 12:24:14 | Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-13 12:24:14 | Running getDataSet(directoryPath)\n",
      "2017-12-13 12:24:22 | Number of files: 37\n",
      "2017-12-13 12:25:05 | Reading files: [------------------->] 100%\n",
      "2017-12-13 12:25:06 | dataSet.shape: (1414560, 37)\n",
      "2017-12-13 12:25:06 | Ending getDataSet(directoryPath)\n",
      "2017-12-13 12:25:06 | Running getDataSet(directoryPath)\n",
      "2017-12-13 12:25:07 | Number of files: 26\n",
      "2017-12-13 12:25:22 | Reading files: [------------------->] 100%\n",
      "2017-12-13 12:25:22 | dataSet.shape: (1414560, 26)\n",
      "2017-12-13 12:25:22 | Ending getDataSet(directoryPath)\n",
      "2017-12-13 12:25:22 | Running getMask(filePath)\n",
      "2017-12-13 12:25:22 | mask_data.shape: (840, 1684)\n",
      "2017-12-13 12:25:22 | Ending getMask(filePath)\n",
      "2017-12-13 12:25:22 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-13 12:25:23 | dataSetFiltered.shape: (483850, 26)\n",
      "2017-12-13 12:25:23 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-13 12:25:23 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-13 12:25:23 | dataSetFiltered.shape: (483850, 26)\n",
      "2017-12-13 12:25:23 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-13 12:25:25 | U.shape: (483850, 26)\n",
      "2017-12-13 12:25:25 | s.shape: (26,)\n",
      "2017-12-13 12:25:25 | V.shape: (483850, 26)\n",
      "2017-12-13 12:25:25 | Singular values of product: \n",
      "2017-12-13 12:25:25 | [  3.71339543e+11   2.51393310e+09   2.01861687e+09   1.45796090e+09\n",
      "   1.21618683e+09   1.02458244e+09   9.43478143e+08   8.30069198e+08\n",
      "   7.36652457e+08   6.32076519e+08   5.35974656e+08   5.14338882e+08\n",
      "   4.85148375e+08   4.55368599e+08   4.25951951e+08   3.88687305e+08\n",
      "   3.84733683e+08   3.65141930e+08   3.15456418e+08   3.01477787e+08\n",
      "   2.84619214e+08   2.72462586e+08   2.46625386e+08   2.22385291e+08\n",
      "   2.15316733e+08   1.42928083e+08]\n",
      "2017-12-13 12:25:25 | Running validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-13 12:25:27 | Validating norms: [------------------->] 100%\n",
      "2017-12-13 12:25:27 | Largest norm difference: 8.095154113830593e-12\n",
      "2017-12-13 12:25:27 | Ending validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-13 12:25:27 | Running plotSingularValues(resultDirectory, s)\n",
      "2017-12-13 12:25:30 | Ending plotSingularValues(resultDirectory, s)\n",
      "2017-12-13 12:25:30 | Running writeModes(resultDirectory, U, s, V)\n",
      "2017-12-13 12:26:53 | Writing modes: [------------------->] 100%\n",
      "2017-12-13 12:26:56 | Ending writeModes(resultDirectory, U, s, V)\n",
      "2017-12-13 12:26:56 | Running plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-13 12:27:05 | Plotting modes: [------------------->] 100%\n",
      "2017-12-13 12:27:10 | Ending plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-13 12:27:10 | Running writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-13 12:27:31 | Writing CSV: [------------------->] 100%\n",
      "2017-12-13 12:27:48 | Ending writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-13 12:27:48 | Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-13 12:27:48 | -------------------------------\n",
      "2017-12-13 12:27:48 | Ending analysis 2\n",
      "2017-12-13 12:27:48 | -------------------------------\n"
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 2\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "bandNum1 = 3\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOST4Km/\"\n",
    "bandNum2 = 0\n",
    "maskFile = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/BloomGridmetSOST4Km/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 2\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 3\n",
    "This analysis focusses on Leaf and SOS data from the USA from 1980 to 2016 at a 4K spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 3\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "bandNum1 = 3\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOST4Km/\"\n",
    "bandNum2 = 0\n",
    "maskFile = \"hdfs:///user/hadoop/usa_mask_gridmet.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/LeafGridmetSOST4Km/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 3\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 4\n",
    "This analysis focusses on BloomFinalLowPR and SOSTLowPR data from the USA from 1989 to 2014 1Km resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-12 10:48:05 | -------------------------------\n",
      "2017-12-12 10:48:05 | Running analysis 4\n",
      "2017-12-12 10:48:05 | -------------------------------\n",
      "2017-12-12 10:48:07 | Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-12 10:48:07 | Running getDataSet(directoryPath)\n",
      "2017-12-12 10:48:11 | Number of files: 26\n",
      "2017-12-12 10:48:37 | Reading files: [------------------->] 100%\n",
      "2017-12-12 10:48:38 | dataSet.shape: (30870, 26)\n",
      "2017-12-12 10:48:38 | Ending getDataSet(directoryPath)\n",
      "2017-12-12 10:48:38 | Running getDataSet(directoryPath)\n",
      "2017-12-12 10:48:39 | Number of files: 26\n",
      "2017-12-12 10:48:58 | Reading files: [------------------->] 100%\n",
      "2017-12-12 10:48:59 | dataSet.shape: (30870, 26)\n",
      "2017-12-12 10:48:59 | Ending getDataSet(directoryPath)\n",
      "2017-12-12 10:48:59 | Running getMask(filePath)\n",
      "2017-12-12 10:48:59 | mask_data.shape: (210, 147)\n",
      "2017-12-12 10:48:59 | Ending getMask(filePath)\n",
      "2017-12-12 10:48:59 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:48:59 | dataSetFiltered.shape: (30870, 26)\n",
      "2017-12-12 10:48:59 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:48:59 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:48:59 | dataSetFiltered.shape: (30870, 26)\n",
      "2017-12-12 10:48:59 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:49:00 | U.shape: (30870, 26)\n",
      "2017-12-12 10:49:00 | s.shape: (26,)\n",
      "2017-12-12 10:49:00 | V.shape: (30870, 26)\n",
      "2017-12-12 10:49:00 | Singular values of product: \n",
      "2017-12-12 10:49:00 | [  1.20587459e+10   1.61809893e+07   7.14959958e+06   5.43663867e+06\n",
      "   3.36146096e+06   2.87319546e+06   2.60868335e+06   1.66709042e+06\n",
      "   1.56316663e+06   1.50092982e+06   1.38065996e+06   1.09581415e+06\n",
      "   9.92082603e+05   8.73109084e+05   8.03291265e+05   6.94849214e+05\n",
      "   6.48117639e+05   5.47592331e+05   5.42842103e+05   4.75190088e+05\n",
      "   4.69942723e+05   4.61514941e+05   3.77371034e+05   2.92247653e+05\n",
      "   2.75634196e+05   1.84255706e+05]\n",
      "2017-12-12 10:49:00 | Running validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-12 10:49:00 | Validating norms: [------------------->] 100%\n",
      "2017-12-12 10:49:00 | Largest norm difference: 4.258566607612194e-12\n",
      "2017-12-12 10:49:00 | Ending validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-12 10:49:00 | Running plotSingularValues(resultDirectory, s)\n",
      "2017-12-12 10:49:02 | Ending plotSingularValues(resultDirectory, s)\n",
      "2017-12-12 10:49:02 | Running writeModes(resultDirectory, U, s, V)\n",
      "2017-12-12 10:49:57 | Writing modes: [------------------->] 100%\n",
      "2017-12-12 10:49:59 | Ending writeModes(resultDirectory, U, s, V)\n",
      "2017-12-12 10:49:59 | Running plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-12 10:50:18 | Plotting modes: [------------------->] 100%\n",
      "2017-12-12 10:50:27 | Ending plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-12 10:50:27 | Running writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-12 10:50:32 | Writing CSV: [------------------->] 100%\n",
      "2017-12-12 10:50:34 | Ending writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-12 10:50:34 | Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-12 10:50:34 | -------------------------------\n",
      "2017-12-12 10:50:34 | Ending analysis 4\n",
      "2017-12-12 10:50:34 | -------------------------------\n"
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 4\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLowPR/\"\n",
    "bandNum1 = 0\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLowPR/\"\n",
    "bandNum2 = 0\n",
    "maskFile = \"hdfs:///user/hadoop/spring-index/BloomFinalLowPR/1989.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/BloomFinalLowPRSOSTLowPR/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 4\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 5\n",
    "This analysis focusses on LeafFinalLowPR and SOSTLowPR data from the USA from 1989 to 2014 1Km resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-12 10:50:34 | -------------------------------\n",
      "2017-12-12 10:50:34 | Running analysis 5\n",
      "2017-12-12 10:50:34 | -------------------------------\n",
      "2017-12-12 10:50:36 | Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-12 10:50:36 | Running getDataSet(directoryPath)\n",
      "2017-12-12 10:50:44 | Number of files: 26\n",
      "2017-12-12 10:51:07 | Reading files: [------------------->] 100%\n",
      "2017-12-12 10:51:08 | dataSet.shape: (30870, 26)\n",
      "2017-12-12 10:51:08 | Ending getDataSet(directoryPath)\n",
      "2017-12-12 10:51:08 | Running getDataSet(directoryPath)\n",
      "2017-12-12 10:51:09 | Number of files: 26\n",
      "2017-12-12 10:51:30 | Reading files: [------------------->] 100%\n",
      "2017-12-12 10:51:31 | dataSet.shape: (30870, 26)\n",
      "2017-12-12 10:51:31 | Ending getDataSet(directoryPath)\n",
      "2017-12-12 10:51:31 | Running getMask(filePath)\n",
      "2017-12-12 10:51:31 | mask_data.shape: (210, 147)\n",
      "2017-12-12 10:51:31 | Ending getMask(filePath)\n",
      "2017-12-12 10:51:31 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:51:31 | dataSetFiltered.shape: (30870, 26)\n",
      "2017-12-12 10:51:31 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:51:31 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:51:31 | dataSetFiltered.shape: (30870, 26)\n",
      "2017-12-12 10:51:31 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:51:31 | U.shape: (30870, 26)\n",
      "2017-12-12 10:51:31 | s.shape: (26,)\n",
      "2017-12-12 10:51:31 | V.shape: (30870, 26)\n",
      "2017-12-12 10:51:31 | Singular values of product: \n",
      "2017-12-12 10:51:31 | [  7.41470894e+09   2.63031158e+07   9.02425892e+06   6.62606689e+06\n",
      "   5.24978714e+06   3.97618140e+06   3.46456570e+06   2.96742328e+06\n",
      "   2.63031152e+06   2.37920646e+06   1.87791908e+06   1.80924544e+06\n",
      "   1.68349978e+06   1.51099198e+06   1.41970179e+06   1.38772949e+06\n",
      "   1.26231120e+06   1.14775291e+06   9.76859389e+05   9.25336223e+05\n",
      "   8.23103195e+05   7.01515433e+05   6.58872327e+05   4.73111161e+05\n",
      "   3.32454318e+05   1.93274133e+05]\n",
      "2017-12-12 10:51:31 | Running validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-12 10:51:31 | Validating norms: [------------------->] 100%\n",
      "2017-12-12 10:51:31 | Largest norm difference: 2.4033564860515393e-12\n",
      "2017-12-12 10:51:31 | Ending validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-12 10:51:31 | Running plotSingularValues(resultDirectory, s)\n",
      "2017-12-12 10:51:33 | Ending plotSingularValues(resultDirectory, s)\n",
      "2017-12-12 10:51:33 | Running writeModes(resultDirectory, U, s, V)\n",
      "2017-12-12 10:52:28 | Writing modes: [------------------->] 100%\n",
      "2017-12-12 10:52:31 | Ending writeModes(resultDirectory, U, s, V)\n",
      "2017-12-12 10:52:31 | Running plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-12 10:52:48 | Plotting modes: [------------------->] 100%\n",
      "2017-12-12 10:52:57 | Ending plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-12 10:52:57 | Running writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-12 10:53:02 | Writing CSV: [------------------->] 100%\n",
      "2017-12-12 10:53:05 | Ending writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-12 10:53:05 | Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-12 10:53:05 | -------------------------------\n",
      "2017-12-12 10:53:05 | Ending analysis 5\n",
      "2017-12-12 10:53:05 | -------------------------------\n"
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 5\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/LeafFinalLowPR/\"\n",
    "bandNum1 = 0\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLowPR/\"\n",
    "bandNum2 = 0\n",
    "maskFile = \"hdfs:///user/hadoop/spring-index/LeafFinalLowPR/1989.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/LeafFinalLowPRSOSTLowPR/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 5\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 6\n",
    "This analysis focusses on BloomFinalLowPR and LeafFinalLowPR data from the USA from 1989 to 2014 1Km resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-12 10:53:05 | -------------------------------\n",
      "2017-12-12 10:53:05 | Running analysis 6\n",
      "2017-12-12 10:53:05 | -------------------------------\n",
      "2017-12-12 10:53:07 | Running runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-12 10:53:07 | Running getDataSet(directoryPath)\n",
      "2017-12-12 10:53:15 | Number of files: 26\n",
      "2017-12-12 10:53:39 | Reading files: [------------------->] 100%\n",
      "2017-12-12 10:53:40 | dataSet.shape: (30870, 26)\n",
      "2017-12-12 10:53:40 | Ending getDataSet(directoryPath)\n",
      "2017-12-12 10:53:40 | Running getDataSet(directoryPath)\n",
      "2017-12-12 10:53:41 | Number of files: 26\n",
      "2017-12-12 10:54:03 | Reading files: [------------------->] 100%\n",
      "2017-12-12 10:54:04 | dataSet.shape: (30870, 26)\n",
      "2017-12-12 10:54:04 | Ending getDataSet(directoryPath)\n",
      "2017-12-12 10:54:04 | Running getMask(filePath)\n",
      "2017-12-12 10:54:04 | mask_data.shape: (210, 147)\n",
      "2017-12-12 10:54:04 | Ending getMask(filePath)\n",
      "2017-12-12 10:54:04 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:54:04 | dataSetFiltered.shape: (30870, 26)\n",
      "2017-12-12 10:54:04 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:54:04 | Running filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:54:04 | dataSetFiltered.shape: (30870, 26)\n",
      "2017-12-12 10:54:04 | Ending filterDataSet(dataSet, maskIndex)\n",
      "2017-12-12 10:54:04 | U.shape: (30870, 26)\n",
      "2017-12-12 10:54:04 | s.shape: (26,)\n",
      "2017-12-12 10:54:04 | V.shape: (30870, 26)\n",
      "2017-12-12 10:54:04 | Singular values of product: \n",
      "2017-12-12 10:54:04 | [  3.68226752e+09   8.06563531e+05   4.85824678e+05   3.10463362e+05\n",
      "   2.30006476e+05   1.61068309e+05   1.12546660e+05   9.62783613e+04\n",
      "   7.68308440e+04   6.31607624e+04   5.01396173e+04   4.52413785e+04\n",
      "   4.27108688e+04   3.49840664e+04   3.29666237e+04   3.12362959e+04\n",
      "   2.51067545e+04   2.44929799e+04   2.18680338e+04   2.03446059e+04\n",
      "   1.83310559e+04   1.52361370e+04   1.48177991e+04   1.34717678e+04\n",
      "   1.03196921e+04   8.50797897e+03]\n",
      "2017-12-12 10:54:04 | Running validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-12 10:54:04 | Validating norms: [------------------->] 100%\n",
      "2017-12-12 10:54:04 | Largest norm difference: 2.909347094357988e-11\n",
      "2017-12-12 10:54:04 | Ending validateNorms(dataSet1, dataSet2, U, s, V)\n",
      "2017-12-12 10:54:04 | Running plotSingularValues(resultDirectory, s)\n",
      "2017-12-12 10:54:06 | Ending plotSingularValues(resultDirectory, s)\n",
      "2017-12-12 10:54:06 | Running writeModes(resultDirectory, U, s, V)\n",
      "2017-12-12 10:55:01 | Writing modes: [------------------->] 100%\n",
      "2017-12-12 10:55:03 | Ending writeModes(resultDirectory, U, s, V)\n",
      "2017-12-12 10:55:03 | Running plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-12 10:55:19 | Plotting modes: [------------------->] 100%\n",
      "2017-12-12 10:55:28 | Ending plotModes(resultDirectory, U, s, V, maskData, maskTransform)\n",
      "2017-12-12 10:55:28 | Running writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-12 10:55:33 | Writing CSV: [------------------->] 100%\n",
      "2017-12-12 10:55:36 | Ending writeCSV(resultDirectory, U, s, V)\n",
      "2017-12-12 10:55:36 | Ending runAnalysis(dataDirectory1, dataDirectory2, maskFile, resultDirectory)\n",
      "2017-12-12 10:55:36 | -------------------------------\n",
      "2017-12-12 10:55:36 | Ending analysis 6\n",
      "2017-12-12 10:55:36 | -------------------------------\n"
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running analysis 6\")\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLowPR/\"\n",
    "bandNum1 = 0\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafFinalLowPR/\"\n",
    "bandNum2 = 0\n",
    "maskFile = \"hdfs:///user/hadoop/spring-index/BloomFinalLowPR/1989.tif\"\n",
    "resultDirectory = \"hdfs:///user/emma/svd/BloomFinalLowPRLeafFinalLowPR/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runAnalysis(dataDirectory1, dataDirectory2, bandNum1, bandNum2, maskFile, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending analysis 6\")\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
