{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Product SVD in Python\n",
    "\n",
    "\n",
    "In this NoteBook the reader finds code to read a GeoTiff file, single- or multi-band, from HDFS. It reads the GeoTiff as a **ByteArray** and then stores the GeoTiff in memory using **MemFile** from **RasterIO** python package. Subsequently, the Python module _productsvd_ is used to determine the SVD of two phenology datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/data/local/jupyterhub/modules/python\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from hdfs import InsecureClient\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#from osgeo import gdal\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "from scipy.linalg import norm, qr, svd\n",
    "from productsvd import qrproductsvd\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugMode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"plot_GeoTiff\"\n",
    "masterURL=\"spark://pheno0.phenovari-utwente.surf-hosted.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A new Spark Context will be created.\")\n",
    "\n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(msg):\n",
    "    if (debugMode):\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_client():\n",
    "    return InsecureClient(\"pheno0.phenovari-utwente.surf-hosted.nl:50070\", user=\"pheno\",\n",
    "         root=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressBar(message, value, endvalue, bar_length = 20):\n",
    "    if (debugMode):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        sys.stdout.write(\"\\r\" + message + \": [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
    "        if value == endvalue:\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(directoryPath):\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Running getDataSet(directoryPath)\")\n",
    "    dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"directoryPath: \" + directoryPath)\n",
    "    dprint(\"-------------------------------\")\n",
    "    files = sc.binaryFiles(directoryPath + \"/*.tif\")\n",
    "    fileList = files.keys().collect()\n",
    "    dprint(\"Number of files: \" + str(len(fileList)))\n",
    "    dataArray = []\n",
    "    plotShapes = []\n",
    "    flattenedShapes = []\n",
    "    for i, f in enumerate(fileList):\n",
    "        progressBar(\"Reading files\", i + 1, len(fileList))\n",
    "        data = files.lookup(f)\n",
    "        dataByteArray = bytearray(data[0])\n",
    "        memfile = MemoryFile(dataByteArray)\n",
    "        dataset = memfile.open()\n",
    "        relevantBand = np.array(dataset.read()[0])\n",
    "        memfile.close()\n",
    "        plotShapes.append(relevantBand.shape)\n",
    "        flattenedDataSet = relevantBand.flatten()\n",
    "        flattenedShapes.append(flattenedDataSet.shape)\n",
    "        dataArray.append(flattenedDataSet)\n",
    "    #Pandas appends a vectors as a column to a DataFrame\n",
    "    # Check if plotShapes & flattenedShapes all equal\n",
    "    dataSet = pandas.DataFrame(dataArray).T\n",
    "    maxDimension = max(dataSet.shape)\n",
    "    minDimension = min(dataSet.shape)\n",
    "    dataSetWithIndex = dataSet.reset_index()\n",
    "    dataSetWithoutNan = dataSetWithIndex.dropna(axis = 0, thresh = minDimension)\n",
    "    dataSetIndex = dataSetWithoutNan.index\n",
    "    dataSetWithoutIndex = np.array(dataSetWithoutNan.drop(\"index\", axis = 1))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"Ending getDataSet(directoryPath)\")\n",
    "    dprint(\"-------------------------------\")\n",
    "    return dataSetWithoutIndex, dataSetIndex, maxDimension, plotShapes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDifferenceUpToSign(vector1, vector2): # Necesarry because algorithm sometimes gives back the negative of the expected result\n",
    "    normDifference = norm(vector1 - vector2)\n",
    "    if normDifference > 1:\n",
    "            normDifference = norm(vector1 + vector2)\n",
    "    return normDifference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateNorms(dataSet1, dataSet2, U, s, V):\n",
    "    length = len(s)\n",
    "    norms = []\n",
    "    for i in range(length):\n",
    "        progressBar(\"Validating norms\", i + 1, length)\n",
    "        u = dataSet1 @ (dataSet2.T @ V.T[i]) / s[i]\n",
    "        v = dataSet2 @ (dataSet1.T @ U.T[i]) / s[i]\n",
    "        norms.append(normDifferenceUpToSign(U.T[i], u))\n",
    "        norms.append(normDifferenceUpToSign(V.T[i], v))\n",
    "    return max(norms) < 10^-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMode(resultDir, fileName, i, U, s, V): \n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDir + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    U.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    decompositionFile = open(inFile, \"a\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    s[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    V.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(resultDir, fileName, res):\n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDir + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    res.T.tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMode(singularVector, shape):\n",
    "    data = np.reshape(singularVector, shape)\n",
    "    plt.figure(1)\n",
    "    cmap = plt.cm.get_cmap('YlGn')\n",
    "    img = plt.imshow(data.T, cmap = 'YlGn')\n",
    "    plt.colorbar(orientation = 'horizontal')\n",
    "    plt.clim(float(np.min(data.T)), float(np.max(data.T)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest(dataDirectory1, dataDirectory2, resultDirectory):\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Running runTest(dataDirectory1, dataDirectory2, resultDirectory)\")\n",
    "    dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"dataDirectory1: \" + dataDirectory1)\n",
    "    dprint(\"dataDirectory2: \" + dataDirectory2)\n",
    "    dprint(\"resultDirectory: \" + resultDirectory)\n",
    "    dprint(\"-------------------------------\")\n",
    "\n",
    "    dataSet1, dataSetIndex1, maxDimension1, plotShape1 = getDataSet(dataDirectory1)\n",
    "    dataSet2, dataSetIndex2, maxDimension2, plotShape2 = getDataSet(dataDirectory2)\n",
    "    dprint(\"dataSet1.shape: \" + str(dataSet1.shape))\n",
    "    dprint(\"dataSet2.shape: \" + str(dataSet2.shape))\n",
    "    maxDimension = max(max(dataSet1.shape), max(dataSet2.shape))\n",
    "    minDimension = min(min(dataSet1.shape), min(dataSet2.shape))\n",
    "    U, s, Vt = qrproductsvd(dataSet1, dataSet2)\n",
    "    V = Vt.T\n",
    "    new_index1 = pandas.Index(range(maxDimension1), name = \"index\")\n",
    "    new_index2 = pandas.Index(range(maxDimension2), name = \"index\")\n",
    "    UWithNan = np.array(pandas.DataFrame(U).reindex(dataSetIndex1).reindex(new_index1))\n",
    "    VWithNan = np.array(pandas.DataFrame(V).reindex(dataSetIndex2).reindex(new_index2))\n",
    "    dprint(\"U.shape: \" + str(U.shape))\n",
    "    dprint(\"s.shape: \" + str(s.shape))\n",
    "    dprint(\"V.shape: \" + str(V.shape))\n",
    "    dprint(\"UWithNan.shape: \" + str(UWithNan.shape))\n",
    "    dprint(\"VWithNan.shape: \" + str(VWithNan.shape))\n",
    "    dprint(\"Singular values of product: \")\n",
    "    dprint(s)\n",
    "    dprint(\"U.T[0][:minDimension]: \")\n",
    "    dprint(U.T[0][:minDimension])\n",
    "    dprint(\"V.T[0][:minDimension]: \")\n",
    "    dprint(V.T[0][:minDimension])\n",
    "    validNorms = validateNorms(dataSet1, dataSet2, U, s, V)\n",
    "    dprint(\"Valid norms: \" + str(validNorms))\n",
    "    for i in range(len(s)):\n",
    "        iString = str(i + 1).zfill(2)\n",
    "        writeMode(resultDirectory, \"ModeWithoutNan\" + iString + \".txt\", i, U, s, V)\n",
    "        writeMode(resultDirectory, \"ModeWithNan\" + iString + \".txt\", i, UWithNan, s, VWithNan)\n",
    "        plotMode(UWithNan.T[i], plotShape1)\n",
    "    writeCSV(resultDirectory, \"U.csv\", U)\n",
    "    writeCSV(resultDirectory, \"s.csv\", s)\n",
    "    writeCSV(resultDirectory, \"V.csv\", V)\n",
    "\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Ending test\")\n",
    "    dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running test 1\")\n",
    "dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomGridmetLeafGridmet/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending test 1\")\n",
    "dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running test 2\")\n",
    "dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafFinalLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowLeafFinalLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending test 2\")\n",
    "dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running test 3\")\n",
    "dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowSOSTLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending test 3\")\n",
    "dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running test 4\")\n",
    "dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/LeafFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/LeafFinalLowSOSTLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending test 4\")\n",
    "dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running test 5\")\n",
    "dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLowPR/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLowPR/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/LeafFinalLowPRSOSTLowPR/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending test 5\")\n",
    "dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
