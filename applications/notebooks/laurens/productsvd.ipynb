{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVD Python\n",
    "\n",
    "\n",
    "In this NoteBook the reader finds code to read a GeoTiff file, single- or multi-band, from HDFS. It reads the GeoTiff as a **ByteArray** and then stores the GeoTiff in memory using **MemFile** from **RasterIO** python package. Then scipy is used to determine the SVD of a matrix multiplication between two phenology products.\n",
    "\n",
    "With this example the user can load GeoTiffs from HDFS and then explore all the features of Python packages such as [rasterio](https://github.com/mapbox/rasterio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/data/local/jupyterhub/modules/python\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from hdfs import InsecureClient\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#from osgeo import gdal\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio import plot\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy import exp, log\n",
    "from numpy.random import standard_normal\n",
    "from scipy.linalg import norm, qr, svd\n",
    "from productsvd import qrproductsvd\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new Spark Context will be created.\n"
     ]
    }
   ],
   "source": [
    "appName = \"plot_GeoTiff\"\n",
    "masterURL=\"spark://pheno0.phenovari-utwente.surf-hosted.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"A new Spark Context will be created.\")\n",
    "\n",
    "sc = SparkContext(conf = SparkConf().setAppName(appName).setMaster(masterURL))\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugMode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(msg):\n",
    "    if (debugMode):\n",
    "        print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_client():\n",
    "    return InsecureClient(\"pheno0.phenovari-utwente.surf-hosted.nl:50070\", user=\"pheno\",\n",
    "         root=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(directoryPath):\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Running getDataSet(directoryPath)\")\n",
    "    dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"directoryPath: \" + directoryPath)\n",
    "    dprint(\"-------------------------------\")\n",
    "    files = sc.binaryFiles(directoryPath)\n",
    "    fileList = files.keys().collect()\n",
    "    dprint(\"Found files: \" + str(fileList))\n",
    "    dataArray = []\n",
    "    for f in fileList:\n",
    "        data = files.lookup(f)\n",
    "        dataByteArray = bytearray(data[0])\n",
    "        memfile = MemoryFile(dataByteArray)\n",
    "        dataset = memfile.open()\n",
    "        relevantBand = np.array(dataset.read()[0])\n",
    "        memfile.close()\n",
    "        dprint(\"relevantBand.shape: \" + str(relevantBand.shape))\n",
    "        flattenedDataSet = relevantBand.flatten()\n",
    "        dprint(\"flattenedDataSet.shape: \" + str(flattenedDataSet.shape))\n",
    "        dataArray.append(flattenedDataSet)\n",
    "    #Pandas appends a vectors as a column to a DataFrame\n",
    "    dataSet = pandas.DataFrame(dataArray).T\n",
    "    maxDimension = max(dataSet.shape)\n",
    "    minDimension = min(dataSet.shape)\n",
    "    dataSetWithIndex = dataSet.reset_index()\n",
    "    dataSetWithoutNan = dataSetWithIndex.dropna(axis = 0, thresh = minDimension)\n",
    "    dataSetIndex = dataSetWithoutNan.index\n",
    "    dataSetWithoutIndex = np.array(dataSetWithoutNan.drop(\"index\", axis = 1))\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"Ending getDataSet(directoryPath)\")\n",
    "    dprint(\"-------------------------------\")\n",
    "    return dataSetWithoutIndex, dataSetIndex, maxDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDifferenceUpToSign(vector1, vector2): # Necesarry because algorithm sometimes gives back the negative of the expected result\n",
    "    normDifference = norm(vector1 - vector2)\n",
    "    if normDifference > 1:\n",
    "            normDifference = norm(vector1 + vector2)\n",
    "    return normDifference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMode(resultDir, fileName, i, U, s, V): \n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDir + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    U.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    decompositionFile = open(inFile, \"a\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    s[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.write(\"\\n\")\n",
    "    V.T[i].tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(resultDir, fileName, res):\n",
    "    inFile = \"/tmp/\" + fileName\n",
    "    outFile = resultDir + fileName\n",
    "    \n",
    "    decompositionFile = open(inFile, \"w\")\n",
    "    res.T.tofile(decompositionFile, sep = \",\")\n",
    "    decompositionFile.close()\n",
    "    \n",
    "    #Upload to HDFS\n",
    "    subprocess.run(['hadoop', 'dfs', '-copyFromLocal', '-f', inFile, outFile])  \n",
    "\n",
    "    #Remove from /tmp/\n",
    "    subprocess.run(['rm', '-fr', inFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest(dataDirectory1, dataDirectory2, resultDir):\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Running test\")\n",
    "    dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")\n",
    "\n",
    "    dataSet1, dataSetIndex1, maxDimension1 = getDataSet(dataDirectory1)\n",
    "    dataSet2, dataSetIndex2, maxDimension2 = getDataSet(dataDirectory2)\n",
    "    dprint(\"dataSet1.shape: \" + str(dataSet1.shape))\n",
    "    dprint(\"dataSet2.shape: \" + str(dataSet2.shape))\n",
    "    maxDimension = max(max(dataSet1.shape), max(dataSet2.shape))\n",
    "    minDimension = min(min(dataSet1.shape), min(dataSet2.shape))\n",
    "    doFullSVD = maxDimension <= 33000\n",
    "    U, s, Vt = qrproductsvd(dataSet1, dataSet2)\n",
    "    V = Vt.T\n",
    "    new_index1 = pandas.Index(range(maxDimension1), name = \"index\")\n",
    "    new_index2 = pandas.Index(range(maxDimension2), name = \"index\")\n",
    "    UWithNan = np.array(pandas.DataFrame(U).reindex(dataSetIndex1).reindex(new_index1))\n",
    "    VWithNan = np.array(pandas.DataFrame(V).reindex(dataSetIndex2).reindex(new_index2))\n",
    "    dprint(\"U.shape: \" + str(U.shape))\n",
    "    dprint(\"s.shape: \" + str(s.shape))\n",
    "    dprint(\"V.shape: \" + str(V.shape))\n",
    "    dprint(\"UWithNan.shape: \" + str(UWithNan.shape))\n",
    "    dprint(\"VWithNan.shape: \" + str(VWithNan.shape))\n",
    "    dprint(\"Singular values of low-rank product: \")\n",
    "    dprint(s)\n",
    "    dprint(\"U.T[0][:minDimension]: \")\n",
    "    dprint(U.T[0][:minDimension])\n",
    "    dprint(\"V.T[0][:minDimension]: \")\n",
    "    dprint(V.T[0][:minDimension])\n",
    "    if doFullSVD:\n",
    "        fullProduct = dataSet1 @ dataSet2.T\n",
    "        dprint(\"fullProduct shape \" + str(fullProduct.shape))\n",
    "        dprint(\"ncomponents \" + str(minDimension))\n",
    "        fullU, fullS, fullVt = randomized_svd(fullProduct, n_components = minDimension)\n",
    "        fullV = fullVt.T\n",
    "        dprint(\"fullU.shape: \" + str(fullU.shape))\n",
    "        dprint(\"fullS.shape: \" + str(fullS.shape))\n",
    "        dprint(\"fullV.shape: \" + str(fullV.shape))\n",
    "        dprint(\"Singular values of full product: \")\n",
    "        dprint(fullS)\n",
    "        dprint(\"fullU.T[0][:minDimension]: \")\n",
    "        dprint(fullU.T[0][:minDimension])\n",
    "        dprint(\"fullV.T[0][:minDimension]: \")\n",
    "        dprint(fullV.T[0][:minDimension])\n",
    "    for i in range(len(lowRankS)):\n",
    "        iString = str(i + 1).zfill(2)\n",
    "        if doFullSVD:\n",
    "            u = fullU.T[i]\n",
    "            v = fullV.T[i]\n",
    "        else:\n",
    "            u = dataSet1 @ (dataSet2.T @ V.T[i]) / s[i]\n",
    "            v = dataSet2 @ (dataSet1.T @ U.T[i]) / s[i]\n",
    "        dprint(\"Norm difference u\" + iString + \": \" + str(normDifferenceUpToSign(U.T[i], u)))\n",
    "        dprint(\"Norm difference v\" + iString + \": \" + str(normDifferenceUpToSign(V.T[i], v)))\n",
    "        writeMode(resultDir, \"ModeWithoutNan\" + iString + \".txt\", i, U, s, V)\n",
    "        writeMode(resultDir, \"ModeWithNan\" + iString + \".txt\", i, UWithNan, s, VWithNan)\n",
    "    #lowRankU2.T.tofile(resultDirectory + \"/U.csv\", sep = \",\")\n",
    "    writeCSV(resultDir, \"U.csv\", U)\n",
    "    #lowRankS.tofile(resultDirectory + \"/s.csv\", sep = \",\")\n",
    "    writeCSV(resultDir, \"s.csv\", s)\n",
    "    #lowRankV2.T.tofile(resultDirectory + \"/V.csv\", sep = \",\")\n",
    "    writeCSV(resultDir, \"V.csv\", V)\n",
    "\n",
    "    dprint(\"-------------------------------\")\n",
    "    dprint(\"Ending test\")\n",
    "    dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "    dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 1\n",
      "Start time: 2017-11-06 16:10:35.899213\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Running test\n",
      "Start time: 2017-11-06 16:10:41.740134\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Running getDataSet(directoryPath)\n",
      "Start time: 2017-11-06 16:10:41.741054\n",
      "-------------------------------\n",
      "directoryPath: hdfs:///user/hadoop/spring-index/BloomGridmet/\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1b5e52c3129a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hadoop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dfs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-mkdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultDirectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrunTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDirectory1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataDirectory2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultDirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-396a2a338034>\u001b[0m in \u001b[0;36mrunTest\u001b[0;34m(dataDirectory1, dataDirectory2, resultDir)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdataSet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataSetIndex1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDimension1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDirectory1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataSet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataSetIndex2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDimension2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDirectory2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataSet1.shape: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5d776a8f6aca>\u001b[0m in \u001b[0;36mgetDataSet\u001b[0;34m(directoryPath)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectoryPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfileList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found files: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdataArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Running test 1\")\n",
    "dprint(\"Start time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomGridmet/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafGridmet/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomGridmetLeafGridmet/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "dprint(\"-------------------------------\")\n",
    "dprint(\"Ending test 1\")\n",
    "dprint(\"End time: \" + str(datetime.datetime.now()))\n",
    "dprint(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 2\n",
      "Start time: 2017-10-27 13:16:20.092836\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 13:16:55.997545\n",
      "---------------------------\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31089, 26)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 26\n",
      "l: 26\n",
      "lowRankQ1.shape: (31089, 26)\n",
      "lowRankQ2.shape: (31089, 26)\n",
      "B1.shape: (26, 26)\n",
      "B2.shape: (26, 26)\n",
      "lowRankProduct.shape: (26, 26)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 13:17:06.911020\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 2\n",
      "End time: 2017-10-27 13:19:24.926910\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 2\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/spring-index/LeafFinalLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowLeafFinalLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 2\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 3\n",
      "Start time: 2017-10-27 13:06:04.778482\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 13:06:41.716855\n",
      "---------------------------\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31524, 26)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 26\n",
      "l: 26\n",
      "lowRankQ1.shape: (31089, 26)\n",
      "lowRankQ2.shape: (31524, 26)\n",
      "B1.shape: (26, 26)\n",
      "B2.shape: (26, 26)\n",
      "lowRankProduct.shape: (26, 26)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 13:06:51.317871\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 3\n",
      "End time: 2017-10-27 13:09:08.199955\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 3\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowSOSTLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 3\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 4\n",
      "Start time: 2017-10-27 13:11:26.834135\n",
      "-------------------------------\n",
      "---------------------------\n",
      "Running lowrankproduct(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "Start time: 2017-10-27 13:12:03.052125\n",
      "---------------------------\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31524, 26)\n",
      "p: 0\n",
      "i: 2\n",
      "ifgram: False\n",
      "iffast: True\n",
      "---------------------------\n",
      "k: 26\n",
      "l: 26\n",
      "lowRankQ1.shape: (31089, 26)\n",
      "lowRankQ2.shape: (31524, 26)\n",
      "B1.shape: (26, 26)\n",
      "B2.shape: (26, 26)\n",
      "lowRankProduct.shape: (26, 26)\n",
      "-------------------------------\n",
      "Ending lowrankproductsvd(dataSet1, dataSet2, p, i, ifgram, iffast)\n",
      "End time: 2017-10-27 13:12:14.168737\n",
      "-------------------------------\n",
      "-------------------------------\n",
      "Ending test 4\n",
      "End time: 2017-10-27 13:14:30.913878\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 4\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/LeafFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/LeafFinalLowSOSTLow/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 4\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Running test 5\n",
      "Start time: 2017-10-27 16:46:10.109739\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-a98fecbaf10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hadoop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dfs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-mkdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultDirectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrunTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDirectory1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataDirectory2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultDirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-ad7849584eeb>\u001b[0m in \u001b[0;36mrunTest\u001b[0;34m(dataDirectory1, dataDirectory2, resultDir)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdataSet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataSetIndex1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDimension1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDirectory1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataSet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataSetIndex2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDimension2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDirectory2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataSet1.shape: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ed9b3458bd05>\u001b[0m in \u001b[0;36mgetDataSet\u001b[0;34m(directoryPath)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectoryPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfileList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found files: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdataArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------\")\n",
    "print(\"Running test 5\")\n",
    "print(\"Start time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLowPR/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLowPR/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/LeafFinalLowPRSOSTLowPR/\"\n",
    "\n",
    "#Create Result dir\n",
    "subprocess.run(['hadoop', 'dfs', '-mkdir', resultDirectory])\n",
    "\n",
    "runTest(dataDirectory1, dataDirectory2, resultDirectory)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"Ending test 5\")\n",
    "print(\"End time: \" + str(datetime.datetime.now()))\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD test ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31524, 26)\n",
      "(31524, 26)\n",
      "dataSet1.shape: (31089, 26)\n",
      "dataSet2.shape: (31524, 26)\n"
     ]
    }
   ],
   "source": [
    "dataDirectory1 = \"hdfs:///user/hadoop/spring-index/BloomFinalLow/\"\n",
    "dataDirectory2 = \"hdfs:///user/hadoop/avhrr/SOSTLow/\"\n",
    "resultDirectory = \"hdfs:///user/pheno/svd/BloomFinalLowSOSTLow/\"\n",
    "\n",
    "dataSet1, dataSetIndex1, maxDimension1 = getDataSet(dataDirectory1)\n",
    "dataSet2, dataSetIndex2, maxDimension2 = getDataSet(dataDirectory2)\n",
    "\n",
    "print(\"dataSet1.shape: \" + str(dataSet1.shape))\n",
    "print(\"dataSet2.shape: \" + str(dataSet2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullProduct = dataSet1 @ dataSet2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31089, 31524)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullProduct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDimension = min(min(dataSet1.shape), min(dataSet2.shape))\n",
    "randU, randS, randVt = randomized_svd(fullProduct, n_components=minDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normU, normS, normVt = svd(fullProduct, full_matrices = True)\n",
    "normU, normS, normVt = svd(fullProduct, full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
