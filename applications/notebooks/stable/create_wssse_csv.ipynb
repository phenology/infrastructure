{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV file\n",
    "\n",
    "This notebook show how to load an Array of triples stored as objectFile and save it again as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.io.{IOUtils, SequenceFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file hdfs:///user/emma/avhrr/SOST/wssse.csv already exists we will delete it!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "offline_dir_path = hdfs:///user/emma/avhrr/\n",
       "geoTiff_dir = SOST\n",
       "wssse_path = hdfs:///user/emma/avhrr/SOST/wssse\n",
       "wssse_csv_path = hdfs:///user/emma/avhrr/SOST/wssse.csv\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_148277917_36, ugi=emma (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var offline_dir_path = \"hdfs:///user/emma/avhrr/\"\n",
    "var geoTiff_dir = \"SOST\"\n",
    "var wssse_path :String = offline_dir_path + geoTiff_dir + \"/wssse\"\n",
    "var wssse_csv_path :String = offline_dir_path + geoTiff_dir + \"/wssse.csv\"\n",
    "\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_csv_path))) {\n",
    "    println(\"The file \" + wssse_csv_path + \" already exists we will delete it!!!\")\n",
    "    try { fs.delete(new org.apache.hadoop.fs.Path(wssse_csv_path), true) } catch { case _ : Throwable => { } }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List((2,35,6.281697927369641E11), (3,35,5.843197255442158E11), (4,35,5.574289149814163E11), (5,35,5.276943809948959E11), (6,35,5.136070634290901E11), (7,35,4.9937888440249774E11), (8,35,4.9311821404946277E11), (9,35,4.838515055580271E11), (10,35,4.798542547476693E11), (11,35,4.7323297056703436E11), (12,35,4.671958128254339E11), (13,35,4.5866040054326794E11), (14,35,4.5548726263775037E11), (15,35,4.515019533644096E11), (16,35,4.4583101100674805E11), (17,35,4.438638846034233E11), (18,35,4.3967035532729364E11), (19,35,4.3996804002525604E11), (20,35,4.334172575635536E11), (21,35,4.331638238508667E11), (22,35,4.295550702489807E11), (23,35,4.282566779032344E11), (24,35,4.2326081676395526E11), (25,35,4.2136332411314667E11))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wssse_data = MapPartitionsRDD[2] at objectFile at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at objectFile at <console>:49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wssse_data :RDD[(Int, Int, Double)] = sc.emptyRDD\n",
    "\n",
    "//from disk\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_path))) {\n",
    "    wssse_data = sc.objectFile(wssse_path)\n",
    "    println(wssse_data.collect().toList)        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wssse = MapPartitionsRDD[10] at map at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wssse = wssse_data.repartition(1).sortBy(_._1).map{case (a,b,c) => Array(a,b,c).mkString(\",\")}\n",
    "wssse.saveAsTextFile(wssse_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
