{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV file\n",
    "\n",
    "This notebook show how to load an Array of triples stored as objectFile and save it again as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.io.{IOUtils, SequenceFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "offline_dir_path = hdfs:///user/pheno/avhrr/\n",
       "geoTiff_dir = SOST\n",
       "wssse_path = hdfs:///user/pheno/avhrr/SOST/75_wssse\n",
       "wssse_csv_path = hdfs:///user/pheno/avhrr/SOST/75_wssse.csv\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_475836789_36, ugi=pheno (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var offline_dir_path = \"hdfs:///user/pheno/avhrr/\"\n",
    "//var offline_dir_path = \"hdfs:///user/pheno/spring-index/\"\n",
    "var geoTiff_dir = \"SOST\"\n",
    "//var geoTiff_dir = \"LeafFinal\"\n",
    "var wssse_path :String = offline_dir_path + geoTiff_dir + \"/75_wssse\"\n",
    "var wssse_csv_path :String = offline_dir_path + geoTiff_dir + \"/75_wssse.csv\"\n",
    "\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_csv_path))) {\n",
    "    println(\"The file \" + wssse_csv_path + \" already exists we will delete it!!!\")\n",
    "    try { fs.delete(new org.apache.hadoop.fs.Path(wssse_csv_path), true) } catch { case _ : Throwable => { } }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List((10,75,3.543594088100406E11), (20,75,3.1164671707882837E11), (30,75,2.891401201066724E11), (40,75,2.7659957276233606E11), (50,75,2.6393510342638516E11), (60,75,2.5526775470250888E11), (70,75,2.474527242908672E11), (80,75,2.4158928746856482E11), (90,75,2.3546354225689584E11), (100,75,2.3163178150354764E11), (110,75,2.2620214478763037E11), (120,75,2.231121099726902E11), (130,75,2.1968520531267715E11), (140,75,2.1637970796204584E11), (150,75,2.1311835641170624E11), (160,75,2.100999719988994E11), (170,75,2.0805760370421555E11), (180,75,2.0606506045153506E11), (190,75,2.034304514480542E11), (200,75,2.0235881533249454E11), (210,75,1.9951537144941003E11), (220,75,1.9824697995193225E11), (230,75,1.9573530305860495E11), (240,75,1.94558876227357E11), (250,75,1.925478193220847E11), (260,75,1.908033219040186E11), (270,75,1.8954723146873877E11), (280,75,1.8836500863517624E11), (290,75,1.8699108636352213E11), (300,75,1.858526017795722E11), (310,75,1.8423297708310217E11), (320,75,1.8317663056064175E11), (330,75,1.823668408364441E11), (340,75,1.8115073571288522E11), (350,75,1.8020940315103592E11), (360,75,1.7874518307706036E11), (370,75,1.7821354947981213E11), (380,75,1.770531429434101E11), (390,75,1.7625400513016193E11), (400,75,1.7508245702737253E11), (410,75,1.74471484549489E11), (420,75,1.7334350000927664E11), (430,75,1.7247372967251416E11), (440,75,1.718031362254259E11), (450,75,1.7092756856140845E11), (460,75,1.703876445560928E11), (470,75,1.6950616282129953E11), (480,75,1.6886305610852612E11), (490,75,1.684841457440525E11), (500,75,1.6772424497378024E11))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wssse_data = MapPartitionsRDD[2] at objectFile at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at objectFile at <console>:49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wssse_data :RDD[(Int, Int, Double)] = sc.emptyRDD\n",
    "\n",
    "//from disk\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_path))) {\n",
    "    wssse_data = sc.objectFile(wssse_path)\n",
    "    println(wssse_data.collect().toList)        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wssse = MapPartitionsRDD[10] at map at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wssse = wssse_data.repartition(1).sortBy(_._1).map{case (a,b,c) => Array(a,b,c).mkString(\",\")}\n",
    "wssse.saveAsTextFile(wssse_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
