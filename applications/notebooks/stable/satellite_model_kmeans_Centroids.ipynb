{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans over a set of GeoTiffs\n",
    "\n",
    "This notebook loads a set of GeoTiffs into a **RDD** of Tiles, with each Tile being a band in the GeoTiff. Each GeoTiff file contains **SpringIndex-** or **LastFreeze-** value for one year over the entire USA.\n",
    "\n",
    "Kmeans takes years as dimensions. Hence, the matrix has cells as rows and the years as columns. To cluster on all years, the matrix needs to be transposed. The notebook has two flavors of matrix transpose, locally by the Spark-driver or distributed using the Spark-workers. Once transposed the matrix is converted to a **RDD** of dense vectors to be used by **Kmeans** algorithm from **Spark-MLlib**. The end result is a grid where each cell has a cluster ID which is then saved into a SingleBand GeoTiff. By saving the result into a GeoTiff, the reader can plot it using a Python notebook as the one defined in the [python examples](../examples/python).\n",
    "\n",
    "<span style=\"color:red\">In this notebook the reader only needs to modify the variables in **Mode of Operation Setup**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n",
    "\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile, MultibandTile, Tile, UByteCellType}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.hadoop.io.SequenceFile.Writer\n",
    "import org.apache.hadoop.io.{SequenceFile, _}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "import scala.sys.process._\n",
    "\n",
    "//Spire is a numeric library for Scala which is intended to be generic, fast, and precise.\n",
    "import spire.syntax.cfor._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode of operation\n",
    "\n",
    "Here the user can define the mode of operation.\n",
    "* **rdd_offline_mode**: If false it means the notebook will create all data from scratch and store grid0, grid0_index, protected_extent and num_cols_rows (from grid0) into HDFS. Otherwise, these data structures are read from HDFS.\n",
    "* **matrix_offline_mode**: If false it means the notebook will create a mtrix,  transposed it and save it to HDFS. Otherwise, these data structures are read from HDFS.\n",
    "* **kmeans_offline_mode**: If false it means the notebook will train kmeans and run kemans and store kmeans model into HDFS. Otherwise, these data structures are read from HDFS.\n",
    "\n",
    "It is also possible to define which directory of GeoTiffs is to be used and on which **band** to run Kmeans. The options are\n",
    "* **BloomFinal** or **LeafFinal** which are multi-band (**4 bands**)\n",
    "* **DamageIndex** and **LastFreeze** which are single-band and if set band_num higher, it will reset to 0\n",
    "\n",
    "For kmeans the user can define the **number of iterations** and **number of clusters** as an inclusive range. Such range is defined using **minClusters**, **maxClusters**, and **stepClusters**. These variables will set a loop starting at **minClusters** and stopping at **maxClusters** (inclusive), iterating **stepClusters** at the time. <span style=\"color:red\">Note that when using a range **kemans offline mode** is not possible and it will be reset to **online mode**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation setup\n",
    "<a id='mode_of_operation_setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inpB_rdd_offline_mode = true\n",
       "inpA_rdd_offline_mode = true\n",
       "matrix_offline_mode = true\n",
       "kmeans_offline_mode = true\n",
       "inpB_path = hdfs:///user/hadoop/spring-index/\n",
       "inpB_dir = BloomFinal\n",
       "inpA_path = hdfs:///user/hadoop/spring-index/\n",
       "inpA_dir = LeafFinal\n",
       "out_path = hdfs:///user/pheno/kmeans_BloomFinal_LeafFinalCentroidS/\n",
       "band_num = 3\n",
       "timeseries = (1980,2015)\n",
       "first_year = 1980\n",
       "last_year = 2015\n",
       "toBeMasked = true\n",
       "mask_path = hdfs:///user/hadoop/usa_mask.tif\n",
       "numIterations = 75\n",
       "minClusters = 70\n",
       "maxClusters = 70\n",
       "stepClusters = 10\n",
       "save_kmeans_inpB = false\n",
       "save_rdds = false\n",
       "save_matrix = false\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var inpB_rdd_offline_mode = true\n",
    "var inpA_rdd_offline_mode = true\n",
    "var matrix_offline_mode = true\n",
    "var kmeans_offline_mode = true\n",
    "\n",
    "//Using spring-index inpB\n",
    "var inpB_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var inpB_dir = \"BloomFinal\"\n",
    "\n",
    "//Using AVHRR Satellite data\n",
    "var inpA_path = \"hdfs:///user/hadoop/spring-index/\"\n",
    "var inpA_dir = \"LeafFinal\"\n",
    "\n",
    "var out_path = \"hdfs:///user/pheno/kmeans_\" + inpB_dir + \"_\" + inpA_dir + \"CentroidS/\"\n",
    "var band_num = 3\n",
    "\n",
    "//Satellite years between (inclusive) 1989 - 2014\n",
    "//Model years between (inclusive) 1980 - 2015\n",
    "\n",
    "val timeseries = (1980, 2015)\n",
    "var first_year = 1980\n",
    "var last_year = 2015\n",
    "\n",
    "//Mask\n",
    "val toBeMasked = true\n",
    "val mask_path = \"hdfs:///user/hadoop/usa_mask.tif\"\n",
    "\n",
    "//Kmeans number of iterations and clusters\n",
    "var numIterations = 75\n",
    "var minClusters = 70\n",
    "var maxClusters = 70\n",
    "var stepClusters = 10\n",
    "var save_kmeans_inpB = false\n",
    "val save_rdds = false\n",
    "val save_matrix = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">DON'T MODIFY ANY PIECE OF CODE FROM HERE ON!!!</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of operation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Load GeoTiffs\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "\"Matrix\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n",
      "1\n",
      "\"Kmeans\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-18954578_46, ugi=pheno (auth:SIMPLE)]]\n",
       "mask_str = _mask\n",
       "inpB_grid_path = hdfs:///user/pheno/kmeans_BloomFinal_LeafFinalCentroidS/BloomFinal_grid\n",
       "inpA_grid_path = hdfs:///user/pheno/kmeans_BloomFinal_LeafFinalCentroidS/LeafFinal_grid\n",
       "grid0_path = hdfs:///user/pheno/kmeans_BloomFinal_LeafFinalCentroidS/BloomFinal_grid0\n",
       "grid0_index_path = hdfs:///user/pheno/kmeans_BloomFinal_LeafFinalCentroidS/BloomFinal_grid0...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/pheno/kmeans_BloomFinal_LeafFinalCentroidS/BloomFinal_grid0_index"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "//Paths to store data structures for Offline runs\n",
    "var mask_str = \"\"\n",
    "if (toBeMasked)\n",
    "  mask_str = \"_mask\"\n",
    "\n",
    "var inpB_grid_path = out_path + inpB_dir + \"_grid\"\n",
    "var inpA_grid_path = out_path + inpA_dir + \"_grid\"\n",
    "var grid0_path = out_path + inpB_dir + \"_grid0\"\n",
    "var grid0_index_path = out_path + inpB_dir + \"_grid0_index\"\n",
    "var matrix_path = out_path + inpB_dir + \"_matrix\" + \"_\" + first_year + \"_\" + last_year\n",
    "var metadata_path = out_path + inpB_dir + \"_metadata\"\n",
    "\n",
    "val inpB_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(inpB_grid_path))\n",
    "val inpA_rdd_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(inpA_grid_path))\n",
    "val matrix_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(matrix_path))\n",
    "\n",
    "if (minClusters > maxClusters) {\n",
    "  maxClusters = minClusters\n",
    "  stepClusters = 1\n",
    "}\n",
    "if (stepClusters < 1) {\n",
    "  stepClusters = 1\n",
    "}\n",
    "\n",
    "if (inpB_rdd_offline_mode != inpB_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + inpB_rdd_offline_exists.toString())\n",
    "  inpB_rdd_offline_mode = inpB_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (inpA_rdd_offline_mode != inpA_rdd_offline_exists) {\n",
    "  println(\"\\\"Load GeoTiffs\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + inpA_rdd_offline_exists.toString())\n",
    "  inpA_rdd_offline_mode = inpA_rdd_offline_exists\n",
    "}\n",
    "\n",
    "if (matrix_offline_mode != matrix_offline_exists) {\n",
    "  println(\"\\\"Matrix\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + matrix_offline_exists.toString())\n",
    "  matrix_offline_mode = matrix_offline_exists\n",
    "}\n",
    "\n",
    "//Years\n",
    "//Years\n",
    "val years = timeseries._1 to timeseries._2\n",
    "\n",
    "if (!years.contains(first_year) || !(years.contains(last_year))) {\n",
    "  println(\"Invalid range of years for \" + inpB_dir + \". I should be between \" + first_year + \" and \" + last_year)\n",
    "  System.exit(0)\n",
    "}\n",
    "\n",
    "var years_range = (years.indexOf(first_year), years.indexOf(last_year))\n",
    "\n",
    "var num_kmeans: Int = 1\n",
    "if (minClusters != maxClusters) {\n",
    "  num_kmeans = ((maxClusters - minClusters) / stepClusters) + 1\n",
    "}\n",
    "println(num_kmeans)\n",
    "var kmeans_inpB_paths: Array[String] = Array.fill[String](num_kmeans)(\"\")\n",
    "var wssse_path: String = out_path + \"/\" + numIterations + \"_wssse\" + \"_\" + first_year + \"_\" + last_year\n",
    "var geotiff_hdfs_paths: Array[String] = Array.fill[String](num_kmeans)(\"\")\n",
    "var geotiff_tmp_paths: Array[String] = Array.fill[String](num_kmeans)(\"\")\n",
    "var numClusters_id = 0\n",
    "\n",
    "if (num_kmeans > 1) {\n",
    "  numClusters_id = 0\n",
    "  cfor(minClusters)(_ <= maxClusters, _ + stepClusters) { numClusters =>\n",
    "    kmeans_inpB_paths(numClusters_id) = out_path + \"/kmeans_inpB_\" + band_num + \"_\" + numClusters + \"_\" + numIterations + \"_\" + first_year + \"_\" + last_year\n",
    "\n",
    "    //Check if the file exists\n",
    "    val kmeans_exist = fs.exists(new org.apache.hadoop.fs.Path(kmeans_inpB_paths(numClusters_id)))\n",
    "    if (kmeans_exist && !kmeans_offline_mode) {\n",
    "      println(\"The kmeans inpB path \" + kmeans_inpB_paths(numClusters_id) + \" exists, please remove it.\")\n",
    "    } else if (!kmeans_exist && kmeans_offline_mode) {\n",
    "      kmeans_offline_mode = false\n",
    "    }\n",
    "\n",
    "    geotiff_hdfs_paths(numClusters_id) = out_path + \"/clusters_\" + band_num + \"_\" + numClusters + \"_\" + numIterations + \"_\" + first_year + \"_\" + last_year\n",
    "    geotiff_tmp_paths(numClusters_id) = \"/tmp/clusters_\" + band_num + \"_\" + numClusters + \"_\" + numIterations\n",
    "    numClusters_id += 1\n",
    "  }\n",
    "  kmeans_offline_mode = false\n",
    "} else {\n",
    "  kmeans_inpB_paths(0) = out_path + \"/kmeans_inpB_\" + band_num + \"_\" + minClusters + \"_\" + numIterations + \"_\" + first_year + \"_\" + last_year\n",
    "  val kmeans_offline_exists = fs.exists(new org.apache.hadoop.fs.Path(kmeans_inpB_paths(0)))\n",
    "  if (kmeans_offline_mode != kmeans_offline_exists) {\n",
    "    println(\"\\\"Kmeans\\\" offline mode is not set properly, i.e., either it was set to false and the required file does not exist or vice-versa. We will reset it to \" + kmeans_offline_exists.toString())\n",
    "    kmeans_offline_mode = kmeans_offline_exists\n",
    "  }\n",
    "  geotiff_hdfs_paths(0) = out_path + \"/clusters_\" + band_num + \"_\" + minClusters + \"_\" + numIterations + \"_\" + first_year + \"_\" + last_year\n",
    "  geotiff_tmp_paths(0) = \"/tmp/clusters_\" + band_num + \"_\" + minClusters + \"_\" + numIterations\n",
    "}\n",
    "\n",
    "//Global variables\n",
    "var inpA_grids_RDD: RDD[(Long,Array[Double])] = sc.emptyRDD\n",
    "var inpA_grids: RDD[(Long,Array[Double])] = sc.emptyRDD\n",
    "var inpB_grids_RDD: RDD[(Long, Array[Double])] = sc.emptyRDD\n",
    "var inpB_grids: RDD[(Long, Array[Double])] = sc.emptyRDD\n",
    "var projected_extent = new ProjectedExtent(new Extent(0, 0, 0, 0), CRS.fromName(\"EPSG:3857\"))\n",
    "var grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var num_cols_rows: (Int, Int) = (0, 0)\n",
    "var cellT: CellType = UByteCellType\n",
    "var mask_tile0: Tile = new SinglebandGeoTiff(geotrellis.raster.ArrayTile.empty(cellT, num_cols_rows._1, num_cols_rows._2), projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT).tile\n",
    "//We are comparing 2 data sets only\n",
    "var cells_size: Long = 2\n",
    "var t0: Long = 0\n",
    "var t1: Long = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to (de)serialize any structure into Array[Byte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serialize: (value: Any)Array[Byte]\n",
       "deserialize: (bytes: Array[Byte])Any\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def serialize(value: Any): Array[Byte] = {\n",
    "    val out_stream: ByteArrayOutputStream = new ByteArrayOutputStream()\n",
    "    val obj_out_stream = new ObjectOutputStream(out_stream)\n",
    "    obj_out_stream.writeObject(value)\n",
    "    obj_out_stream.close\n",
    "    out_stream.toByteArray\n",
    "}\n",
    "\n",
    "def deserialize(bytes: Array[Byte]): Any = {\n",
    "    val obj_in_stream = new ObjectInputStream(new ByteArrayInputStream(bytes))\n",
    "    val value = obj_in_stream.readObject\n",
    "    obj_in_stream.close\n",
    "    value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GeoTiffs\n",
    "\n",
    "Using GeoTrellis all GeoTiffs of a directory will be loaded into a RDD. Using the RDD, we extract a grid from the first file to lated store the Kmeans cluster_IDS, we build an Index for populate such grid and we filter out here all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoopGeoTiffRDD: (inpA_filepath: String, pattern: String)org.apache.spark.rdd.RDD[(Int, (geotrellis.vector.ProjectedExtent, geotrellis.raster.Tile))]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hadoopGeoTiffRDD(inpA_filepath :String, pattern :String): RDD[(Int, (ProjectedExtent, Tile))] = {\n",
    "  val listFiles = sc.binaryFiles(inpA_filepath + \"/\" + pattern).sortBy(_._1).keys.collect()\n",
    "  var prevRDD :RDD[(Int,(ProjectedExtent, Tile))] = sc.emptyRDD\n",
    "\n",
    "  cfor(0)(_ < listFiles.length, _ + 1) { k =>\n",
    "    val filePath :String = listFiles(k)\n",
    "    val kB = sc.broadcast(k)\n",
    "    val currRDD = sc.hadoopGeoTiffRDD(filePath).map(m => (kB.value, m))\n",
    "    prevRDD = currRDD.union(prevRDD)\n",
    "    //kB.destroy()\n",
    "  }\n",
    "  prevRDD.sortBy(_._1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoopMultibandGeoTiffRDD: (inpA_filepath: String, pattern: String)org.apache.spark.rdd.RDD[(Int, (geotrellis.vector.ProjectedExtent, geotrellis.raster.MultibandTile))]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hadoopMultibandGeoTiffRDD(inpA_filepath :String, pattern :String): RDD[(Int, (ProjectedExtent, MultibandTile))] = {\n",
    "  val listFiles = sc.binaryFiles(inpA_filepath + \"/\" + pattern).sortBy(_._1).keys.collect()\n",
    "  var prevRDD :RDD[(Int,(ProjectedExtent, MultibandTile))] = sc.emptyRDD\n",
    "\n",
    "  cfor(0)(_ < listFiles.length, _ + 1) { k =>\n",
    "    val filePath :String = listFiles(k)\n",
    "    val kB = sc.broadcast(k)\n",
    "    val currRDD = sc.hadoopMultibandGeoTiffRDD(filePath).map(m => (kB.value,m))\n",
    "    prevRDD = currRDD.union(prevRDD)\n",
    "    //kB.destroy()\n",
    "  }\n",
    "  prevRDD.sortBy(_._1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoTiffs A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5964118702ns\n",
      "[Stage 16:====================================================>   (34 + 2) / 36]Elapsed time: 51882298002ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 129210412241220\n",
       "pattern = *.tif\n",
       "inpA_filepath = hdfs:///user/hadoop/spring-index/LeafFinal\n",
       "inpB_filepath = hdfs:///user/hadoop/spring-index//BloomFinal\n",
       "t1 = 129262294539222\n",
       "t0 = 129210412241220\n",
       "inpA_grids_withIndex = MapPartitionsRDD[204] at map at <console>:109\n",
       "inpA_year_diff = 0\n",
       "inputA_year_diffB = Broadcast(89)\n",
       "inpA_grids = MapPartitionsRDD[206] at map at <console>:124\n",
       "inpA_grid0_index = MapPartitionsRDD[209] at flatMap at <console>:126\n",
       "t1 = 129262294539222\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "129262294539222"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "//Load Mask\n",
    "if (toBeMasked) {\n",
    "  val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mask_path).values\n",
    "  val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  mask_tile0 = (mask_tiles_withIndex.filter(m => m._1 == 0).filter(m => !m._1.isNaN).values.collect()) (0)\n",
    "}\n",
    "\n",
    "//Local variables\n",
    "val pattern: String = \"*.tif\"\n",
    "val inpA_filepath: String = inpA_path + inpA_dir\n",
    "val inpB_filepath: String = inpB_path + \"/\" + inpB_dir\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")\n",
    "\n",
    "t0 = System.nanoTime()\n",
    "if (inpA_rdd_offline_mode) {\n",
    "  inpA_grids_RDD = sc.objectFile(inpA_grid_path)\n",
    "} else {\n",
    "  val inpA_geos_RDD = hadoopMultibandGeoTiffRDD(inpA_filepath, pattern)\n",
    "  val inpA_tiles_RDD = inpA_geos_RDD.map{ case (i,(p,t)) => (i,t)}\n",
    "\n",
    "  val band_numB :Broadcast[Int] = sc.broadcast(band_num)\n",
    "  if (toBeMasked) {\n",
    "    val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "    inpA_grids_RDD = inpA_tiles_RDD.map{case (i,m) => (i,m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble().filter(_ != -1000).filter(!_.isNaN))}\n",
    "  } else {\n",
    "    inpA_grids_RDD = inpA_tiles_RDD.map{case (i,m) => (i,m.band(band_numB.value).toArrayDouble().filter(!_.isNaN))}\n",
    "  }\n",
    "\n",
    "  //Store in HDFS\n",
    "  if (save_rdds) {\n",
    "    inpA_grids_RDD.saveAsObjectFile(inpA_grid_path)\n",
    "  }\n",
    "}\n",
    "val inpA_grids_withIndex = inpA_grids_RDD//.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "val inpA_year_diff = first_year-timeseries._1\n",
    "val inputA_year_diffB = sc.broadcast(inpA_year_diff)\n",
    "inpA_grids = inpA_grids_withIndex.filterByRange(years_range._1, years_range._2).map{ case(i,a) => (i-(inputA_year_diffB.value),a)}//.values\n",
    "\n",
    "var inpA_grid0_index: RDD[Double] = inpA_grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m)\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoTiffs B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================> (34 + 1) / 35]Elapsed time: 450273383195ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 129266658208706\n",
       "inpB_grids_withIndex = MapPartitionsRDD[388] at map at <console>:164\n",
       "inpB_grids = MapPartitionsRDD[390] at values at <console>:167\n",
       "t1 = 129716931591901\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "129716931591901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "if (inpB_rdd_offline_mode) {\n",
    "  inpB_grids_RDD = sc.objectFile(inpB_grid_path)\n",
    "  grid0 = sc.objectFile(grid0_path)\n",
    "  grid0_index = sc.objectFile(grid0_index_path)\n",
    "\n",
    "  val metadata = sc.sequenceFile(metadata_path, classOf[IntWritable], classOf[BytesWritable]).map(_._2.copyBytes()).collect()\n",
    "  projected_extent = deserialize(metadata(0)).asInstanceOf[ProjectedExtent]\n",
    "  num_cols_rows = (deserialize(metadata(1)).asInstanceOf[Int], deserialize(metadata(2)).asInstanceOf[Int])\n",
    "  cellT = deserialize(metadata(3)).asInstanceOf[CellType]\n",
    "} else {\n",
    "  val inpB_geos_RDD = hadoopMultibandGeoTiffRDD(inpB_filepath, pattern)\n",
    "  val inpB_tiles_RDD = inpB_geos_RDD.map{case (i,(p,t)) => (i,t)}\n",
    "\n",
    "  //Retrieve the number of cols and rows of the Tile's grid\n",
    "  val tiles_withIndex = inpB_tiles_RDD//.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "  val tile0 = (tiles_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "  num_cols_rows = (tile0.cols, tile0.rows)\n",
    "  cellT = tile0.cellType\n",
    "\n",
    "  //Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "  val projected_extents_withIndex = inpB_geos_RDD.map{case (i,(p,t)) => (i,p)}\n",
    "  projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect()) (0)\n",
    "\n",
    "  val band_numB: Broadcast[Int] = sc.broadcast(band_num)\n",
    "  if (toBeMasked) {\n",
    "    val mask_tile_broad: Broadcast[Tile] = sc.broadcast(mask_tile0)\n",
    "    inpB_grids_RDD = inpB_tiles_RDD.map{ case (i, m) => (i,m.band(band_numB.value).localInverseMask(mask_tile_broad.value, 1, -1000).toArrayDouble())}\n",
    "  } else {\n",
    "    inpB_grids_RDD = inpB_tiles_RDD.map{ case (i, m) => (i, m.band(band_numB.value).toArrayDouble())}\n",
    "  }\n",
    "\n",
    "  //Get Index for each Cell\n",
    "  val grids_withIndex = inpB_grids_RDD//.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "  if (toBeMasked) {\n",
    "    grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => m._1 != -1000.0).filter(m => !m._1.isNaN).map { case (v, i) => (i) }\n",
    "  } else {\n",
    "    grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).filter(m => !m.isNaN).zipWithIndex.map { case (v, i) => (i) }\n",
    "  }\n",
    "\n",
    "  //Get the Tile's grid\n",
    "  grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.map { case (v, i) => (i, v) }\n",
    "\n",
    "  //Lets filter out NaN\n",
    "  if (toBeMasked) {\n",
    "    inpB_grids_RDD = inpB_grids_RDD.map{ case (i,m) => (i,m.filter(m => m != -1000.0).filter(m => !m.isNaN))}\n",
    "  } else {\n",
    "    inpB_grids_RDD = inpB_grids_RDD.map{ case (i,m) => (i, m.filter(!_.isNaN))}\n",
    "  }\n",
    "\n",
    "  //Store data in HDFS\n",
    "  if (save_rdds) {\n",
    "    grid0.saveAsObjectFile(grid0_path)\n",
    "    grid0_index.saveAsObjectFile(grid0_index_path)\n",
    "    inpB_grids_RDD.saveAsObjectFile(inpB_grid_path)\n",
    "\n",
    "    val writer: SequenceFile.Writer = SequenceFile.createWriter(conf,\n",
    "      Writer.file(metadata_path),\n",
    "      Writer.keyClass(classOf[IntWritable]),\n",
    "      Writer.valueClass(classOf[BytesWritable])\n",
    "    )\n",
    "\n",
    "    writer.append(new IntWritable(1), new BytesWritable(serialize(projected_extent)))\n",
    "    writer.append(new IntWritable(2), new BytesWritable(serialize(num_cols_rows._1)))\n",
    "    writer.append(new IntWritable(3), new BytesWritable(serialize(num_cols_rows._2)))\n",
    "    writer.append(new IntWritable(4), new BytesWritable(serialize(cellT)))\n",
    "    writer.hflush()\n",
    "    writer.close()\n",
    "  }\n",
    "}\n",
    "val inpB_grids_withIndex = inpB_grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "\n",
    "//Filter out the range of years:\n",
    "inpB_grids = inpB_grids_withIndex.filterByRange(years_range._1, years_range._2).values\n",
    "\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "\n",
    "We need to do a Matrix transpose to have clusters per cell and not per year. With a GeoTiff representing a single year, the loaded data looks liks this:\n",
    "```\n",
    "bands_RDD.map(s => Vectors.dense(s)).cache()\n",
    "\n",
    "//The vectors are rows and therefore the matrix will look like this:\n",
    "[\n",
    "Vectors.dense(0.0, 1.0, 2.0),\n",
    "Vectors.dense(3.0, 4.0, 5.0),\n",
    "Vectors.dense(6.0, 7.0, 8.0),\n",
    "Vectors.dense(9.0, 0.0, 1.0)\n",
    "]\n",
    "```\n",
    "\n",
    "To achieve that we convert the **RDD[Vector]** into a distributed Matrix, a [**CoordinateMatrix**](https://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix), which as a **transpose** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 52:====================================================>(998 + 2) / 1000]Elapsed time: 121827359929ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 130151123233939\n",
       "grids_matrix = MapPartitionsRDD[430] at map at <console>:80\n",
       "t1 = 130272950593868\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "130272950593868"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "var grids_matrix: RDD[Vector] = sc.emptyRDD\n",
    "\n",
    "if (matrix_offline_mode) {\n",
    "  grids_matrix = sc.objectFile(matrix_path)\n",
    "} else {\n",
    "  val inp_grids :RDD[Array[Double]] = inpA_grids.flatMap{ case (i,m) => m}.zipWithIndex().map{ case (v,i) => (i,v)}.join(inpB_grids.flatMap{case (i,m) => m}.zipWithIndex().map{case (v,i) => (i,v)}).sortByKey(true).map{case (i, (a1,a2)) => Array(a1, a2)}\n",
    "  val cells_sizeB = sc.broadcast(cells_size)\n",
    "  grids_matrix = inp_grids.map(m => m.zipWithIndex).map(m => m.filter(!_._1.isNaN)).map(m => Vectors.sparse(cells_sizeB.value.toInt, m.map(v => v._2), m.map(v => v._1)))\n",
    "  if (save_matrix)\n",
    "    grids_matrix.saveAsObjectFile(matrix_path)\n",
    "}\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kmeans\n",
    "\n",
    "We use Kmeans from Sparl-MLlib. The user should only modify the variables on Kmeans setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "Within Set Sum of Squared Errors = 1.1873425150746796E10                        \n",
      "Lets create it with the new data\n",
      "Elapsed time: 1195522686875ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 130278572398597\n",
       "kmeans_inpBs = Array(org.apache.spark.mllib.clustering.KMeansModel@2d933be)\n",
       "wssse_data = List((70,75,1.1873425150746796E10))\n",
       "t1 = 131474095085472\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "131474095085472"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "//Global variables\n",
    "var kmeans_inpBs :Array[KMeansModel] = new Array[KMeansModel](num_kmeans)\n",
    "var wssse_data :List[(Int, Int, Double)] = List.empty\n",
    "\n",
    "if (kmeans_offline_mode) {\n",
    "  numClusters_id = 0\n",
    "  cfor(minClusters)(_ <= maxClusters, _ + stepClusters) { numClusters =>\n",
    "    if (!fs.exists(new org.apache.hadoop.fs.Path(kmeans_inpB_paths(numClusters_id)))) {\n",
    "      println(\"One of the files does not exist, we will abort!!!\")\n",
    "      System.exit(0)\n",
    "    } else {\n",
    "      kmeans_inpBs(numClusters_id) = KMeansModel.load(sc, kmeans_inpB_paths(numClusters_id))\n",
    "    }\n",
    "    numClusters_id += 1\n",
    "  }\n",
    "  val wssse_data_RDD :RDD[(Int, Int, Double)]  = sc.objectFile(wssse_path)\n",
    "  wssse_data  = wssse_data_RDD.collect().toList\n",
    "} else {\n",
    "  numClusters_id = 0\n",
    "  if (fs.exists(new org.apache.hadoop.fs.Path(wssse_path))) {\n",
    "    val wssse_data_RDD :RDD[(Int, Int, Double)]  = sc.objectFile(wssse_path)\n",
    "    wssse_data  = wssse_data_RDD.collect().toList\n",
    "  }\n",
    "  grids_matrix.cache()\n",
    "  cfor(minClusters)(_ <= maxClusters, _ + stepClusters) { numClusters =>\n",
    "    println(numClusters)\n",
    "    kmeans_inpBs(numClusters_id) = {\n",
    "      KMeans.train(grids_matrix, numClusters, numIterations)\n",
    "    }\n",
    "\n",
    "    // Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "    val WSSSE = kmeans_inpBs(numClusters_id).computeCost(grids_matrix)\n",
    "    println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n",
    "\n",
    "    wssse_data = wssse_data :+ (numClusters, numIterations, WSSSE)\n",
    "\n",
    "    //Save kmeans inpB\n",
    "    if (save_kmeans_inpB) {\n",
    "      if (!fs.exists(new org.apache.hadoop.fs.Path(kmeans_inpB_paths(numClusters_id)))) {\n",
    "        kmeans_inpBs(numClusters_id).save(sc, kmeans_inpB_paths(numClusters_id))\n",
    "      }\n",
    "    }\n",
    "    numClusters_id += 1\n",
    "\n",
    "    if (fs.exists(new org.apache.hadoop.fs.Path(wssse_path))) {\n",
    "      println(\"We will delete the wssse file\")\n",
    "      try { fs.delete(new org.apache.hadoop.fs.Path(wssse_path), true) } catch { case _ : Throwable => { } }\n",
    "    }\n",
    "\n",
    "    println(\"Lets create it with the new data\")\n",
    "    sc.parallelize(wssse_data, 1).saveAsObjectFile(wssse_path)\n",
    "  }\n",
    "\n",
    "  //Un-persist it to save memory\n",
    "  grids_matrix.unpersist()\n",
    "\n",
    "}\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WSSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List((70,75,1.1873425150746796E10))\n",
      "List((70,75,1.1873425150746796E10))\n",
      "Elapsed time: 1370049310ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 131476627149620\n",
       "t1 = 131477997198930\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "131477997198930"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "//current\n",
    "println(wssse_data)\n",
    "\n",
    "//from disk\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_path))) {\n",
    "    var wssse_data_tmp :RDD[(Int, Int, Double)] = sc.objectFile(wssse_path)//.collect()//.toList\n",
    "    println(wssse_data_tmp.collect().toList)    \n",
    "}\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Kmeans clustering\n",
    "\n",
    "Run Kmeans and obtain the clusters per each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 65874715ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 131478823750440\n",
       "kmeans_res = Array(MapPartitionsRDD[605] at map at KMeansModel.scala:69)\n",
       "kmeans_centroids = Array(Array(56.17556246887822, 103.44256505723837, 137.6315427813874, 83.84484421655864, 53.7678071884187, 122.52128518121197, 21.605910912313128, 94.89144616206633, 75.07605474299004, 36.30307410658765, 112.66860609929438, 86.46482482714609, 150.7715577061751, 95.7996039253704, 135.94152974851127, 102.84186374095559, 33.438895413826664, 147.87563188719034, 92.00507730228097, 61.17536176291704, 58.575806784566716, 75.43451573025675, 49.59918831594512, 42.69622702766812, 84.0807256352624, 99.25195843953021, 111.16299738442571, 121.27302223360294, 48.80438276170649, 134.63933201878305, 31.213707346154376, 62.8454...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[56.17556246887822, 103.44256505723837, 137.6315427813874, 83.84484421655864, 53.7678071884187, 122.52128518121197, 21.605910912313128, 94.89144616206633, 75.07605474299004, 36.30307410658765, 112.66860609929438, 86.46482482714609, 150.7715577061751, 95.7996039253704, 135.94152974851127, 102.84186374095559, 33.438895413826664, 147.87563188719034, 92.00507730228097, 61.17536176291704, 58.575806784566716, 75.43451573025675, 49.59918831594512, 42.69622702766812, 84.0807256352624, 99.25195843953021, 111.16299738442571, 121.27302223360294, 48.80438276170649, 134.63933201878305, 31.213707346154376, 62.84542896936752, 57.89916034232198, 119.44766588252524, 113.56777853324533, 43.51983251208396, 119.55308841518094, 104.76821883450359, 129.1723659571981, 75.22498521517062, 14.791392702597978, 30.72930960720677, 40.35073318517985, 118.72679034230609, 47.2698666321098, 64.59608973726682, 105.90683484586613, 153.99211781895443, 40.372108660678755, 25.024567085094684, 85.21879425283912, 31.75981884253955, 78.16182037560675, 64.52279806939772, 96.77290763553445, 42.267461104345145, 31.50611918172694, 28.198806140483317, 66.89600998453295, 112.7250040275084, 74.28884024094123, 27.07553302698894, 19.04191844741385, 98.15481355578416, 91.95694206140531, 100.22214213989537, 35.790137843827836, 86.97283139198876, 77.46579283098262, 73.32458685554766]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "//Cache it so kmeans is more efficient\n",
    "grids_matrix.cache()\n",
    "\n",
    "var kmeans_res: Array[RDD[Int]] = Array.fill(num_kmeans)(sc.emptyRDD)\n",
    "var kmeans_centroids: Array[Array[Double]] = Array.fill(num_kmeans)(Array.emptyDoubleArray)\n",
    "numClusters_id = 0\n",
    "cfor(minClusters)(_ <= maxClusters, _ + stepClusters) { numClusters =>\n",
    "  kmeans_res(numClusters_id) = kmeans_inpBs(numClusters_id).predict(grids_matrix)\n",
    "  kmeans_centroids(numClusters_id) = kmeans_inpBs(numClusters_id).clusterCenters.map(m => m(0))\n",
    "  numClusters_id += 1\n",
    "}\n",
    "\n",
    "//Un-persist it to save memory\n",
    "grids_matrix.unpersist()\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity test\n",
    "\n",
    "It can be skipped, it only shows the cluster ID for the first 50 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010150\n",
      "Elapsed time: 561480418ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 131479807231924\n",
       "kmeans_res_out = Array(10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n",
       "t1 = 131480368712342\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "131480368712342"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = System.nanoTime()\n",
    "val kmeans_res_out = kmeans_res(0).take(150)\n",
    "kmeans_res_out.foreach(print)\n",
    "\n",
    "println(kmeans_res_out.size)\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GeoTiff with Kmeans cluster_IDs\n",
    "\n",
    "The Grid with the cluster IDs is stored in a SingleBand GeoTiff and uploaded to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign cluster ID to each grid cell and save the grid as SingleBand GeoTiff\n",
    "\n",
    "To assign the clusterID to each grid cell it is necessary to get the indices of gird cells they belong to. The process is not straight forward because the ArrayDouble used for the creation of each dense Vector does not contain the NaN values, therefore there is not a direct between the indices in the Tile's grid and the ones in **kmeans_res** (kmeans result).\n",
    "\n",
    "To join the two RDDS the knowledge was obtaing from a stackoverflow post on [how to perform basic joins of two rdd tables in spark using python](https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 642:===================================================>(999 + 1) / 1000]10115578\n",
      "364160808\n",
      "[Stage 648:====================================================>(997 + 2) / 999]Saving GeoTiff for numClustersID: 0 year: 1980\n",
      "[Stage 669:===================================================>(998 + 2) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1981\n",
      "[Stage 690:==================================================>(981 + 19) / 1000]] / 1000]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1982\n",
      "[Stage 711:===================================================>(999 + 1) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "17/12/29 15:52:00 INFO hdfs.DataStreamer: Exception in createBlockOutputStream\n",
      "java.io.IOException: Got error, status=ERROR, status message , ack with firstBadLink as 145.100.58.29:50010\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1643)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1547)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:658)\n",
      "17/12/29 15:52:00 WARN hdfs.DataStreamer: Abandoning BP-1930291640-145.100.58.119-1511393132874:blk_1073817012_76188\n",
      "17/12/29 15:52:00 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[145.100.58.29:50010,DS-4b982238-d69a-4ee0-9e7d-768736c03cb0,DISK]\n",
      "Saving GeoTiff for numClustersID: 0 year: 1983\n",
      "[Stage 732:===================================================>(997 + 3) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1984\n",
      "[Stage 753:===================================================>(999 + 1) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1985\n",
      "[Stage 774:===================================================>(991 + 9) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1986\n",
      "[Stage 795:===================================================>(996 + 4) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1987\n",
      "[Stage 816:===================================================>(995 + 5) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1988\n",
      "[Stage 837:=================================================> (968 + 32) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1989\n",
      "[Stage 858:===================================================>(996 + 4) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1990\n",
      "[Stage 879:===================================================>(998 + 2) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1991\n",
      "[Stage 900:===================================================>(996 + 4) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1992\n",
      "[Stage 921:===================================================>(999 + 1) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1993\n",
      "[Stage 942:===================================================>(995 + 5) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1994\n",
      "[Stage 963:===================================================>(998 + 2) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1995\n",
      "[Stage 984:===================================================>(998 + 2) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1996\n",
      "[Stage 1005:==================================================>(996 + 4) / 1000]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1997\n",
      "[Stage 1026:==================================================>(996 + 4) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1998\n",
      "[Stage 1047:=================================================>(990 + 10) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 1999\n",
      "[Stage 1068:==================================================>(999 + 1) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2000\n",
      "[Stage 1089:==================================================>(996 + 4) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2001\n",
      "[Stage 1110:==================================================>(998 + 2) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2002\n",
      "[Stage 1131:==================================================>(999 + 1) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2003\n",
      "[Stage 1152:=================================================>(990 + 10) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2004\n",
      "[Stage 1173:==================================================>(995 + 5) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2005\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.           ]]\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2006\n",
      "[Stage 1215:==================================================>(997 + 3) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2007\n",
      "[Stage 1236:==================================================>(995 + 5) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2008\n",
      "[Stage 1257:==================================================>(994 + 6) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2009\n",
      "[Stage 1278:==================================================>(995 + 5) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2010\n",
      "[Stage 1299:==================================================>(999 + 1) / 1000]]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GeoTiff for numClustersID: 0 year: 2011\n",
      "[Stage 1320:=================================================>(981 + 19) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2012\n",
      "[Stage 1341:================================================> (976 + 24) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2013\n",
      "[Stage 1362:==================================================>(997 + 3) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2014\n",
      "[Stage 1383:==================================================>(995 + 5) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Saving GeoTiff for numClustersID: 0 year: 2015\n",
      "[Stage 1404:==================================================>(999 + 1) / 1000]]]DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Elapsed time: 4236536775063ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 131481735632490\n",
       "numClusters_id = 1\n",
       "grid0_index_I = MapPartitionsRDD[607] at map at <console>:82\n",
       "num_cells = 364160808\n",
       "cells_per_year = 10115578\n",
       "cells_per_yearB = Broadcast(435)\n",
       "t1 = 135718272407553\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "135718272407553"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CREATE GeoTiffs\n",
    "t0 = System.nanoTime()\n",
    "numClusters_id = 0\n",
    "grid0_index.cache()\n",
    "grid0.cache()\n",
    "val grid0_index_I = grid0_index.zipWithIndex().map{ case (v,i) => (i,v)}\n",
    "grid0_index_I.cache()\n",
    "grid0_index.unpersist()\n",
    "kmeans_res(0).cache()\n",
    "var num_cells = kmeans_res(0).count().toInt\n",
    "kmeans_res(0).unpersist()\n",
    "var cells_per_year = num_cells / ((last_year-first_year)+1)\n",
    "println(cells_per_year)\n",
    "println(num_cells)\n",
    "val cells_per_yearB = sc.broadcast(cells_per_year)\n",
    "\n",
    "cfor(minClusters)(_ <= maxClusters, _ + stepClusters) { numClusters =>\n",
    "  //Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells\n",
    "  var year :Int = 0\n",
    "  val kmeans_res_zip = kmeans_res(numClusters_id).zipWithIndex()\n",
    "  kmeans_res_zip.cache()\n",
    "  cfor(0) (_ < num_cells, _ + cells_per_year) { cellID =>\n",
    "    println(\"Saving GeoTiff for numClustersID: \" + numClusters_id + \" year: \" + years(year))\n",
    "    val cellIDB = sc.broadcast(cellID)\n",
    "    val kmeans_res_sing = kmeans_res(numClusters_id)\n",
    "    val cluster_cell_pos = ((kmeans_res_zip.map{ case (v,i) => (i,v)}.filterByRange(cellIDB.value, (cellIDB.value+cells_per_yearB.value-1)).map{case (i,v) => (i-cellIDB.value, v)}.join(grid0_index_I)).map{ case (k,(v,i)) => (v,i)})\n",
    "\n",
    "    //Associate a Cluster_IDs to respective Grid_cell\n",
    "    val grid_clusters :RDD[ (Long, (Double, Option[Int]))] = grid0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i.toLong, c)})\n",
    "\n",
    "    //Convert all None to NaN\n",
    "    val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Int.MaxValue) else (k, c.get)}\n",
    "\n",
    "    //Define a Tile\n",
    "    val cluster_cellsID :Array[Int] = grid_clusters_res.values.collect()\n",
    "    var cluster_cells :Array[Double] = Array.fill(cluster_cellsID.length)(Double.NaN)\n",
    "    cfor(0)(_ < cluster_cellsID.length, _ + 1) { cellID =>\n",
    "      if (cluster_cellsID(cellID) != Int.MaxValue) {\n",
    "        cluster_cells(cellID) = kmeans_centroids(numClusters_id)(cluster_cellsID(cellID))\n",
    "      }\n",
    "    }\n",
    "    val cluster_cellsD = DoubleArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "    val geoTif = new SinglebandGeoTiff(cluster_cellsD, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "\n",
    "    //Save to /tmp/\n",
    "    GeoTiffWriter.write(geoTif, geotiff_tmp_paths(numClusters_id) + \"_\" + years(year) + \".tif\")\n",
    "\n",
    "    //Upload to HDFS\n",
    "    var cmd = \"hadoop dfs -copyFromLocal -f \" + geotiff_tmp_paths(numClusters_id) + \"_\" + years(year) + \".tif\" + \" \" + geotiff_hdfs_paths(numClusters_id) + \"_\" + years(year) + \".tif\"\n",
    "    Process(cmd)!\n",
    "\n",
    "    //Remove from /tmp/\n",
    "    cmd = \"rm -fr \" + geotiff_tmp_paths(numClusters_id) + \"_\" + years(year) + \".tif\"\n",
    "    Process(cmd)!\n",
    "\n",
    "    cellIDB.destroy()\n",
    "    year += 1\n",
    "  }\n",
    "  kmeans_res_zip.unpersist()\n",
    "  numClusters_id += 1\n",
    "}\n",
    "cells_per_yearB.destroy()\n",
    "grid0_index_I.unpersist()\n",
    "grid0.unpersist()\n",
    "t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0) + \"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Visualize results](plot_kmeans_clusters.ipynb) --------------- [Plot WSSE](kmeans_wsse.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
