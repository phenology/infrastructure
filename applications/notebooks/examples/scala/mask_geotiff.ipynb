{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask a GeoTiff\n",
    "\n",
    "In this notebook the user can load two GeoTiffs, extract a Tile from the first GeoTiff and mask it with second GeoTiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "//Spire is a numeric library for Scala which is intended to be generic, fast, and precise.\n",
    "import spire.syntax.cfor._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(spark.eventLog.enabled,true), (spark.hadoop.fs.s3a.connection.ssl.enabled,false), (spark.jars.ivy,), (spark.eventLog.dir,/data/local/spark/spark-events), (spark.hadoop.fs.s3a.fs.s3a.fast.upload,true), (spark.app.id,app-20170718202052-0004), (spark.hadoop.fs.s3a.endpoint,http://145.100.59.64:9091), (spark.jars.packages,), (spark.app.name,Apache Toree), (spark.hadoop.fs.s3a.access.key,A24H1RIGV4RKFGXJTEMS), (spark.serializer,org.apache.spark.serializer.KryoSerializer), (spark.local.dir,/data/local/spark/tmp/), (spark.memory.storageFraction,0.5), (spark.daemon.memory,3g), (spark.kryoserializer.buffer,512m), (spark.repl.class.outputDir,/data/local/spark/tmp/spark-b1ab533c-784b-41dc-9252-676994667033/repl-8286075f-fa24-4bd0-a9ad-d769bd89bc94), (spark.worker.memory,14g), (spark.driver.memory,8g), (spark.default.parallelism,32), (spark.dynamicAllocation.initialExecutors,1), (spark.submit.deployMode,client), (spark.network.timeout,360s), (spark.repl.class.uri,spark://145.100.59.64:39510/classes), (spark.broadcast.blockSize,20m), (spark.hadoop.fs.s3a.secret.key,5jd7ARCOi/XVjLzXqT5wA1NSgjmUo9mYJBgyGyIh), (spark.sql.warehouse.dir,file:///data/local/spark/), (spark.shuffle.service.enabled,true), (spark.master,spark://emma0.phenovari-utwente.surf-hosted.nl:7077), (spark.driver.allowMultipleContexts,true), (spark.scheduler.mode,FAIR), (spark.executor.id,driver), (spark.executor.cores,2), (spark.driver.extraClassPath,/usr/lib/spark/jars/aws-java-sdk-s3-1.10.6.jar:/usr/lib/spark/jars/hadoop-aws-2.8.0.jar:/usr/lib/spark/jars/joda-time-2.9.4.jar), (spark.history.fs.cleaner.enabled,true), (spark.executor.memory,12g), (spark.shuffle.service.port,7338), (spark.driver.port,39510), (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (spark.hadoop.fs.s3a.buffer.dir,/root/spark/work,/tmp), (spark.kryoserializer.buffer.max,1g), (spark.jars,file:/usr/local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.2.0.dev1-incubating-SNAPSHOT.jar), (spark.executor.extraClassPath,/usr/lib/spark/jars/aws-java-sdk-s3-1.10.6.jar:/usr/lib/spark/jars/hadoop-aws-2.8.0.jar:/usr/lib/spark/jars/joda-time-2.9.4.jar), (spark.driver.host,145.100.59.64), (spark.memory.fraction,0.6), (spark.driver.maxResultSize,2g), (spark.dynamicAllocation.maxExecutors,8), (spark.dynamicAllocation.enabled,true)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a GeoTiff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geo_projected_extent = ProjectedExtent(Extent(-171.1296209227216, 19.996701428244357, -42.56469205974807, 49.99999999547987),geotrellis.proj4.CRS$$anon$3@41d0d1b7)\n",
       "geo_num_cols_rows = (17800,4154)\n",
       "geo_path = hdfs:///user/emma/modis/usa_mask.tif\n",
       "geo_tiles_RDD = MapPartitionsRDD[2] at values at <console>:49\n",
       "geo_extents_withIndex = MapPartitionsRDD[7] at map at <console>:51\n",
       "geo_projected_extent = ProjectedExtent(Extent(-171.1296209227216, 19.996701428244357, -42.56469205974807, 49.99999999547987),geotrellis.proj4.CRS$$anon$3@41d0d1b7)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "geo_tiles_withIndex: org.apache.spark.rdd.RDD[(Lon...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ProjectedExtent(Extent(-171.1296209227216, 19.996701428244357, -42.56469205974807, 49.99999999547987),geotrellis.proj4.CRS$$anon$3@41d0d1b7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var geo_projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var geo_num_cols_rows :(Int, Int) = (0, 0)\n",
    "val geo_path = \"hdfs:///user/emma/modis/usa_mask.tif\"\n",
    "val geo_tiles_RDD = sc.hadoopGeoTiffRDD(geo_path).values\n",
    "\n",
    "val geo_extents_withIndex = sc.hadoopMultibandGeoTiffRDD(geo_path).keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "geo_projected_extent = (geo_extents_withIndex.filter(m => m._1 == 0).values.collect())(0)\n",
    "\n",
    "val geo_tiles_withIndex = geo_tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "val geo_tile0 = (geo_tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "geo_num_cols_rows = (geo_tile0.cols, geo_tile0.rows)\n",
    "val geo_cellT = geo_tile0.cellType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask_path = hdfs:///user/hadoop/modis/usa_mask.tif\n",
       "mask_tiles_RDD = MapPartitionsRDD[16] at values at <console>:47\n",
       "mask_tiles_withIndex = MapPartitionsRDD[18] at map at <console>:48\n",
       "mask_tile0 = FloatRawArrayTile([F@7119d8d1,15616,7784)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FloatRawArrayTile([F@7119d8d1,15616,7784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val mask_path = \"hdfs:///user/hadoop/modis/usa_mask.tif\"\n",
    "    val mask_tiles_RDD = sc.hadoopGeoTiffRDD(mask_path).values\n",
    "    val mask_tiles_withIndex = mask_tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    val mask_tile0 = (mask_tiles_withIndex.filter(m => m._1==0).values.collect())(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: geotrellis.raster.GeoAttrsError\n",
       "Message: Cannot combine rasters with different dimensions.(17800,4154) does not match (15616,7784)\n",
       "StackTrace:   at geotrellis.raster.package$TileTupleExtensions.assertEqualDimensions(package.scala:141)\n",
       "  at geotrellis.raster.ArrayTile$class.combineDouble(ArrayTile.scala:248)\n",
       "  at geotrellis.raster.DoubleArrayTile.combineDouble(DoubleArrayTile.scala:24)\n",
       "  at geotrellis.raster.ArrayTile$class.combineDouble(ArrayTile.scala:273)\n",
       "  at geotrellis.raster.DoubleArrayTile.combineDouble(DoubleArrayTile.scala:24)\n",
       "  at geotrellis.raster.Tile$class.dualCombine(Tile.scala:101)\n",
       "  at geotrellis.raster.DoubleArrayTile.dualCombine(DoubleArrayTile.scala:24)\n",
       "  at geotrellis.raster.mapalgebra.local.InverseMask$.apply(InverseMask.scala:32)\n",
       "  at geotrellis.raster.mask.SinglebandTileMaskMethods$class.localInverseMask(SinglebandTileMaskMethods.scala:54)\n",
       "  at geotrellis.raster.package$withTileMethods.localInverseMask(package.scala:53)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res_tile = geo_tile0.localInverseMask(mask_tile0, 1, 0).toArrayDouble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the new GeoTiff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:47: error: not found: value res_tile\n",
       "       val clone_tile = DoubleArrayTile(res_tile, geo_num_cols_rows._1, geo_num_cols_rows._2)\n",
       "                                        ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clone_tile = DoubleArrayTile(res_tile, geo_num_cols_rows._1, geo_num_cols_rows._2)\n",
    "\n",
    "val cloned = geotrellis.raster.DoubleArrayTile.empty(geo_num_cols_rows._1, geo_num_cols_rows._2)\n",
    "cfor(0)(_ < geo_num_cols_rows._1, _ + 1) { col =>\n",
    "    cfor(0)(_ < geo_num_cols_rows._2, _ + 1) { row =>\n",
    "        val v = clone_tile.getDouble(col, row)\n",
    "        cloned.setDouble(col, row, v)\n",
    "    }\n",
    "}\n",
    "\n",
    "val geoTif = new SinglebandGeoTiff(cloned, geo_projected_extent.extent, geo_projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "\n",
    "//Save GeoTiff to /tmp\n",
    "val output = \"/user/emma/modis/modis_usa_mask.tif\"\n",
    "val tmp_output = \"/tmp/modis_usa_mask.tif\"\n",
    "GeoTiffWriter.write(geoTif, tmp_output)\n",
    "\n",
    "//Upload to HDFS\n",
    "var cmd = \"hadoop dfs -copyFromLocal -f \" + tmp_output + \" \" + output\n",
    "Process(cmd)!\n",
    "\n",
    "cmd = \"rm -fr \" + tmp_output\n",
    "Process(cmd)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
