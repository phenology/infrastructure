{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV file\n",
    "\n",
    "This notebook show how to load an Array of triples stored as objectFile and save it again as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.io.{IOUtils, SequenceFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "offline_dir_path = hdfs:///user/emma/spring-index/\n",
       "geoTiff_dir = BloomFinal\n",
       "wssse_path = hdfs:///user/emma/spring-index/BloomFinal/wssse\n",
       "wssse_csv_path = hdfs:///user/emma/spring-index/BloomFinal/wssse.csv\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/usr/lib/spark-2.1.1-bin-without-hadoop/conf/hive-site.xml\n",
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1047414169_36, ugi=emma (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1047414169_36, ugi=emma (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var offline_dir_path = \"hdfs:///user/emma/spring-index/\"\n",
    "var geoTiff_dir = \"BloomFinal\"\n",
    "var wssse_path :String = offline_dir_path + geoTiff_dir + \"/wssse\"\n",
    "var wssse_csv_path :String = offline_dir_path + geoTiff_dir + \"/wssse.csv\"\n",
    "\n",
    "var conf = sc.hadoopConfiguration\n",
    "var fs = org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_csv_path))) {\n",
    "    println(\"The file \" + wssse_csv_path + \" already exists we will delete it!!!\")\n",
    "    try { fs.delete(new org.apache.hadoop.fs.Path(wssse_csv_path), true) } catch { case _ : Throwable => { } }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List((12,35,4.3297733110180664E10), (2,35,1.3269493028574286E11), (3,35,7.612110282892206E10), (4,35,5.192563731031121E10), (5,35,3.86547975867681E10), (6,35,3.1070608358128193E10), (7,35,2.657061805793561E10), (8,35,2.307391282797956E10), (9,35,2.0876037759553226E10), (10,35,1.9153577999634872E10), (11,35,1.8028667788894394E10), (12,35,1.7020487136558575E10), (13,35,1.671272347748397E10), (14,35,1.5865979019165756E10), (15,35,1.5225546057630272E10))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wssse_data = MapPartitionsRDD[2] at objectFile at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at objectFile at <console>:49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wssse_data :RDD[(Int, Int, Double)] = sc.emptyRDD\n",
    "\n",
    "//from disk\n",
    "if (fs.exists(new org.apache.hadoop.fs.Path(wssse_path))) {\n",
    "    wssse_data = sc.objectFile(wssse_path)\n",
    "    println(wssse_data.collect().toList)        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wssse = MapPartitionsRDD[10] at map at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wssse = wssse_data.repartition(1).sortBy(_._1).map{case (a,b,c) => Array(a,b,c).mkString(\",\")}\n",
    "wssse.saveAsTextFile(wssse_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
