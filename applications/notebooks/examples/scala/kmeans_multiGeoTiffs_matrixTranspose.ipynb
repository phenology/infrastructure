{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans over a set of GeoTiffs\n",
    "\n",
    "This notebook loads a set of GeoTiffs into a **RDD** of Tiles, with each Tile being a band in the GeoTiff. Each GeoTiff file contains **SpringIndex-** or **LastFreeze-** value for one year over the entire USA.\n",
    "\n",
    "Kmeans takes years as dimensions. Hence, the matrix has cells as rows and the years as columns. To cluster on all years, the matrix needs to be transposed. The notebook has two flavors of matrix transpose, locally by the Spark-driver or distributed using the Spark-workers. Once transposed the matrix is converted to a **RDD** of dense vectors to be used by **Kmeans** algorithm from **Spark-MLlib**. The end result is a grid where each cell has a cluster ID which is then saved into a SingleBand GeoTiff. By saving the result into a GeoTiff, the reader can plot it using a Python notebook as the one defined in the [python examples](../examples/python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.{ArrayTile, DoubleArrayTile, Tile}\n",
    "import geotrellis.raster.io.geotiff._\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{GeoTiff, SinglebandGeoTiff}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multiple GeoTiffs into a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val single_band = True;\n",
    "\n",
    "val local_mode = True;\n",
    "\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var band_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "var band_vec: RDD[Vector] = sc.emptyRDD\n",
    "var band0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var band0_index: RDD[Long] = sc.emptyRDD\n",
    "\n",
    "val pattern: String = \"tif\"\n",
    "var filepath: String = \"\"\n",
    "\n",
    "if (single_band) {\n",
    "    //Single band GeoTiff\n",
    "    filepath = \"hdfs:///user/hadoop/spring-index/LastFreeze/\"\n",
    "} else {\n",
    "    //Multi band GeoTiff\n",
    "    filepath = \"hdfs:///user/hadoop/spring-index/BloomFinal/\"\n",
    "}\n",
    "\n",
    "if (single_band) {\n",
    "    //Lets load a Singleband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopGeoTiffRDD(filepath, pattern).values\n",
    "    val bands_RDD = tiles_RDD.map(m => m.toArrayDouble())\n",
    "else {\n",
    "    //Lets load Multiband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopMultibandGeoTiffRDD(filepath, pattern).values\n",
    "    \n",
    "    //Lets read the average of the Spring-Index which is stored in the 4th band\n",
    "    val bands_RDD = tiles_RDD.map(m => m.band(3).toArrayDouble())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata and create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "val extents_withIndex = sc.hadoopGeoTiffRDD(filepath, pattern).keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "projected_extent = (extents_withIndex.filter(m => m._1 == 0).values.collect())(0)\n",
    "\n",
    "//Retrive the numbre of cols and rows of the Tile's grid\n",
    "val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "val tile0 = (tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "num_cols_rows = (tile0.cols,tile0.rows)\n",
    "\n",
    "//Get Index for each Cell\n",
    "val bands_withIndex = bands_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "band0_index = bands_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => !m._1.isNaN).map { case (v, i) => (i) }\n",
    "\n",
    "//Get the Tile's grid\n",
    "band0 = bands_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "//Lets filter out NaN\n",
    "band_RDD = bands_RDD.map(m => m.filter(!_.isNaN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix transpose\n",
    "\n",
    "We need to do a Matrix transpose to have clusters per cell and not per year. With a GeoTiff representing a single year, the loaded data looks liks this:\n",
    "```\n",
    "bands_RDD.map(s => Vectors.dense(s)).cache()\n",
    "\n",
    "//The vectors are rows and therefore the matrix will look like this:\n",
    "[\n",
    "Vectors.dense(0.0, 1.0, 2.0),\n",
    "Vectors.dense(3.0, 4.0, 5.0),\n",
    "Vectors.dense(6.0, 7.0, 8.0),\n",
    "Vectors.dense(9.0, 0.0, 1.0)\n",
    "]\n",
    "```\n",
    "\n",
    "The information was gathered from the blog [how to convert a matrix to a RDD of vectors](http://jacob119.blogspot.nl/2015/11/how-to-convert-matrix-to-rddvector-in.html) and a stackoverflow post on [how to transpose an rdd in spark](https://stackoverflow.com/questions/29390717/how-to-transpose-an-rdd-in-spark).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//A) For small memory footprint RDDs we can simply bring it to the Driver node and transpose it    \n",
    "if (local_mode) {\n",
    "    //First transpose and then parallelize otherwise you get:\n",
    "    //error: polymorphic expression cannot be instantiated to expected type;\n",
    "    val band_vec_T = band_RDD.collect().transpose\n",
    "    \n",
    "    //Convert to a RDD\n",
    "    val transposed = sc.parallelize(band_vec_T)\n",
    "    band_vec = sc.parallelize(band_vec_T).map(m => Vectors.dense(m)).cache()\n",
    "\n",
    "//B) For large memory footpring RDDs we need to run in distributed mode\n",
    "} else {\n",
    "    // Split the matrix into one number per line.\n",
    "    val byColumnAndRow = band_RDD.zipWithIndex.flatMap {\n",
    "        case (row, rowIndex) => row.zipWithIndex.map {\n",
    "            case (number, columnIndex) => columnIndex -> (rowIndex, number)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Build up the transposed matrix. Group and sort by column index first.\n",
    "    val byColumn = byColumnAndRow.groupByKey.sortByKey().values\n",
    "\n",
    "    // Then sort by row index.\n",
    "    val transposed = byColumn.map {\n",
    "        indexedRow => indexedRow.toSeq.sortBy(_._1).map(_._2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kmeans training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Create a RDD of dense vectors and cache it\n",
    "band_vec = transposed.map(m => Vectors.dense(m.toArray)).cache()\n",
    "    \n",
    "/*\n",
    " Here we will train kmeans\n",
    "*/\n",
    "\n",
    "val numClusters = 3\n",
    "val numIterations = 5\n",
    "val clusters = {\n",
    "    KMeans.train(band_vec, numClusters, numIterations)\n",
    "}\n",
    "\n",
    "// Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "val WSSSE = clusters.computeCost(band_vec)\n",
    "println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n",
    "\n",
    "//Un-persist it to save memory\n",
    "band_vec.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster model's result management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Lets show the result.\n",
    "println(\"Cluster Centers: \")\n",
    "//clusters.clusterCenters.foreach(println)\n",
    "\n",
    "//Lets save the model into HDFS. If the file already exists it will abort and report error.\n",
    "if (single_band) {\n",
    "    clusters.save(sc, \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model\")\n",
    "} else {\n",
    "    clusters.save(sc, \"hdfs:///user/emma/spring_index/BloomFinal/all_kmeans_model\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Kmeans clustering\n",
    "\n",
    "Run Kmeans and obtain the clusters per each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Cache it so kmeans is more efficient\n",
    "band_vec.cache()\n",
    "\n",
    "val res = clusters.predict(band_vec)\n",
    "res.repartition(1)getNumPartitions\n",
    "\n",
    "//Un-persist it to save memory\n",
    "band_vec.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the cluster ID for the first 50 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val res_out = res.collect()//.take(50)\n",
    "res_out.foreach(println)\n",
    "\n",
    "println(res_out.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign cluster ID to each grid cell\n",
    "\n",
    "To assign the clusterID to each grid cell it is necessary to get the indices of gird cells they belong to. The process is not straight forward because the ArrayDouble used for the creation of each dense Vector does not contain the NaN values, therefore there is not a direct between the indices in the Tile's grid and the ones in **res** (kmeans result).\n",
    "\n",
    "To join the two RDDS the knowledge was obtaing from a stackoverflow post on [how to perform basic joins of two rdd tables in spark using python](https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//The zip operator would be the most appropriated operator for this operation.\n",
    "//However, it requires the RRDs to have the same number of partitions and each partition have the same number of records.\n",
    "//val cluster_cell_pos = res.zip(band0_index)\n",
    "\n",
    "//Since we can't use Zip, we index each RDD and then we join them.\n",
    "val cluster_cell_pos = ((res.zipWithIndex().map{ case (v,i) => (i,v)}).join(band0_index.zipWithIndex().map{ case (v,i) => (i,v)})).map{ case (k,(v,i)) => (v,i)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate a Cluster_IDs to respective Grid_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//We use a left join if an Cluster_ID indice does not exist, None is set as value.\n",
    "val grid_clusters = band0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i.toLong, c)})\n",
    "\n",
    "//Convert all None to NaN\n",
    "val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get.toDouble)}//.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the results in a SingleBand GeoTiff\n",
    "\n",
    "The Grid with the cluster IDs is stored in a SinglBand GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cluster_cells :Array[Int] = grid_clusters_res.values.map(m => m.toInt).collect()\n",
    "\n",
    "//Define a Tile\n",
    "val cluster_tile = ArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "//Generate GeoTiff with the same header as the one used by the input GeoTiffs\n",
    "val geoTiff = SinglebandGeoTiff(cluster_tile, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT)\n",
    "\n",
    "//Convert to RDD and save it as a ByteArray, if the file already exists it will report error.\n",
    "sc.parallelize(geoTiff.toByteArray).saveAsObjectFile(\"hdfs:///user/emma/spring-index/LastFreeze/clusters.tif\")\n",
    "\n",
    "//Another option is to write to the local file system of the node where JupyterHub is running\n",
    "//val path = \"~/clusters.tif\"\n",
    "//GeoTiffWriter.write(geoTiff, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
