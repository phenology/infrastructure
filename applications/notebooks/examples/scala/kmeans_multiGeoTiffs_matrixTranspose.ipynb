{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans over a set of GeoTiffs\n",
    "\n",
    "This notebook loads a set of GeoTiffs into a **RDD** of Tiles, with each Tile being a band in the GeoTiff. Each GeoTiff file contains **SpringIndex-** or **LastFreeze-** value for one year over the entire USA.\n",
    "\n",
    "Kmeans takes years as dimensions. Hence, the matrix has cells as rows and the years as columns. To cluster on all years, the matrix needs to be transposed. The notebook has two flavors of matrix transpose, locally by the Spark-driver or distributed using the Spark-workers. Once transposed the matrix is converted to a **RDD** of dense vectors to be used by **Kmeans** algorithm from **Spark-MLlib**. The end result is a grid where each cell has a cluster ID which is then saved into a SingleBand GeoTiff. By saving the result into a GeoTiff, the reader can plot it using a Python notebook as the one defined in the [python examples](../examples/python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.{ArrayTile, DoubleArrayTile, Tile}\n",
    "import geotrellis.raster.io.geotiff._\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{GeoTiff, SinglebandGeoTiff}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry, RowMatrix}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multiple GeoTiffs into a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 106:=====================================================> (35 + 1) / 36]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "single_band = false\n",
       "local_mode = false\n",
       "num_cols_rows = (7808,3892)\n",
       "grids_RDD = MapPartitionsRDD[181] at map at <console>:121\n",
       "pattern = tif\n",
       "filepath = hdfs:///user/hadoop/spring-index/BloomFinal/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/hadoop/spring-index/BloomFinal/"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val single_band = false\n",
    "val local_mode = false\n",
    "\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "var grids_RDD :RDD[Array[Double]] = sc.emptyRDD\n",
    "\n",
    "val pattern: String = \"tif\"\n",
    "var filepath: String = \"\"\n",
    "\n",
    "if (single_band) {\n",
    "    //Single band GeoTiff\n",
    "    filepath = \"hdfs:///user/hadoop/spring-index/LastFreeze/\"\n",
    "} else {\n",
    "    //Multi band GeoTiff\n",
    "    filepath = \"hdfs:///user/hadoop/spring-index/BloomFinal/\"\n",
    "}\n",
    "\n",
    "if (single_band) {\n",
    "    //Lets load a Singleband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopGeoTiffRDD(filepath, pattern).values\n",
    "    \n",
    "    //Retrive the numbre of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "    num_cols_rows = (tile0.cols,tile0.rows)\n",
    "    \n",
    "    grids_RDD = tiles_RDD.map(m => m.toArrayDouble())\n",
    "} else {\n",
    "    //Lets load Multiband GeoTiffs and return RDD just with the tiles.\n",
    "    val tiles_RDD = sc.hadoopMultibandGeoTiffRDD(filepath, pattern).values\n",
    "    \n",
    "    //Retrive the numbre of cols and rows of the Tile's grid\n",
    "    val tiles_withIndex = tiles_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "    val tile0 = (tiles_withIndex.filter(m => m._1==0).values.collect())(0)\n",
    "    num_cols_rows = (tile0.cols,tile0.rows)\n",
    "    \n",
    "    //Lets read the average of the Spring-Index which is stored in the 4th band\n",
    "    grids_RDD = tiles_RDD.map(m => m.band(3).toArrayDouble())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata and create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 111:=====================================================> (34 + 1) / 35]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "projected_extent = ProjectedExtent(Extent(-126.30312894720473, 14.29219617034159, -56.162671563152486, 49.25462702827337),geotrellis.proj4.CRS$$anon$3@41d0d1b7)\n",
       "grid0 = MapPartitionsRDD[204] at map at <console>:102\n",
       "grid0_index = MapPartitionsRDD[199] at map at <console>:99\n",
       "grids_noNaN_RDD = MapPartitionsRDD[205] at map at <console>:105\n",
       "projected_extents_withIndex = MapPartitionsRDD[189] at map at <console>:94\n",
       "projected_extent = ProjectedExtent(Extent(-126.30312894720473, 14.29219617034159, -56.162671563152486, 49.25462702827337),geotrell...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ProjectedExtent(Extent(-126.30312894720473, 14.29219617034159, -56.162671563152486, 49.25462702827337),geotrellis.proj4.CRS$$anon$3@41d0d1b7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Global variables\n",
    "var projected_extent = new ProjectedExtent(new Extent(0,0,0,0), CRS.fromName(\"EPSG:3857\"))\n",
    "var grid0: RDD[(Long, Double)] = sc.emptyRDD\n",
    "var grid0_index: RDD[Long] = sc.emptyRDD\n",
    "var grids_noNaN_RDD: RDD[Array[Double]] = sc.emptyRDD\n",
    "\n",
    "//Retrieve the ProjectExtent which contains metadata such as CRS and bounding box\n",
    "val projected_extents_withIndex = sc.hadoopGeoTiffRDD(filepath, pattern).keys.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "projected_extent = (projected_extents_withIndex.filter(m => m._1 == 0).values.collect())(0)\n",
    "\n",
    "//Get Index for each Cell\n",
    "val grids_withIndex = grids_RDD.zipWithIndex().map { case (e, v) => (v, e) }\n",
    "grid0_index = grids_withIndex.filter(m => m._1 == 0).values.flatMap(m => m).zipWithIndex.filter(m => !m._1.isNaN).map { case (v, i) => (i) }\n",
    "\n",
    "//Get the Tile's grid\n",
    "grid0 = grids_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}\n",
    "\n",
    "//Lets filter out NaN\n",
    "grids_noNaN_RDD = grids_RDD.map(m => m.filter(!_.isNaN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix transpose\n",
    "\n",
    "We need to do a Matrix transpose to have clusters per cell and not per year. With a GeoTiff representing a single year, the loaded data looks liks this:\n",
    "```\n",
    "bands_RDD.map(s => Vectors.dense(s)).cache()\n",
    "\n",
    "//The vectors are rows and therefore the matrix will look like this:\n",
    "[\n",
    "Vectors.dense(0.0, 1.0, 2.0),\n",
    "Vectors.dense(3.0, 4.0, 5.0),\n",
    "Vectors.dense(6.0, 7.0, 8.0),\n",
    "Vectors.dense(9.0, 0.0, 1.0)\n",
    "]\n",
    "```\n",
    "\n",
    "The information was gathered from the blog [how to convert a matrix to a RDD of vectors](http://jacob119.blogspot.nl/2015/11/how-to-convert-matrix-to-rddvector-in.html) and a stackoverflow post on [how to transpose an rdd in spark](https://stackoverflow.com/questions/29390717/how-to-transpose-an-rdd-in-spark).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                         (0 + 8) / 36]"
     ]
    }
   ],
   "source": [
    "var grids_vec: RDD[Vector] = sc.emptyRDD\n",
    "\n",
    "//A) For small memory footprint RDDs we can simply bring it to the Driver node and transpose it    \n",
    "if (local_mode) {\n",
    "    //First transpose and then parallelize otherwise you get:\n",
    "    //error: polymorphic expression cannot be instantiated to expected type;\n",
    "    val grids_noNaN_RDD_T = grids_noNaN_RDD.collect().transpose\n",
    "    \n",
    "    //Convert to a RDD\n",
    "    val transposed = sc.parallelize(grids_noNaN_RDD_T)\n",
    "    \n",
    "    //Create a RDD of dense vectors and cache it\n",
    "    grids_vec = transposed.map(m => Vectors.dense(m)).cache()\n",
    "\n",
    "//B) For large memory footpring RDDs we need to run in distributed mode\n",
    "} else {\n",
    "      val mat :RowMatrix = new RowMatrix(grids_noNaN_RDD.map(m => Vectors.dense(m)))\n",
    "\n",
    "      // Split the matrix into one number per line.\n",
    "      val byColumnAndRow = mat.rows.zipWithIndex.map {\n",
    "        case (row, rowIndex) => row.toArray.zipWithIndex.map {\n",
    "          case (number, columnIndex) => new MatrixEntry(rowIndex, columnIndex, number)\n",
    "        }\n",
    "      }.flatMap(x => x)\n",
    "    \n",
    "    val matt: CoordinateMatrix = new CoordinateMatrix(byColumnAndRow)\n",
    "    val matt_T = matt.transpose()\n",
    "    var grids_vec: RDD[Vector] = sc.emptyRDD\n",
    "    grids_vec = matt_T.toRowMatrix().rows\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kmeans training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 2.6870621488867938E11                        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numClusters = 3\n",
       "numIterations = 5\n",
       "clusters = org.apache.spark.mllib.clustering.KMeansModel@54b35a67\n",
       "WSSSE = 2.6870621488867938E11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[216] at map at IndexedRowMatrix.scala:90"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClusters = 3\n",
    "val numIterations = 5\n",
    "val clusters = {\n",
    "    KMeans.train(grids_vec, numClusters, numIterations)\n",
    "}\n",
    "\n",
    "// Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "val WSSSE = clusters.computeCost(grids_vec)\n",
    "println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n",
    "\n",
    "//Un-persist it to save memory\n",
    "grids_vec.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster model's result management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[Stage 147:=====================================================> (31 + 1) / 32]"
     ]
    }
   ],
   "source": [
    "// Lets show the result.\n",
    "println(\"Cluster Centers: \")\n",
    "//clusters.clusterCenters.foreach(println)\n",
    "\n",
    "//Lets save the model into HDFS. If the file already exists it will abort and report error.\n",
    "if (single_band) {\n",
    "    clusters.save(sc, \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model\")\n",
    "} else {\n",
    "    clusters.save(sc, \"hdfs:///user/emma/spring_index/BloomFinal/all_kmeans_model\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-load the KmeansModel\n",
    "\n",
    "In case the notebook was left running in the background the next task helps the user to re-load the Kmeans Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 45:======================================================> (31 + 1) / 32]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clusters = org.apache.spark.mllib.clustering.KMeansModel@340b3947\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@340b3947"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "val clusters = KMeansModel.load(sc, \"hdfs:///user/emma/spring_index/LastFreeze/all_kmeans_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Kmeans clustering\n",
    "\n",
    "Run Kmeans and obtain the clusters per each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans_res = MapPartitionsRDD[252] at map at KMeansModel.scala:69\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[216] at map at IndexedRowMatrix.scala:90"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Cache it so kmeans is more efficient\n",
    "grids_vec.cache()\n",
    "\n",
    "val kmeans_res = clusters.predict(grids_vec)\n",
    "\n",
    "//Un-persist it to save memory\n",
    "grids_vec.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the cluster ID for the first 50 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                        (0 + 8) / 32]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.InterruptedException\n",
       "Message: null\n",
       "StackTrace:   at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n",
       "  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n",
       "  at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)\n",
       "  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)\n",
       "  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmeans_res_out = kmeans_res.take(50)\n",
    "kmeans_res_out.foreach(println)\n",
    "\n",
    "println(kmeans_res_out.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign cluster ID to each grid cell\n",
    "\n",
    "To assign the clusterID to each grid cell it is necessary to get the indices of gird cells they belong to. The process is not straight forward because the ArrayDouble used for the creation of each dense Vector does not contain the NaN values, therefore there is not a direct between the indices in the Tile's grid and the ones in **kmeans_res** (kmeans result).\n",
    "\n",
    "To join the two RDDS the knowledge was obtaing from a stackoverflow post on [how to perform basic joins of two rdd tables in spark using python](https://stackoverflow.com/questions/31257077/how-do-you-perform-basic-joins-of-two-rdd-tables-in-spark-using-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two RDDs, one containing the clusters_ID indices and the other one the indices of a Tile's grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 150:=====================================================> (34 + 1) / 35]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster_cell_pos = MapPartitionsRDD[260] at map at <console>:91\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[260] at map at <console>:91"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//The zip operator would be the most appropriated operator for this operation.\n",
    "//However, it requires the RRDs to have the same number of partitions and each partition have the same number of records.\n",
    "//val cluster_cell_pos = res.zip(band0_index)\n",
    "\n",
    "//Since we can't use Zip, we index each RDD and then we join them.\n",
    "val cluster_cell_pos = ((kmeans_res.zipWithIndex().map{ case (v,i) => (i,v)}).join(grid0_index.zipWithIndex().map{ case (v,i) => (i,v)})).map{ case (k,(v,i)) => (v,i)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate a Cluster_IDs to respective Grid_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 156:=====================================================> (31 + 1) / 32]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grid_clusters = MapPartitionsRDD[264] at leftOuterJoin at <console>:98\n",
       "grid_clusters_res = MapPartitionsRDD[268] at map at <console>:101\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[268] at map at <console>:101"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//We use a left join if an Cluster_ID indice does not exist, None is set as value.\n",
    "val grid_clusters = grid0.leftOuterJoin(cluster_cell_pos.map{ case (c,i) => (i.toLong, c)})\n",
    "\n",
    "//Convert all None to NaN\n",
    "val grid_clusters_res = grid_clusters.sortByKey(true).map{case (k, (v, c)) => if (c == None) (k, Double.NaN) else (k, c.get.toDouble)}//.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the results in a SingleBand GeoTiff\n",
    "\n",
    "The Grid with the cluster IDs is stored in a SinglBand GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 165:=====================================================> (31 + 1) / 32]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster_cells = Array(0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "val cluster_cells :Array[Double] = grid_clusters_res.values.map(m => m.toDouble).collect()\n",
    "\n",
    "//Define a Tile\n",
    "val cluster_tile = ArrayTile(cluster_cells, num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "//Generate GeoTiff with the same header as the one used by the input GeoTiffs\n",
    "val geoTiff = SinglebandGeoTiff(cluster_tile, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions.DEFAULT)\n",
    "\n",
    "//Convert to RDD and save it as a ByteArray, if the file already exists it will report error.\n",
    "if (single_band) {\n",
    "    sc.parallelize(GeoTiffWriter.write(geoTiff)).saveAsObjectFile(\"hdfs:///user/emma/spring-index/LastFreeze/clusters_3.tif\")    \n",
    "} else {\n",
    "    sc.parallelize(GeoTiffWriter.write(geoTiff)).saveAsObjectFile(\"hdfs:///user/emma/spring-index/BloomFinal/clusters.tif\")    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
